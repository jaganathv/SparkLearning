{"cells":[{"cell_type":"code","source":["from pyspark.sql.types import *\nidColumn=StructField(\"id\",StringType(),True)\ngenderColumn=StructField(\"gender\",StringType(),True)\nOccupationColumn=StructField(\"Occupation\",StringType(),True) \nswimTimeInSecondsColumn=StructField(\"swimTimeInSeconds\",DoubleType(),True) "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["DfColumnList=[idColumn,genderColumn,OccupationColumn,swimTimeInSecondsColumn]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["swimmerDfSchema=StructType(DfColumnList)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["swimmerDfSchema"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[5]: StructType(List(StructField(id,StringType,true),StructField(gender,StringType,true),StructField(Occupation,StringType,true),StructField(swimTimeInSeconds,DoubleType,true)))</div>"]}}],"execution_count":5},{"cell_type":"code","source":["swimmerDfSchema"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: StructType(List(StructField(id,StringType,true),StructField(gender,StringType,true),StructField(Occupation,StringType,true),StructField(swimTimeInSeconds,DoubleType,true)))</div>"]}}],"execution_count":6},{"cell_type":"code","source":["import pandas as pd\nlst=['Geeks','for','Geeks','is','portal','for','geeks']\nlst_df=pd.DataFrame(lst)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["type(lst_df)\nlst_df\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Geeks</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>for</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Geeks</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>is</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>portal</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>for</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>geeks</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["lst_df.head(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Geeks</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>for</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["lst_df.tail(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>for</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>geeks</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["import pandas as pd\ndata={\"name\":['Tom','nick','krish','josh'],\"Age\":[23,24,25,26]}\nprint(type(data))\nperson_df=pd.DataFrame(data)\nprint(person_df)\nprint(type(person_df))\nperson_df.iloc[3]\ntemp=person_df.iloc[3,0],person_df.iloc[3,1]\nprint(temp)\nprint(type(temp))\nprint(\"\\n\")\nprint(\"Example of a Non_Indexed Named Tuple\")\nprint(\"By DEFAULT all TUPLES are Non Named and Non Indexed\")\nprint(\"\\n\")\nfor row in person_df.itertuples(name='Person',index=False):\n# print(row)\n  print(type(row))\n   print(row.name,row.Age)\n  temp=(row.name,row.Age)\n   print(row.name)\n  print(type(temp))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-cyan-fg\">  File </span><span class=\"ansi-green-fg\">&#34;&lt;command-3216309235988770&gt;&#34;</span><span class=\"ansi-cyan-fg\">, line </span><span class=\"ansi-green-fg\">18</span>\n<span class=\"ansi-red-fg\">    print(row.name,row.Age)</span>\n    ^\n<span class=\"ansi-red-fg\">IndentationError</span><span class=\"ansi-red-fg\">:</span> unexpected indent\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["import pymysql\nlst=['Geeks','for','Geeks','is','portal','for','geeks']\nlst_df=pd.DataFrame(lst)\nprint(lst_df)\nfor row in lst_df.iterrows():\n  print(type(row))\nfor row in lst_df.itertuples(name='Person',index=False):\n  print(type(row))\n  \ndata={\"name\":['Tom','nick','krish','josh'],\"Age\":[23,24,25,26]}\ndata_df=pd.DataFrame(data)\nprint(data_df.columns)\nfor row in data_df.itertuples():\n  print(type(row.name))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4377340460066971&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     14</span>   print<span class=\"ansi-blue-fg\">(</span>type<span class=\"ansi-blue-fg\">(</span>row<span class=\"ansi-blue-fg\">.</span>name<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     15</span> <span class=\"ansi-green-fg\">for</span> row <span class=\"ansi-green-fg\">in</span> data_df<span class=\"ansi-blue-fg\">.</span>iterrows<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 16</span><span class=\"ansi-red-fg\">   </span>print<span class=\"ansi-blue-fg\">(</span>row<span class=\"ansi-blue-fg\">.</span>name<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AttributeError</span>: &#39;tuple&#39; object has no attribute &#39;name&#39;</div>"]}}],"execution_count":12},{"cell_type":"code","source":["from pyspark.sql.types import *\nidColumn=StructField(\"id\",StringType(),True)\ngenderColumn=StructField(\"gender\",StringType(),True)\nOccupationColumn=StructField(\"Occupation\",StringType(),True)\nswimTimeInSecondColumn=StructField(\"swimTimeInSecond\",DoubleType(),True)\n\ncolumnList=[idColumn,genderColumn,OccupationColumn,swimTimeInSecondColumn]\nswimmerDfSchema=StructType(columnList)\ndf_swimmers=spark.read.csv('/FileStore/tables/swimmerData.csv',header=True,schema=swimmerDfSchema)\ndf_swimmers.show(4)\ndf_swimmers.printSchema()\ntype(df_swimmers)\ndf_swimmers.registerTempTable(\"df_swimmers\")\ntemp=spark.sql(\"select * from df_swimmers\")\nprint(temp)\n\nswimmer_df=spark.read.csv('/FileStore/tables/swimmerData.csv',header=True,inferSchema=True)\nswimmer_df.show(4)\nswimmer_df.printSchema()\n\nswimmer_df1=spark.read.csv('/FileStore/tables/swimmerData1.csv',header=True,inferSchema=True,sep='#')\nswimmer_df1.show(6)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+------+-----------+----------------+\n id|gender| Occupation|swimTimeInSecond|\n+---+------+-----------+----------------+\nid1|  Male| Programmer|           16.73|\nid2|Female|    Manager|           15.56|\nid3|  Male|    Manager|           15.15|\nid4|  Male|RiskAnalyst|           15.27|\n+---+------+-----------+----------------+\nonly showing top 4 rows\n\nroot\n-- id: string (nullable = true)\n-- gender: string (nullable = true)\n-- Occupation: string (nullable = true)\n-- swimTimeInSecond: double (nullable = true)\n\nDataFrame[id: string, gender: string, Occupation: string, swimTimeInSecond: double]\n+---+------+-----------+----------------+\n id|Gender| Occupation|swimTimeInSecond|\n+---+------+-----------+----------------+\nid1|  Male| Programmer|           16.73|\nid2|Female|    Manager|           15.56|\nid3|  Male|    Manager|           15.15|\nid4|  Male|RiskAnalyst|           15.27|\n+---+------+-----------+----------------+\nonly showing top 4 rows\n\nroot\n-- id: string (nullable = true)\n-- Gender: string (nullable = true)\n-- Occupation: string (nullable = true)\n-- swimTimeInSecond: double (nullable = true)\n\n+---+------+-----------+----------------+\n id|Gender| Occupation|swimTimeInSecond|\n+---+------+-----------+----------------+\nid1|  Male| Programmer|           16.73|\nid2|Female|    Manager|           15.56|\nid3|  Male|    Manager|           15.15|\nid4|  Male|RiskAnalyst|           15.27|\nid5|  Male| Programmer|           15.65|\nid6|  Male|RiskAnalyst|           15.74|\n+---+------+-----------+----------------+\nonly showing top 6 rows\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["from pyspark.sql.types import *\ncorr_df=spark.read.json(path='/FileStore/tables/corrData.json')\ncorr_df.show(6)\niv1Column=StructField(\"iv1\",DoubleType(),True)\niv2Column=StructField(\"iv2\",DoubleType(),True)\niv3Column=StructField(\"iv3\",DoubleType(),True)\n\ncolumnList=[iv1Column,iv2Column,iv3Column]\ncorr_df_schema = StructType(columnList)\n\ncorr_df1=spark.read.json(path='/FileStore/tables/corrData.json',schema=corr_df_schema)\ncorr_df1.printSchema()\ncorr_df2=spark.read.json(path='/FileStore/tables/corrData.json')\ncorr_df2.printSchema()\n\ncorr_df2.write.csv(path='/FileStore/tables/corr_df4/',header=True)\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+-----+\n iv1| iv2|  iv3|\n+----+----+-----+\n 5.5| 8.5|  9.5|\n6.13|9.13|10.13|\n5.92|8.92| 9.92|\n6.89|9.89|10.89|\n6.12|9.12|10.12|\n+----+----+-----+\n\nroot\n-- iv1: double (nullable = true)\n-- iv2: double (nullable = true)\n-- iv3: double (nullable = true)\n\nroot\n-- iv1: double (nullable = true)\n-- iv2: double (nullable = true)\n-- iv3: double (nullable = true)\n\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["%sh ls -lrt /FileStore/tables/corr_df2/part-00000-tid-7581461481460486349-34bfd5a0-707d-4458-96f4-78f406e1ec11-63-1-c000.csv"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">ls: cannot access &#39;/FileStore/tables/corr_df2/part-00000-tid-7581461481460486349-34bfd5a0-707d-4458-96f4-78f406e1ec11-63-1-c000.csv&#39;: No such file or directory\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["%fs head dbfs:/FileStore/tables/corr_df3/part-00000-tid-1600818275588989250-b0d8c1a6-c7a6-46a1-aa4f-a70b0f7bd4d6-8-1-c000.csv"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">iv1,iv2,iv3\n5.5,8.5,9.5\n6.13,9.13,10.13\n5.92,8.92,9.92\n6.89,9.89,10.89\n6.12,9.12,10.12\n\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["from pyspark.sql.types import *\nidColumn=StructField(\"id\",StringType(),True)\ngenderColumn=StructField(\"gender\",StringType(),True)\nOccupationColumn=StructField(\"Occupation\",StringType(),True)\nswimTimeInSecondColumn=StructField(\"swimTimeInSecond\",DoubleType(),True)\n\ncolumnList=[idColumn,genderColumn,OccupationColumn,swimTimeInSecondColumn]\nswimmerDfSchema=StructType(columnList)\ndf_swimmers=spark.read.csv('/FileStore/tables/swimmerData.csv',header=True,schema=swimmerDfSchema)\n\n# df_swimmers.write.json(path='/FileStore/tables/swimmersJson/')\ndf_swimmers.write.csv('/FileStore/tables/swimmersDf1/',header=True,sep=',')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["%fs head /FileStore/tables/swimmersJson/part-00000-tid-5570975838623073821-876a6066-2da5-414b-bdd3-988dbe5ac678-9-1-c000.json"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{&quot;id&quot;:&quot;id1&quot;,&quot;gender&quot;:&quot;Male&quot;,&quot;Occupation&quot;:&quot;Programmer&quot;,&quot;swimTimeInSecond&quot;:16.73}\n{&quot;id&quot;:&quot;id2&quot;,&quot;gender&quot;:&quot;Female&quot;,&quot;Occupation&quot;:&quot;Manager&quot;,&quot;swimTimeInSecond&quot;:15.56}\n{&quot;id&quot;:&quot;id3&quot;,&quot;gender&quot;:&quot;Male&quot;,&quot;Occupation&quot;:&quot;Manager&quot;,&quot;swimTimeInSecond&quot;:15.15}\n{&quot;id&quot;:&quot;id4&quot;,&quot;gender&quot;:&quot;Male&quot;,&quot;Occupation&quot;:&quot;RiskAnalyst&quot;,&quot;swimTimeInSecond&quot;:15.27}\n{&quot;id&quot;:&quot;id5&quot;,&quot;gender&quot;:&quot;Male&quot;,&quot;Occupation&quot;:&quot;Programmer&quot;,&quot;swimTimeInSecond&quot;:15.65}\n{&quot;id&quot;:&quot;id6&quot;,&quot;gender&quot;:&quot;Male&quot;,&quot;Occupation&quot;:&quot;RiskAnalyst&quot;,&quot;swimTimeInSecond&quot;:15.74}\n{&quot;id&quot;:&quot;id7&quot;,&quot;gender&quot;:&quot;Female&quot;,&quot;Occupation&quot;:&quot;Programmer&quot;,&quot;swimTimeInSecond&quot;:16.8}\n{&quot;id&quot;:&quot;id8&quot;,&quot;gender&quot;:&quot;Male&quot;,&quot;Occupation&quot;:&quot;Manager&quot;,&quot;swimTimeInSecond&quot;:17.11}\n{&quot;id&quot;:&quot;id9&quot;,&quot;gender&quot;:&quot;Female&quot;,&quot;Occupation&quot;:&quot;Programmer&quot;,&quot;swimTimeInSecond&quot;:16.83}\n{&quot;id&quot;:&quot;id10&quot;,&quot;gender&quot;:&quot;Female&quot;,&quot;Occupation&quot;:&quot;RiskAnalyst&quot;,&quot;swimTimeInSecond&quot;:16.34}\n{&quot;id&quot;:&quot;id11&quot;,&quot;gender&quot;:&quot;Male&quot;,&quot;Occupation&quot;:&quot;Programmer&quot;,&quot;swimTimeInSecond&quot;:15.96}\n{&quot;id&quot;:&quot;id12&quot;,&quot;gender&quot;:&quot;Female&quot;,&quot;Occupation&quot;:&quot;RiskAnalyst&quot;,&quot;swimTimeInSecond&quot;:15.9}\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["df_orc_temp=spark.read.orc(path='/FileStore/tables/000000_0')\ndf_orc_temp.show()\nfrom pyspark.sql.types import *\nidColumn=StructField(\"id\",StringType(),True)\ngenderColumn=StructField(\"gender\",StringType(),True)\nOccupationColumn=StructField(\"Occupation\",StringType(),True)\nswimTimeInSecondColumn=StructField(\"swimTimeInSecond\",DoubleType(),True)\n\ncolumnList=[idColumn,genderColumn,OccupationColumn,swimTimeInSecondColumn]\nswimmerDfSchema=StructType(columnList)\ndf_swimmers=spark.read.csv('/FileStore/tables/swimmerData.csv',header=True,schema=swimmerDfSchema)\ndf_swimmers.show()\n# df_swimmers.write.orc(path='/FileStore/tables/duplicateDate/swimmers/')\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o1323.orc.\n: org.apache.spark.sql.AnalysisException: path dbfs:/FileStore/tables/duplicateDate/swimmers already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:148)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:126)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:146)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:134)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:187)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:183)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:134)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:115)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:710)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:710)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:240)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:97)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:170)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:710)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:306)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:292)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:235)\n\tat org.apache.spark.sql.DataFrameWriter.orc(DataFrameWriter.scala:622)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-707413121847363&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     11</span> df_swimmers<span class=\"ansi-blue-fg\">=</span>spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>csv<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;/FileStore/tables/swimmerData.csv&#39;</span><span class=\"ansi-blue-fg\">,</span>header<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">,</span>schema<span class=\"ansi-blue-fg\">=</span>swimmerDfSchema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     12</span> df_swimmers<span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">---&gt; 13</span><span class=\"ansi-red-fg\"> </span>df_swimmers<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>orc<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;/FileStore/tables/duplicateDate/swimmers/&#39;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     14</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">orc</span><span class=\"ansi-blue-fg\">(self, path, mode, partitionBy, compression)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    958</span>             self<span class=\"ansi-blue-fg\">.</span>partitionBy<span class=\"ansi-blue-fg\">(</span>partitionBy<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    959</span>         self<span class=\"ansi-blue-fg\">.</span>_set_opts<span class=\"ansi-blue-fg\">(</span>compression<span class=\"ansi-blue-fg\">=</span>compression<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 960</span><span class=\"ansi-red-fg\">         </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>orc<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    961</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    962</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1.4</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#39;path dbfs:/FileStore/tables/duplicateDate/swimmers already exists.;&#39;</div>"]}}],"execution_count":19},{"cell_type":"code","source":["%sql select * from iv_data_orc;\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>iv1</th><th>iv2</th><th>iv3</th></tr></thead><tbody><tr><td>c1</td><td>d2</td><td>9.8</td></tr><tr><td>c1</td><td>d2</td><td>8.36</td></tr><tr><td>c1</td><td>d2</td><td>9.06</td></tr><tr><td>c1</td><td>d2</td><td>11.15</td></tr><tr><td>c1</td><td>d2</td><td>6.26</td></tr><tr><td>c2</td><td>d2</td><td>8.74</td></tr><tr><td>c2</td><td>d1</td><td>9.92</td></tr><tr><td>c2</td><td>d1</td><td>12.88</td></tr><tr><td>c2</td><td>d1</td><td>8.16</td></tr><tr><td>c2</td><td>d1</td><td>9.97</td></tr></tbody></table></div>"]}}],"execution_count":20},{"cell_type":"code","source":["from pyspark.sql import Row\nour_df=spark.createDataFrame([Row(id=0,value=21),Row(id=1,value=23),Row(id=2,value=24),Row(id=3,value=25)])\nour_df.show()\nour_df.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-----+\n id|value|\n+---+-----+\n  0|   21|\n  1|   23|\n  2|   24|\n  3|   25|\n+---+-----+\n\nroot\n-- id: long (nullable = true)\n-- value: long (nullable = true)\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["%sql CREATE TABLE filamenttable(filamenttype String, bulbpower String, lifeinhours float) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":22},{"cell_type":"code","source":["%sql \nDROP TABLE IF EXISTS filamenttable;\nCREATE TABLE filamenttable(filamenttype String, bulbpower String, lifeinhours float) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;\nLOAD DATA INPATH '/FileStore/tables/filamentData.csv' OVERWRITE INTO TABLE filamenttable;\nSELECT * from filamenttable;\nCREATE TABLE filamenttable_orc(filamenttype String, bulbpower String, lifeinhours float) STORED AS orc;\nINSERT INTO filamenttable_orc select * from filamenttable"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>filamenttype</th><th>bulbpower</th><th>lifeinhours</th></tr></thead><tbody><tr><td>'filamentA'</td><td>'100W'</td><td>605.0</td></tr><tr><td>'filamentB'</td><td>'100W'</td><td>683.0</td></tr><tr><td>'filamentB'</td><td>'100W'</td><td>691.0</td></tr><tr><td>'filamentB'</td><td>'200W'</td><td>561.0</td></tr><tr><td>'filamentA'</td><td>'200W'</td><td>530.0</td></tr><tr><td>'filamentA'</td><td>'100W'</td><td>619.0</td></tr><tr><td>'filamentB'</td><td>'100W'</td><td>686.0</td></tr><tr><td>'filamentB'</td><td>'200W'</td><td>600.0</td></tr><tr><td>'filamentB'</td><td>'100W'</td><td>696.0</td></tr><tr><td>'filamentA'</td><td>'200W'</td><td>579.0</td></tr><tr><td>'filamentA'</td><td>'200W'</td><td>520.0</td></tr><tr><td>'filamentA'</td><td>'100W'</td><td>622.0</td></tr><tr><td>'filamentA'</td><td>'100W'</td><td>668.0</td></tr><tr><td>'filamentB'</td><td>'200W'</td><td>569.0</td></tr><tr><td>'filamentB'</td><td>'200W'</td><td>555.0</td></tr><tr><td>'filamentA'</td><td>'200W'</td><td>541.0</td></tr></tbody></table></div>"]}}],"execution_count":23},{"cell_type":"code","source":["filamentDf=spark.table('learning_1.filamenttable')\nfilamentDf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+---------+-----------+\nfilamenttype|bulbpower|lifeinhours|\n+------------+---------+-----------+\n &#39;filamentA&#39;|   &#39;100W&#39;|      605.0|\n &#39;filamentB&#39;|   &#39;100W&#39;|      683.0|\n &#39;filamentB&#39;|   &#39;100W&#39;|      691.0|\n &#39;filamentB&#39;|   &#39;200W&#39;|      561.0|\n &#39;filamentA&#39;|   &#39;200W&#39;|      530.0|\n &#39;filamentA&#39;|   &#39;100W&#39;|      619.0|\n &#39;filamentB&#39;|   &#39;100W&#39;|      686.0|\n &#39;filamentB&#39;|   &#39;200W&#39;|      600.0|\n &#39;filamentB&#39;|   &#39;100W&#39;|      696.0|\n &#39;filamentA&#39;|   &#39;200W&#39;|      579.0|\n &#39;filamentA&#39;|   &#39;200W&#39;|      520.0|\n &#39;filamentA&#39;|   &#39;100W&#39;|      622.0|\n &#39;filamentA&#39;|   &#39;100W&#39;|      668.0|\n &#39;filamentB&#39;|   &#39;200W&#39;|      569.0|\n &#39;filamentB&#39;|   &#39;200W&#39;|      555.0|\n &#39;filamentA&#39;|   &#39;200W&#39;|      541.0|\n+------------+---------+-----------+\n\n</div>"]}}],"execution_count":24},{"cell_type":"code","source":["from pyspark.sql.functions import round;\nswimmers_df=spark.read.csv('/FileStore/tables/swimmerData.csv',header=True,inferSchema=True)\nswimmers_df.show()\nswimmers_df.printSchema()\nswimmers_df1=swimmers_df.withColumn('swimmerSpeed',20.0/swimmers_df.swimTimeInSecond)\nswimmers_df1.show()\nswimmers_df2=swimmers_df1.withColumn('swimmerSpeed',round(swimmers_df1.swimmerSpeed,3))\nswimmers_df2.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------+-----------+----------------+\n  id|Gender| Occupation|swimTimeInSecond|\n+----+------+-----------+----------------+\n id1|  Male| Programmer|           16.73|\n id2|Female|    Manager|           15.56|\n id3|  Male|    Manager|           15.15|\n id4|  Male|RiskAnalyst|           15.27|\n id5|  Male| Programmer|           15.65|\n id6|  Male|RiskAnalyst|           15.74|\n id7|Female| Programmer|            16.8|\n id8|  Male|    Manager|           17.11|\n id9|Female| Programmer|           16.83|\nid10|Female|RiskAnalyst|           16.34|\nid11|  Male| Programmer|           15.96|\nid12|Female|RiskAnalyst|            15.9|\n+----+------+-----------+----------------+\n\nroot\n-- id: string (nullable = true)\n-- Gender: string (nullable = true)\n-- Occupation: string (nullable = true)\n-- swimTimeInSecond: double (nullable = true)\n\n+----+------+-----------+----------------+------------------+\n  id|Gender| Occupation|swimTimeInSecond|      swimmerSpeed|\n+----+------+-----------+----------------+------------------+\n id1|  Male| Programmer|           16.73| 1.195457262402869|\n id2|Female|    Manager|           15.56|1.2853470437017995|\n id3|  Male|    Manager|           15.15|1.3201320132013201|\n id4|  Male|RiskAnalyst|           15.27| 1.309757694826457|\n id5|  Male| Programmer|           15.65|1.2779552715654952|\n id6|  Male|RiskAnalyst|           15.74|1.2706480304955527|\n id7|Female| Programmer|            16.8|1.1904761904761905|\n id8|  Male|    Manager|           17.11| 1.168907071887785|\n id9|Female| Programmer|           16.83|1.1883541295306002|\nid10|Female|RiskAnalyst|           16.34|1.2239902080783354|\nid11|  Male| Programmer|           15.96|1.2531328320802004|\nid12|Female|RiskAnalyst|            15.9|1.2578616352201257|\n+----+------+-----------+----------------+------------------+\n\n+----+------+-----------+----------------+------------+\n  id|Gender| Occupation|swimTimeInSecond|swimmerSpeed|\n+----+------+-----------+----------------+------------+\n id1|  Male| Programmer|           16.73|       1.195|\n id2|Female|    Manager|           15.56|       1.285|\n id3|  Male|    Manager|           15.15|        1.32|\n id4|  Male|RiskAnalyst|           15.27|        1.31|\n id5|  Male| Programmer|           15.65|       1.278|\n id6|  Male|RiskAnalyst|           15.74|       1.271|\n id7|Female| Programmer|            16.8|        1.19|\n id8|  Male|    Manager|           17.11|       1.169|\n id9|Female| Programmer|           16.83|       1.188|\nid10|Female|RiskAnalyst|           16.34|       1.224|\nid11|  Male| Programmer|           15.96|       1.253|\nid12|Female|RiskAnalyst|            15.9|       1.258|\n+----+------+-----------+----------------+------------+\n\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["\n#swimmers_df3=swimmers_df2.select('id','Occupation','swimmerSpeed')\n#swimmers_df3.show()\nswimmers_df4=swimmers_df2.select('id','Occupation','swimmerSpeed',swimmers_df2.swimmerSpeed*2)\nswimmers_df4.show()\nswimmers_df4=swimmers_df2.select('id','Occupation','swimmerSpeed',swimmers_df2.swimmerSpeed*2)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----------+------------+------------------+\n  id| Occupation|swimmerSpeed|(swimmerSpeed * 2)|\n+----+-----------+------------+------------------+\n id1| Programmer|       1.195|              2.39|\n id2|    Manager|       1.285|              2.57|\n id3|    Manager|        1.32|              2.64|\n id4|RiskAnalyst|        1.31|              2.62|\n id5| Programmer|       1.278|             2.556|\n id6|RiskAnalyst|       1.271|             2.542|\n id7| Programmer|        1.19|              2.38|\n id8|    Manager|       1.169|             2.338|\n id9| Programmer|       1.188|             2.376|\nid10|RiskAnalyst|       1.224|             2.448|\nid11| Programmer|       1.253|             2.506|\nid12|RiskAnalyst|       1.258|             2.516|\n+----+-----------+------------+------------------+\n\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":["#swimmers_df5=swimmers_df2.filter(swimmers_df2.Gender==\"Male\")\n#swimmers_df5.show()\nswimmers_df6=swimmers_df2.filter((swimmers_df2.Gender==\"Male\") & (swimmers_df2.Occupation==\"Programmer\"))\nswimmers_df6.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------+----------+----------------+------------+\n  id|Gender|Occupation|swimTimeInSecond|swimmerSpeed|\n+----+------+----------+----------------+------------+\n id1|  Male|Programmer|           16.73|       1.195|\n id5|  Male|Programmer|           15.65|       1.278|\nid11|  Male|Programmer|           15.96|       1.253|\n+----+------+----------+----------------+------------+\n\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":["swimmers_df4=swimmers_df2.drop(\"id\",\"Occupation\")\nswimmers_df4.show(3)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+----------------+------------+\nGender|swimTimeInSecond|swimmerSpeed|\n+------+----------------+------------+\n  Male|           16.73|       1.195|\nFemale|           15.56|       1.285|\n  Male|           15.15|        1.32|\n+------+----------------+------------+\nonly showing top 3 rows\n\n</div>"]}}],"execution_count":28},{"cell_type":"code","source":["def celciusToFahrenheit(temp):\n return ((temp*9.0/5.0)+32)\n\ndef labelTemperature(temp):\n  if temp > 12.9:\n    return \"High\"\n  else:\n    return \"Low\"\n  \n  \ncelciusToFahrenheit(12.2)\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\ntemp=spark.read.parquet('/user/hive/warehouse/learning_1.db/temp_parquet')\ntemp.show()\nudfCelciusToFahrenheit=udf(celciusToFahrenheit,DoubleType())\nudfLabelTemperature=udf(labelTemperature,StringType())\n\ntemp_2=temp.withColumn('tempInFahrenheit',round(udfCelciusToFahrenheit(temp.tempInCelcius),3))\ntemp_2.show()\ntemp_3=temp_2.withColumn('label',udfLabelTemperature(temp_2.tempInCelcius))\ntemp_3.show()\n# temp_2.write.csv('/FileStore/tables/Temp1/',header=True,sep=',')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-------------+\n  day|tempInCelcius|\n+-----+-------------+\n day1|         7.94|\n day2|        20.61|\n day3|        11.06|\n day4|        19.15|\n day5|        19.66|\n day6|        10.87|\n day7|        18.54|\n day8|          1.2|\n day9|        23.87|\nday10|         14.7|\n+-----+-------------+\n\n+-----+-------------+----------------+\n  day|tempInCelcius|tempInFahrenheit|\n+-----+-------------+----------------+\n day1|         7.94|          46.292|\n day2|        20.61|          69.098|\n day3|        11.06|          51.908|\n day4|        19.15|           66.47|\n day5|        19.66|          67.388|\n day6|        10.87|          51.566|\n day7|        18.54|          65.372|\n day8|          1.2|           34.16|\n day9|        23.87|          74.966|\nday10|         14.7|           58.46|\n+-----+-------------+----------------+\n\n+-----+-------------+----------------+-----+\n  day|tempInCelcius|tempInFahrenheit|label|\n+-----+-------------+----------------+-----+\n day1|         7.94|          46.292|  Low|\n day2|        20.61|          69.098| High|\n day3|        11.06|          51.908|  Low|\n day4|        19.15|           66.47| High|\n day5|        19.66|          67.388| High|\n day6|        10.87|          51.566|  Low|\n day7|        18.54|          65.372| High|\n day8|          1.2|           34.16|  Low|\n day9|        23.87|          74.966| High|\nday10|         14.7|           58.46| High|\n+-----+-------------+----------------+-----+\n\n</div>"]}}],"execution_count":29},{"cell_type":"code","source":["from pyspark.sql.functions import *\ncorr_df=spark.read.json(path='/FileStore/tables/corrData.json')\ncorr_df.show()\n\n\n# corr_df.withColumn(\"iv2_avg\",corr_df.agg({str(corr_df.iv2):\"avg\"})).show()\ncorr_df.agg({\"iv1\":\"max\",\"iv2\":\"count\",\"iv3\":\"var_samp\"}).show()\n\ncorr_df.cov('iv1','iv2')\ncorr_df.cov('iv1','iv3')\n#corr_df.cor(iv1,iv2)\n#corr_df.cor('iv1','iv3')\n#corr_df.cor('iv3','iv2')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+-----+\n iv1| iv2|  iv3|\n+----+----+-----+\n 5.5| 8.5|  9.5|\n6.13|9.13|10.13|\n5.92|8.92| 9.92|\n6.89|9.89|10.89|\n6.12|9.12|10.12|\n+----+----+-----+\n\n+----------+--------+------------------+\ncount(iv2)|max(iv1)|     var_samp(iv3)|\n+----------+--------+------------------+\n         5|    6.89|0.2542699999999997|\n+----------+--------+------------------+\n\nOut[62]: 0.2542699999999998</div>"]}}],"execution_count":30},{"cell_type":"code","source":["\n\n\ncorr_df.cov('iv1','iv3')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[64]: 0.2542699999999996</div>"]}}],"execution_count":31},{"cell_type":"code","source":["corr_df.cov('iv2','iv3')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[65]: 0.2542699999999996</div>"]}}],"execution_count":32},{"cell_type":"code","source":["# corr_df.corr('iv2','iv3')\n#corr_df.describe().show()\n#corr_df.summary().show()\n#corr_df.show()\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import *\n\ndef labelIt(temp):\n  if temp>10.0:\n    return \"High\"\n  else:\n    return \"Low\"\n  \nudfLabelit=udf(labelIt,StringType())\ncorr_df1=corr_df.withColumn(\"label\",udfLabelit(corr_df.iv3))\ncorr_df1.summary([\"mean\",\"min\",\"max\"]).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----+-----+------+-----+\nsummary|  iv1|  iv2|   iv3|label|\n+-------+-----+-----+------+-----+\n   mean|6.112|9.112|10.112| null|\n    min|  5.5|  8.5|   9.5| High|\n    max| 6.89| 9.89| 10.89|  Low|\n+-------+-----+-----+------+-----+\n\n</div>"]}}],"execution_count":33},{"cell_type":"code","source":["swimmerdf=spark.read.csv(\"/FileStore/tables/swimmerData.csv\",header=True)\n#swimmerdf.printSchema()\n\n#swimmerdf.orderBy(\"swimTimeInSecond\").show()\n#swimmerdf.sort(\"swimTimeInSecond\",ascending=False).show()\n#swimmerdf.sort(\"Gender\",\"swimTimeInSecond\",ascending=[True,False]).show()\n#swimmerdf.orderBy(\"gender\",\"swimTimeInSecond\",ascending=[True,False]).show()\n\n\ntemp=swimmerdf.rdd.getNumPartitions()\n#type(temp)\nswimmerdf1=swimmerdf.repartition(3)\nswimmerdf1.rdd.getNumPartitions()\nswimmerdf1.show()\nswimmerdf1.rdd.glom().collect()\nswimmerdf1.sortWithinPartitions(\"gender\",\"swimTimeInSecond\",ascending=[True,False]).show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------+-----------+----------------+\n  id|Gender| Occupation|swimTimeInSecond|\n+----+------+-----------+----------------+\n id5|  Male| Programmer|           15.65|\n id3|  Male|    Manager|           15.15|\nid10|Female|RiskAnalyst|           16.34|\n id8|  Male|    Manager|           17.11|\n id9|Female| Programmer|           16.83|\n id6|  Male|RiskAnalyst|           15.74|\nid11|  Male| Programmer|           15.96|\n id4|  Male|RiskAnalyst|           15.27|\nid12|Female|RiskAnalyst|            15.9|\n id7|Female| Programmer|            16.8|\n id1|  Male| Programmer|           16.73|\n id2|Female|    Manager|           15.56|\n+----+------+-----------+----------------+\n\n+----+------+-----------+----------------+\n  id|Gender| Occupation|swimTimeInSecond|\n+----+------+-----------+----------------+\nid10|Female|RiskAnalyst|           16.34|\n id8|  Male|    Manager|           17.11|\n id5|  Male| Programmer|           15.65|\n id3|  Male|    Manager|           15.15|\n id9|Female| Programmer|           16.83|\nid11|  Male| Programmer|           15.96|\n id6|  Male|RiskAnalyst|           15.74|\n id4|  Male|RiskAnalyst|           15.27|\n id7|Female| Programmer|            16.8|\nid12|Female|RiskAnalyst|            15.9|\n id2|Female|    Manager|           15.56|\n id1|  Male| Programmer|           16.73|\n+----+------+-----------+----------------+\n\n</div>"]}}],"execution_count":34},{"cell_type":"code","source":["iv_data_2=spark.read.csv('/FileStore/tables/iv_data_2.csv',inferSchema=True)\n# iv_data_2.show()\n# iv_data_2.count()\n# iv_data_2.drop_duplicates().count()\n\n# swimmerdf.show()\n# swimmerdf.dropDuplicates().show()\n#swimmerdf.dropDuplicates([\"Gender\",\"Occupation\"]).show()\n\n#iv_data_2.drop_duplicates().show()\n#iv_data_2.drop_duplicates([\"_c0\"]).show()                        \n#iv_data_2.drop_duplicates([\"_c0\",\"_c1\"]).show()                       \niv_data_2.freqItems(cols=[\"_c0\",\"_c1\"]).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+-------------+\n_c0_freqItems|_c1_freqItems|\n+-------------+-------------+\n     [c1, c2]|     [d2, d1]|\n+-------------+-------------+\n\n</div>"]}}],"execution_count":35},{"cell_type":"code","source":["swimmersdf=spark.read.format(\"csv\").options(header=True,inferSchema=True).load(\"/FileStore/tables/swimmerData.csv\")\n#swimmersdf.show()\nswimmersdf.show()\n# Example of Aggregation\n#swimmersdf.filter(swimmersdf.Gender==\"Male\").agg({\"swimTimeInSecond\":\"max\"}).show()\n#swimmersdf.filter(swimmersdf.Gender==\"Female\").agg({\"swimTimeInSecond\":\"max\"}).show()\n#swimmersdf.filter(swimmersdf.Gender==\"Male\").agg({\"swimTimeInSecond\":\"min\"}).show()\n#swimmersdf.filter(swimmersdf.Gender==\"Female\").agg({\"swimTimeInSecond\":\"min\"}).show()\n\n# Find Male Swimmers who took more time (Sorted by most swimTime taken)\n#swimmersdf.filter(swimmersdf.Gender==\"Male\").sort(\"Occupation\",\"swimTimeInSecond\",ascending=[True,False]).show()\n# Find Female Swimmers who took more time (Sorted by most swimTime taken)\nswimmersdf.filter(swimmersdf.Gender==\"Female\").sort(\"Occupation\",\"swimTimeInSecond\",ascending=[True,False]).show()\n# Find Male Swimmers who took least time (sort by least swimTime taken)\n#swimmersdf.filter(swimmersdf.Gender==\"Male\").sort(\"Occupation\",\"swimTimeInSecond\",ascending=[True,False]).show()\n# Find Female Swimmers who took least time (Sort by least swimTime taken)\n#swimmersdf.filter(swimmersdf.Gender==\"Female\".sort(\"Occupation\",\"swimTimeInSecond\",ascending=[True,False]).show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------+-----------+----------------+\n  id|Gender| Occupation|swimTimeInSecond|\n+----+------+-----------+----------------+\n id1|  Male| Programmer|           16.73|\n id2|Female|    Manager|           15.56|\n id3|  Male|    Manager|           15.15|\n id4|  Male|RiskAnalyst|           15.27|\n id5|  Male| Programmer|           15.65|\n id6|  Male|RiskAnalyst|           15.74|\n id7|Female| Programmer|            16.8|\n id8|  Male|    Manager|           17.11|\n id9|Female| Programmer|           16.83|\nid10|Female|RiskAnalyst|           16.34|\nid11|  Male| Programmer|           15.96|\nid12|Female|RiskAnalyst|            15.9|\n+----+------+-----------+----------------+\n\n+----+------+-----------+----------------+\n  id|Gender| Occupation|swimTimeInSecond|\n+----+------+-----------+----------------+\n id2|Female|    Manager|           15.56|\n id9|Female| Programmer|           16.83|\n id7|Female| Programmer|            16.8|\nid10|Female|RiskAnalyst|           16.34|\nid12|Female|RiskAnalyst|            15.9|\n+----+------+-----------+----------------+\n\n</div>"]}}],"execution_count":36},{"cell_type":"code","source":["UCBAdmissions=spark.read.format(\"csv\").options(header=True).load('/FileStore/tables/UCBAdmissions.csv')\n#UCBAdmissions.printSchema()\nfrom pyspark.sql.functions import *\nUCBAdmission_1=UCBAdmissions.select(\"Admit\",\"Gender\",\"Dept\",\"Freq\")\nUCBAdmission_1.registerTempTable(\"UCBAdmission_1\")\nUCBAdmission_2=spark.sql(\"SELECT Admit,Gender, Dept,int(Freq) as Freq from UCBAdmission_1\")\n\n#UCBAdmissions.printSchema()\n#UCBAdmissions.filter(UCBAdmissions.Gender==\"Male\").agg({\"Freq\":\"mean\"}).show() \n#UCBAdmissions.show() \n#temp=UCBAdmissions.groupBy(['Admit']).agg({\"Freq\":\"mean\"})\n#temp.show()\n#temp1=UCBAdmissions.groupBy(['Admit','Gender']).agg({\"Freq\":\"mean\"})\n# temp1.columns=['Admit','Gender','Avg.Freq']\n#temp1.show()\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n#UCBAdmission_2.groupBy(['Admit']).agg({\"Freq.toInt()\":\"avg\"}).show()\n#UCBAdmission_2.groupBy(['Admit','Gender']).agg({\"Freq\":\"avg\"}).show()\n#UCBAdmission_2.groupBy(['Admit','Gender']).agg({\"Freq\":\"min\"}).show()\n#swimmersdf=spark.read.format(\"csv\").options(header=True,inferSchema=True).load(\"/FileStore/tables/swimmerData.csv\").select(\"Occupation\",\"Gender\")\nspark.read.format(\"csv\").options(header=True,inferSchema=True).load(\"/FileStore/tables/swimmerData.csv\").show()\n#swimmersdf.crosstab(\"Gender\",\"Occupation\").show()\n\n#temp1=spark.sql(\"SELECT Admit,Gender, min(int(Freq)) as MIN_Freq,max(int(Freq)) as MAX_Freq from UCBAdmission_1 group by Admit,Gender\")\n#temp1.show()\n#UCBAdmission_2.show()\n\n\n\n#  UCBAdmission_1.select(\"Admit\",\"Gender\",int(\"Freq\")).groupBy(['Admit','Gender']).agg({\"Freq\":\"min\"}).show()\n\n\n\n#UCBAdmission_1.show()\n# UCBAdmission_1.groupBy(['Admit','Gender']).agg({\"Freq\":\"max\"}).show()\n\n##UCBAdmission_1.filter((UCBAdmission_1.Admit==\"Admitted\") & (UCBAdmission_1.Gender==\"Female\")).agg({\"Freq\":\"max\"}).show() # ADMITTED FEMALE correct\n#UCBAdmission_1.filter((UCBAdmission_1.Admit==\"Admitted\") & (UCBAdmission_1.Gender==\"Male\")).agg({\"Freq\":\"max\"}).alias(\"Admitted Male\").show()   # ADMITTED MALE correct\n#UCBAdmission_1.filter((UCBAdmission_1.Admit==\"Rejected\") & (UCBAdmission_1.Gender==\"Female\")).agg({\"Freq\":\"max\"}).show()  # REJECTED FEMALE  correct\n#UCBAdmission_1.filter((UCBAdmission_1.Admit==\"Rejected\") & (UCBAdmission_1.Gender==\"Male\")).agg({\"Freq\":\"max\"}).show()    # REJECTED MALE     correct\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------+-----------+----------------+\n  id|Gender| Occupation|swimTimeInSecond|\n+----+------+-----------+----------------+\n id1|  Male| Programmer|           16.73|\n id2|Female|    Manager|           15.56|\n id3|  Male|    Manager|           15.15|\n id4|  Male|RiskAnalyst|           15.27|\n id5|  Male| Programmer|           15.65|\n id6|  Male|RiskAnalyst|           15.74|\n id7|Female| Programmer|            16.8|\n id8|  Male|    Manager|           17.11|\n id9|Female| Programmer|           16.83|\nid10|Female|RiskAnalyst|           16.34|\nid11|  Male| Programmer|           15.96|\nid12|Female|RiskAnalyst|            15.9|\n+----+------+-----------+----------------+\n\n</div>"]}}],"execution_count":37},{"cell_type":"code","source":["from pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nsubjects=spark.read.csv(\"/FileStore/tables/Subjects_1.csv\",header=True)\nstudent=spark.read.csv(\"/FileStore/tables/studentData_dateofbirth.csv\",header=True)\nattendance=spark.read.csv(\"/FileStore/tables/attendance.csv\",header=True)\nattendance.printSchema()\nsubjects.printSchema()\nstudent.printSchema()\nattendance.createOrReplaceTempView(\"attendance\")\nsubjects.createOrReplaceTempView(\"subjects\")\nstudent.createOrReplaceTempView(\"students\")\ndef genderCode(gender):\n\treturn ('MALE' if gender=='M' else 'FEMALE' if gender=='F' else 'NA')\n\nspark.udf.register('genderCode',genderCode,StringType())\t\nspark.sql(\"SELECT st.name,genderCode(st.gender) as gender,to_date(st.dateofbirth) as dob,sb.marks,sb.subject,at.attendance FROM students st \\\n          INNER JOIN subjects sb \\\n          on st.studentId=sb.studentId \\\n          INNER JOIN attendance at  \\\n          on st.studentId=at.studentId AND  \\\n             sb.subject=at.subject\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- studentId: string (nullable = true)\n-- subject: string (nullable = true)\n-- attendance: string (nullable = true)\n\nroot\n-- studentId: string (nullable = true)\n-- subject: string (nullable = true)\n-- marks: string (nullable = true)\n\nroot\n-- studentId: string (nullable = true)\n-- name: string (nullable = true)\n-- gender: string (nullable = true)\n-- dateofbirth: string (nullable = true)\n\n+-----+------+----------+-----+-------+----------+\n name|gender|       dob|marks|subject|attendance|\n+-----+------+----------+-----+-------+----------+\nRobin|  MALE|1981-12-13|   81|   Java|        34|\nRobin|  MALE|1981-12-13|   75| Python|        30|\nMaria|FEMALE|1986-06-06|   83|   Java|        39|\nMaria|FEMALE|1986-06-06|   85| Python|        39|\nJulie|FEMALE|1988-09-05|   72|   Ruby|        25|\nJulie|FEMALE|1988-09-05|   76|   Java|        22|\n  Bob|  MALE|1987-05-04|   84| Python|        39|\n  Bob|  MALE|1987-05-04|   78|    C++|        38|\n+-----+------+----------+-----+-------+----------+\n\n</div>"]}}],"execution_count":38},{"cell_type":"code","source":["dfOne=spark.read.format(\"csv\").options(header=True,inferSchema=True).load('/FileStore/tables/dfOne.csv')\ndfTwo=spark.read.format(\"csv\").options(header=True,inferSchema=True).load('/FileStore/tables/dfTwo.csv')\n# dfOne.printSchema()\n# dfTwo.printSchema()\ndfOne.show()\ndfTwo.show()\ndfOne.union(dfTwo).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----+-----+\n  iv1|  iv2|  iv3|\n+-----+-----+-----+\n  9.0|11.43|10.25|\n10.26| 8.35| 9.94|\n 9.84| 9.28| 9.22|\n11.77|10.18|11.02|\n+-----+-----+-----+\n\n+-----+-----+-----+\n  iv1|  iv2|  iv3|\n+-----+-----+-----+\n 11.0|12.64|12.18|\n12.26|10.84|12.19|\n11.84|13.43| 11.6|\n+-----+-----+-----+\n\n+-----+-----+-----+\n  iv1|  iv2|  iv3|\n+-----+-----+-----+\n  9.0|11.43|10.25|\n10.26| 8.35| 9.94|\n 9.84| 9.28| 9.22|\n11.77|10.18|11.02|\n 11.0|12.64|12.18|\n12.26|10.84|12.19|\n11.84|13.43| 11.6|\n+-----+-----+-----+\n\n</div>"]}}],"execution_count":39},{"cell_type":"code","source":["from pyspark.sql.functions import monotonically_increasing_id\ndfOne=spark.read.format(\"csv\").options(header=True,inferSchema=True).load(\"/FileStore/tables/dfOne.csv\")\ndfTwo=spark.read.csv('/FileStore/tables/dfTwo.csv',header=True,inferSchema=True)\ndfOne_with_id=dfOne.withColumn('id',monotonically_increasing_id())\ndfOne_with_id.show()\n\ndfTwo_with_id=dfTwo.withColumn('id',monotonically_increasing_id())\ndfTwo_with_id.show()\na=dfOne_with_id.alias('dfOne_with_id')\nb=dfTwo_with_id.alias('dfTwo_with_id')\na.join(b,a.id==b.id,\"inner\").show()\ndfOne_with_id.alias('a').join(dfTwo_with_id.alias('b'),a.id==b.id,\"inner\").drop('id').show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----+-----+---+\n  iv1|  iv2|  iv3| id|\n+-----+-----+-----+---+\n  9.0|11.43|10.25|  0|\n10.26| 8.35| 9.94|  1|\n 9.84| 9.28| 9.22|  2|\n11.77|10.18|11.02|  3|\n+-----+-----+-----+---+\n\n+-----+-----+-----+---+\n  iv1|  iv2|  iv3| id|\n+-----+-----+-----+---+\n 11.0|12.64|12.18|  0|\n12.26|10.84|12.19|  1|\n11.84|13.43| 11.6|  2|\n+-----+-----+-----+---+\n\n+-----+-----+-----+---+-----+-----+-----+---+\n  iv1|  iv2|  iv3| id|  iv1|  iv2|  iv3| id|\n+-----+-----+-----+---+-----+-----+-----+---+\n  9.0|11.43|10.25|  0| 11.0|12.64|12.18|  0|\n10.26| 8.35| 9.94|  1|12.26|10.84|12.19|  1|\n 9.84| 9.28| 9.22|  2|11.84|13.43| 11.6|  2|\n+-----+-----+-----+---+-----+-----+-----+---+\n\n+-----+-----+-----+-----+-----+-----+\n  iv1|  iv2|  iv3|  iv1|  iv2|  iv3|\n+-----+-----+-----+-----+-----+-----+\n  9.0|11.43|10.25| 11.0|12.64|12.18|\n10.26| 8.35| 9.94|12.26|10.84|12.19|\n 9.84| 9.28| 9.22|11.84|13.43| 11.6|\n+-----+-----+-----+-----+-----+-----+\n\n</div>"]}}],"execution_count":40},{"cell_type":"code","source":["from pyspark.sql.functions import monotonically_increasing_id\n#missingDf=spark.read.csv('/FileStore/tables/dfAmput.csv',header=True,inferSchema=True).withColumn('id',monotonically_increasing_id())\n#missingDf.show()\n#missingDf.fillna(value=0).show()\t\t\t\t\t\t\t\t\t\t  \n#spark.tableName()\nfrom pyspark import SQLContext\nsqlcontext=SQLContext(sc)\n\nstudentsdf=spark.read.csv('/FileStore/tables/students.csv',header=True,inferSchema=True)\nstudentsdf.createOrReplaceTempView('students')\nsqlcontext.tableNames()\nspark.sql(\"select * from students\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+------+-------+\nstudentid|gender|   name|\n+---------+------+-------+\n      si3|     F|  Julie|\n      si2|     F|  Maria|\n      si1|     M|  Robin|\n      si6|     M|William|\n      si4|     M|    Bob|\n+---------+------+-------+\n\n</div>"]}}],"execution_count":41},{"cell_type":"code","source":["from pyspark.sql.types import *\nstudents=spark.read.csv('/FileStore/tables/studentData.csv',header=True,inferSchema=True)\nstudents.show()\nStudentIDColumn=StructField(\"StudentID\",StringType(),True)\nNameColumn=StructField(\"Name\",StringType(),True)\nGenderColumn=StructField(\"Gender\",StringType(),True)\ncolumnList=[StudentIDColumn,NameColumn,GenderColumn]\nstudents_schema=StructType(columnList)\nstudents_new=spark.createDataFrame(students.rdd,schema=students_schema)\nstudents_new.show()\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+-------+------+\nStudentId|   name|gender|\n+---------+-------+------+\n      si1|  Robin|     M|\n      si2|  Maria|     F|\n      si3|  Julie|     F|\n      si4|    Bob|     M|\n      si6|William|     M|\n+---------+-------+------+\n\n+---------+-------+------+\nStudentID|   Name|Gender|\n+---------+-------+------+\n      si1|  Robin|     M|\n      si2|  Maria|     F|\n      si3|  Julie|     F|\n      si4|    Bob|     M|\n      si6|William|     M|\n+---------+-------+------+\n\n</div>"]}}],"execution_count":42},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\ncorr_data=spark.read.format(\"json\").options(header=True,inferSchema=True).load('/FileStore/tables/corrData.json')\n#corr_data.show()                                \n#corr_data.filter(corr_data.iv3 >= 10).show()\n\ndef getDataType(value):\n  return str(type(value))\n\ngetDataTypeudf=udf(getDataType,StringType())\n\ncorr_data.select(\"iv1\").withColumn(\"DataType\",getDataTypeudf(corr_data.iv1)).show()\n\n#corr_data.select(\"iv1\").withColumn(\"Datatype\",str(type(\"corr_data.iv1\"))).show()\n\n\n\n\n\n\ntemp=spark.read.csv('/FileStore/tables/temp2_.csv',header=True,inferSchema=True)\ntemp.registerTempTable(\"temp\")\ntemp1=spark.sql(\"select count(*) from temp\")\ntype(temp1)\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---------------+\n iv1|       DataType|\n+----+---------------+\n 5.5|&lt;class &#39;float&#39;&gt;|\n6.13|&lt;class &#39;float&#39;&gt;|\n5.92|&lt;class &#39;float&#39;&gt;|\n6.89|&lt;class &#39;float&#39;&gt;|\n6.12|&lt;class &#39;float&#39;&gt;|\n+----+---------------+\n\nOut[4]: pyspark.sql.dataframe.DataFrame</div>"]}}],"execution_count":43},{"cell_type":"code","source":["from pyspark.sql.types import * \nfrom pyspark.sql.functions import *\nfrom pyspark import SQLContext\n\nsqlcontext=SQLContext(sc)\n\n\nstudentDf=spark.read.csv(\"/FileStore/tables/studentData.csv\",header=True)\nprint(type(studentDf))\nstudentDf.printSchema()\n\nstudentDf.createOrReplaceTempView(\"studentDf\")\n#for i in sqlcontext.tableNames():\n # print(i)\n  \nspark.sql(\"SELECT * FROM studentDf\").show()\n#spark.sql(\"DESCRIBE studentDf\").show()\n#spark.sql(\"SELECT name,gender from studentDf\").show()\n#spark.sql(\"SELECT name as Name, gender as Sex from studentDf\").show()\nspark.sql(\"SELECT name as Name, gender as Sec, StudentId as StudentID from studentDf where gender==\\\"M\\\"\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &#39;pyspark.sql.dataframe.DataFrame&#39;&gt;\nroot\n-- StudentId: string (nullable = true)\n-- name: string (nullable = true)\n-- gender: string (nullable = true)\n\n+---------+-------+------+\nStudentId|   name|gender|\n+---------+-------+------+\n      si1|  Robin|     M|\n      si2|  Maria|     F|\n      si3|  Julie|     F|\n      si4|    Bob|     M|\n      si6|William|     M|\n+---------+-------+------+\n\n+-------+---+---------+\n   Name|Sec|StudentID|\n+-------+---+---------+\n  Robin|  M|      si1|\n    Bob|  M|      si4|\nWilliam|  M|      si6|\n+-------+---+---------+\n\n</div>"]}}],"execution_count":44},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import * \n\n# CREATING FUNCTION\ndef tempInFahrenheit(temp):\n\treturn (temp*(9/5))\n\n# CREATING UDF TO BE USED IN \nspark.udf.register(\"udfTempInFahrenheit\",tempInFahrenheit,DoubleType())\nspark.udf.register(\"udfTempInFahrenheitAlt\",lambda s:(s*(9/5)),DoubleType())\n\n# SCHEMA COLUMN DEFINITION\ndayColumn=StructField(\"Day\",StringType(), True)\ntempInCelciusColumn=StructField(\"TempInCelcius\",DoubleType(), True)\n\n\n# COLUMN_LIST\ncolumnList=[dayColumn,tempInCelciusColumn]\n\n# SCHEMA DEFINITION\ntempSchema=StructType(columnList)\n\ntemp=spark.read.csv(\"/FileStore/tables/tempratureData.csv\",schema=tempSchema)\ntemp.createOrReplaceTempView(\"temp\")\nspark.sql(\"SELECT Day,TempInCelcius,udfTempInFahrenheit(TempInCelcius) as TempInCelcius from temp\").show()\nspark.sql(\"SELECT Day,TempInCelcius,udfTempInFahrenheitAlt(TempInCelcius) as TempInCelcius from temp\").show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-------------+------------------+\n  Day|TempInCelcius|     TempInCelcius|\n+-----+-------------+------------------+\n day1|         7.94|14.292000000000002|\n day2|        20.61|            37.098|\n day3|        11.06|            19.908|\n day4|        19.15|             34.47|\n day5|        19.66|            35.388|\n day6|        10.87|            19.566|\n day7|        18.54|            33.372|\n day8|          1.2|              2.16|\n day9|        23.87|            42.966|\nday10|         14.7|             26.46|\n+-----+-------------+------------------+\n\n+-----+-------------+------------------+\n  Day|TempInCelcius|     TempInCelcius|\n+-----+-------------+------------------+\n day1|         7.94|14.292000000000002|\n day2|        20.61|            37.098|\n day3|        11.06|            19.908|\n day4|        19.15|             34.47|\n day5|        19.66|            35.388|\n day6|        10.87|            19.566|\n day7|        18.54|            33.372|\n day8|          1.2|              2.16|\n day9|        23.87|            42.966|\nday10|         14.7|             26.46|\n+-----+-------------+------------------+\n\n</div>"]}}],"execution_count":45},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n# from datetime import *\nstudent=spark.read.format(\"csv\").options(header=True,inferSchema=True).load(\"/FileStore/tables/studentData_dateofbirth.csv\")\nstudent.printSchema()\n'''\nstudent.createOrReplaceTempView(\"student\")\nstudent_df1=spark.sql(\"SELECT studentId,name,gender,to_date(dateofbirth) as dob from student\")\nstudent_df1.show()\nstudent_df1.createOrReplaceTempView(\"student_df1\")\nspark.sql(\"select studentId,name,gender,dayofmonth(dob),month(dob),year(dob) from student_df1\").show()\n'''\nstudent.select(\"studentId\",\"name\",\"gender\",to_date(\"dateofbirth\").alias(\"dob\"),\n               dayofmonth(to_date(\"dateofbirth\")).alias(\"day_of_month\"),\n                month(to_date(\"dateofbirth\")).alias(\"month\"),\n                year(to_date(\"dateofbirth\")).alias(\"year\")\n              ).show()\n\n#def genderCode(gender):\n#  if(gender==\"M\"):\n#    return \"Male\"\n#  elif (gender==\"F\"):\n#    return \"Female\"\n  \ndef genderCode(gender):\n  return ('FEMALE' if gender=='F' else 'MALE' if gender=='M' else 'NA')\n  \n\nspark.udf.register(\"genderCode\",genderCode,StringType())\nspark.sql(\"SELECT studentId,name,to_date(dateofbirth) as dob,genderCode(gender) as Gender from student\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- studentId: string (nullable = true)\n-- name: string (nullable = true)\n-- gender: string (nullable = true)\n-- dateofbirth: timestamp (nullable = true)\n\n+---------+-------+------+----------+------------+-----+----+\nstudentId|   name|gender|       dob|day_of_month|month|year|\n+---------+-------+------+----------+------------+-----+----+\n      si1|  Robin|     M|1981-12-13|          13|   12|1981|\n      si2|  Maria|     F|1986-06-06|           6|    6|1986|\n      si3|  Julie|     F|1988-09-05|           5|    9|1988|\n      si4|    Bob|     M|1987-05-04|           4|    5|1987|\n      si6|William|     M|1980-11-12|          12|   11|1980|\n+---------+-------+------+----------+------------+-----+----+\n\n+---------+-------+----------+------+\nstudentId|   name|       dob|Gender|\n+---------+-------+----------+------+\n      si1|  Robin|1981-12-13|  MALE|\n      si2|  Maria|1986-06-06|FEMALE|\n      si3|  Julie|1988-09-05|FEMALE|\n      si4|    Bob|1987-05-04|  MALE|\n      si6|William|1980-11-12|  MALE|\n+---------+-------+----------+------+\n\n</div>"]}}],"execution_count":46},{"cell_type":"code","source":["subjects=spark.read.csv(\"/FileStore/tables/Subjects_1.csv\",header=True)\nstudent=spark.read.csv(\"/FileStore/tables/studentData_dateofbirth.csv\",header=True)\nsubjects.printSchema()\nstudent.printSchema()\nsubjects.createOrReplaceTempView(\"subjects\")\nstudent.createOrReplaceTempView(\"students\")\nspark.sql(\"SELECT st.studentId,st.name,st.gender,st.dateofbirth,sb.marks from students st  JOIN  subjects sb ON st.studentId=sb.studentId\").show()\nspark.sql(\"SELECT st.studentId,st.name,st.gender,st.dateofbirth,sb.marks from students st LEFT  JOIN subjects sb ON st.studentId=sb.studentId\").show()\nspark.sql(\"SELECT st.studentId,st.name,st.gender,st.dateofbirth,sb.marks from students st RIGHT  JOIN subjects sb ON st.studentId=sb.studentId\").show()\nspark.sql(\"SELECT st.studentId,st.name,st.gender,st.dateofbirth,sb.marks from students st FULL  JOIN subjects sb ON st.studentId=sb.studentId\").show()\n          "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- studentId: string (nullable = true)\n-- subject: string (nullable = true)\n-- marks: string (nullable = true)\n\nroot\n-- studentId: string (nullable = true)\n-- name: string (nullable = true)\n-- gender: string (nullable = true)\n-- dateofbirth: string (nullable = true)\n\n+---------+-----+------+-----------+-----+\nstudentId| name|gender|dateofbirth|marks|\n+---------+-----+------+-----------+-----+\n      si1|Robin|     M| 1981-12-13|   81|\n      si1|Robin|     M| 1981-12-13|   75|\n      si2|Maria|     F| 1986-06-06|   83|\n      si2|Maria|     F| 1986-06-06|   85|\n      si3|Julie|     F| 1988-09-05|   72|\n      si3|Julie|     F| 1988-09-05|   76|\n      si4|  Bob|     M| 1987-05-04|   84|\n      si4|  Bob|     M| 1987-05-04|   78|\n+---------+-----+------+-----------+-----+\n\n+---------+-------+------+-----------+-----+\nstudentId|   name|gender|dateofbirth|marks|\n+---------+-------+------+-----------+-----+\n      si1|  Robin|     M| 1981-12-13|   81|\n      si1|  Robin|     M| 1981-12-13|   75|\n      si2|  Maria|     F| 1986-06-06|   83|\n      si2|  Maria|     F| 1986-06-06|   85|\n      si3|  Julie|     F| 1988-09-05|   72|\n      si3|  Julie|     F| 1988-09-05|   76|\n      si4|    Bob|     M| 1987-05-04|   84|\n      si4|    Bob|     M| 1987-05-04|   78|\n      si6|William|     M| 1980-11-12| null|\n+---------+-------+------+-----------+-----+\n\n+---------+-----+------+-----------+-----+\nstudentId| name|gender|dateofbirth|marks|\n+---------+-----+------+-----------+-----+\n      si1|Robin|     M| 1981-12-13|   75|\n      si3|Julie|     F| 1988-09-05|   76|\n      si1|Robin|     M| 1981-12-13|   81|\n      si2|Maria|     F| 1986-06-06|   85|\n      si3|Julie|     F| 1988-09-05|   72|\n      si4|  Bob|     M| 1987-05-04|   78|\n     null| null|  null|       null|   77|\n      si4|  Bob|     M| 1987-05-04|   84|\n      si2|Maria|     F| 1986-06-06|   83|\n+---------+-----+------+-----------+-----+\n\n+---------+-------+------+-----------+-----+\nstudentId|   name|gender|dateofbirth|marks|\n+---------+-------+------+-----------+-----+\n     null|   null|  null|       null|   77|\n      si2|  Maria|     F| 1986-06-06|   85|\n      si2|  Maria|     F| 1986-06-06|   83|\n      si4|    Bob|     M| 1987-05-04|   78|\n      si4|    Bob|     M| 1987-05-04|   84|\n      si3|  Julie|     F| 1988-09-05|   76|\n      si3|  Julie|     F| 1988-09-05|   72|\n      si6|William|     M| 1980-11-12| null|\n      si1|  Robin|     M| 1981-12-13|   75|\n      si1|  Robin|     M| 1981-12-13|   81|\n+---------+-------+------+-----------+-----+\n\n</div>"]}}],"execution_count":47},{"cell_type":"code","source":["from pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nsubjects=spark.read.csv(\"/FileStore/tables/Subjects_1.csv\",header=True)\nstudent=spark.read.csv(\"/FileStore/tables/studentData_dateofbirth.csv\",header=True)\nattendance=spark.read.csv(\"/FileStore/tables/attendance.csv\",header=True)\nprint(\"First Record: {0} , Total Count: {1}\".format(subjects.first(),subjects.count()))\n\n#subjects.first()\nattendance.printSchema()\nsubjects.printSchema()\nstudent.printSchema()\nattendance.createOrReplaceTempView(\"attendance\")\nsubjects.createOrReplaceTempView(\"subjects\")\nstudent.createOrReplaceTempView(\"students\")\ndef genderCode(gender):\n\treturn ('MALE' if gender=='M' else 'FEMALE' if gender=='F' else 'NA')\n\nspark.udf.register('genderCode',genderCode,StringType())\t\nspark.sql(\"SELECT st.name,genderCode(st.gender) as gender,to_date(st.dateofbirth) as dob,sb.marks,sb.subject,at.attendance FROM students st \\\n          INNER JOIN subjects sb \\\n          on st.studentId=sb.studentId \\\n          INNER JOIN attendance at  \\\n          on st.studentId=at.studentId AND  \\\n             sb.subject=at.subject\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">First Record: Row(studentId=&#39;si1&#39;, subject=&#39;Python&#39;, marks=&#39;75&#39;) , Total Count: 9\nroot\n-- studentId: string (nullable = true)\n-- subject: string (nullable = true)\n-- attendance: string (nullable = true)\n\nroot\n-- studentId: string (nullable = true)\n-- subject: string (nullable = true)\n-- marks: string (nullable = true)\n\nroot\n-- studentId: string (nullable = true)\n-- name: string (nullable = true)\n-- gender: string (nullable = true)\n-- dateofbirth: string (nullable = true)\n\n+-----+------+----------+-----+-------+----------+\n name|gender|       dob|marks|subject|attendance|\n+-----+------+----------+-----+-------+----------+\nRobin|  MALE|1981-12-13|   81|   Java|        34|\nRobin|  MALE|1981-12-13|   75| Python|        30|\nMaria|FEMALE|1986-06-06|   83|   Java|        39|\nMaria|FEMALE|1986-06-06|   85| Python|        39|\nJulie|FEMALE|1988-09-05|   72|   Ruby|        25|\nJulie|FEMALE|1988-09-05|   76|   Java|        22|\n  Bob|  MALE|1987-05-04|   84| Python|        39|\n  Bob|  MALE|1987-05-04|   78|    C++|        38|\n+-----+------+----------+-----+-------+----------+\n\n</div>"]}}],"execution_count":48},{"cell_type":"code","source":["print(attendance.rdd.getNumPartitions())\nattendance.coalesce(2)\nattendance.rdd.glom().collect()f"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">1\nOut[138]: [[Row(studentId=&#39;si1&#39;, subject=&#39;Python&#39;, attendance=&#39;30&#39;),\n  Row(studentId=&#39;si3&#39;, subject=&#39;Java&#39;, attendance=&#39;22&#39;),\n  Row(studentId=&#39;si1&#39;, subject=&#39;Java&#39;, attendance=&#39;34&#39;),\n  Row(studentId=&#39;si2&#39;, subject=&#39;Python&#39;, attendance=&#39;39&#39;),\n  Row(studentId=&#39;si3&#39;, subject=&#39;Ruby&#39;, attendance=&#39;25&#39;),\n  Row(studentId=&#39;si4&#39;, subject=&#39;C++&#39;, attendance=&#39;38&#39;),\n  Row(studentId=&#39;si5&#39;, subject=&#39;C&#39;, attendance=&#39;35&#39;),\n  Row(studentId=&#39;si4&#39;, subject=&#39;Python&#39;, attendance=&#39;39&#39;),\n  Row(studentId=&#39;si2&#39;, subject=&#39;Java&#39;, attendance=&#39;39&#39;),\n  Row(studentId=&#39;si6&#39;, subject=&#39;Java&#39;, attendance=&#39;35&#39;)]]</div>"]}}],"execution_count":49},{"cell_type":"code","source":["#CREATING DATAFRAME FROM LIST\nl=[(\"alice\",\"30\")]\nprint(spark.createDataFrame(l).collect())\n\n#CREATING DATAFRAME FROM DICTIONARY\nd=[{\"name\":\"Alice\",\"age\":30},{\"name\":\"Ram\",\"age\":32}]\nprint(spark.createDataFrame(d).collect())\n\n#CREATING DATAFRAME FROM RDD\nrdd=sc.parallelize([{\"name\":\"Ram\",\"age\":32,\"sex\":\"M\"},{\"name\":\"Tamy\",\"age\":35,\"sex\":\"F\"},{\"name\":\"Angel\",\"age\":34,\"sex\":\"F\"}])\nprint(spark.createDataFrame(rdd).collect())\n\n# Generating Sequence of Numbers\nprint(spark.range(3).collect())\n\n# Generating Sequence of Number using an interval (In the below example, we are using an Interval of 2)\n# range(start,end,step,numOfPartitions)\nprint(spark.range(1,7,2,3).collect())\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[Row(_1=&#39;alice&#39;, _2=&#39;30&#39;)]\n[Row(age=30, name=&#39;Alice&#39;), Row(age=32, name=&#39;Ram&#39;)]\n[Row(age=32, name=&#39;Ram&#39;, sex=&#39;M&#39;), Row(age=35, name=&#39;Tamy&#39;, sex=&#39;F&#39;), Row(age=34, name=&#39;Angel&#39;, sex=&#39;F&#39;)]\n[Row(id=0), Row(id=1), Row(id=2)]\n[Row(id=1), Row(id=3), Row(id=5)]\n</div>"]}}],"execution_count":50},{"cell_type":"code","source":["from pyspark.sql.functions import udf\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window\nstudent=spark.read.csv(\"/FileStore/tables/studentData_dateofbirth.csv\",header=True)\nstudent.show()\nsubjects=spark.read.csv(\"/FileStore/tables/Subjects_1.csv\",header=True)\nattendance=spark.read.csv(\"/FileStore/tables/attendance.csv\",header=True)\nstudent.printSchema()\nsubjects.printSchema()\nstudent.createOrReplaceTempView(\"student\")\nsubjects.createOrReplaceTempView(\"subjects\")\nattendance.createOrReplaceTempView(\"attendance\")\n# COMPUTING AVG - MARKS BY STUDENT - NAME\nspark.sql(\"SELECT st.name,avg(sb.marks) as avg_marks FROM student st INNER JOIN subjects sb ON st.studentId=sb.studentId group by st.name\").show()\n# COMPUTING AVG - MARKS BY STUDENT - NAME, SUBJECT\nspark.sql(\"SELECT st.name,sb.subject,avg(sb.marks) as avg_marks FROM student st INNER JOIN subjects sb ON st.studentId=sb.studentId group by st.name,sb.subject\").show()\n# FINDING NUMBER OF STUDENTS PER SUBJECT\nspark.sql(\"select sb.subject,count(st.name) Students_count from subjects sb INNER JOIN student st on (st.studentId = sb.studentId) group by sb.subject\").show()\n# FINDING NUMBER OF SUBJECTS PER STUDENT \nspark.sql(\"select st.name,COUNT(sb.subject) FROM student st INNER JOIN subjects sb ON st.studentId=sb.studentId group by st.name\").show()\n\n# OTHER USER DEFINED AGGREGATE FUNCTIONS AVAILABLE FOR USE IN SPARK - sum(), count(), min(), max(), avg(),countDistinct()\n# As the UDFs and UDAFs would be run in JVMs, the data needs to be serialized before passing it to the JVM. So there could be significant Performance Impact when these functions are incorrectly used.\n\n# APPLYING WINDOW FUNCTIONS USING SPARK_SQL:\n# Applying RANK function to get RANK()\n# FIND STUDENT SCORING FIRST and SECOND in each of the SUBJECTS\nprint(\"STudents Scoring First and Second by each subject\")\nspark.sql(\"select tmp.subject,tmp.name,tmp.marks FROM (SELECT sb.subject,st.name,sb.marks,RANK() OVER(PARTITION BY sb.subject ORDER BY sb.marks DESC) as rank FROM student st JOIN subjects sb on st.studentId=sb.studentId) tmp where rank <=2\").show()\n\n# RANK STUDENTS BY HIGHEST SCORE IN EACH SUBJECT\nspark.sql(\"select st.name,sb.subject,sb.marks,RANK() OVER(PARTITION BY sb.subject ORDER BY sb.marks DESC) as rank FROM student st INNER JOIN subjects sb ON st.studentId=sb.studentId\").show()\n\n# FIND STUDENTS WHOSE SCORE IS MORE THAN THE AVERAGE SCORE \nprint(\"Students whose Score is more than the Average Score for a given Subject\")\nspark.sql(\"select name,subject,marks FROM (select st.name,sb.subject,sb.marks, avg(sb.marks) OVER(PARTITION by sb.subject) as avg_score FROM  student st JOIN subjects sb ON st.studentId=sb.studentId)tmp where marks >= avg_score\").show()\n\n\n\n\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+-------+------+-----------+\nstudentId|   name|gender|dateofbirth|\n+---------+-------+------+-----------+\n      si1|  Robin|     M| 1981-12-13|\n      si2|  Maria|     F| 1986-06-06|\n      si3|  Julie|     F| 1988-09-05|\n      si4|    Bob|     M| 1987-05-04|\n      si6|William|     M| 1980-11-12|\n+---------+-------+------+-----------+\n\nroot\n-- studentId: string (nullable = true)\n-- name: string (nullable = true)\n-- gender: string (nullable = true)\n-- dateofbirth: string (nullable = true)\n\nroot\n-- studentId: string (nullable = true)\n-- subject: string (nullable = true)\n-- marks: string (nullable = true)\n\n+-----+---------+\n name|avg_marks|\n+-----+---------+\nRobin|     78.0|\n  Bob|     81.0|\nJulie|     74.0|\nMaria|     84.0|\n+-----+---------+\n\n+-----+-------+---------+\n name|subject|avg_marks|\n+-----+-------+---------+\n  Bob| Python|     84.0|\nRobin| Python|     75.0|\nRobin|   Java|     81.0|\nMaria| Python|     85.0|\nMaria|   Java|     83.0|\n  Bob|    C++|     78.0|\nJulie|   Ruby|     72.0|\nJulie|   Java|     76.0|\n+-----+-------+---------+\n\n+-------+--------------+\nsubject|Students_count|\n+-------+--------------+\n    C++|             1|\n   Ruby|             1|\n Python|             3|\n   Java|             3|\n+-------+--------------+\n\n+-----+--------------+\n name|count(subject)|\n+-----+--------------+\nRobin|             2|\n  Bob|             2|\nJulie|             2|\nMaria|             2|\n+-----+--------------+\n\nSTudents Scoring First and Second by each subject\n+-------+-----+-----+\nsubject| name|marks|\n+-------+-----+-----+\n    C++|  Bob|   78|\n   Ruby|Julie|   72|\n Python|Maria|   85|\n Python|  Bob|   84|\n   Java|Maria|   83|\n   Java|Robin|   81|\n+-------+-----+-----+\n\n+-----+-------+-----+----+\n name|subject|marks|rank|\n+-----+-------+-----+----+\n  Bob|    C++|   78|   1|\nJulie|   Ruby|   72|   1|\nMaria| Python|   85|   1|\n  Bob| Python|   84|   2|\nRobin| Python|   75|   3|\nMaria|   Java|   83|   1|\nRobin|   Java|   81|   2|\nJulie|   Java|   76|   3|\n+-----+-------+-----+----+\n\nStudents whose Score is more than the Average Score for a given Subject\n+-----+-------+-----+\n name|subject|marks|\n+-----+-------+-----+\n  Bob|    C++|   78|\nJulie|   Ruby|   72|\nMaria| Python|   85|\n  Bob| Python|   84|\nRobin|   Java|   81|\nMaria|   Java|   83|\n+-----+-------+-----+\n\n</div>"]}}],"execution_count":51},{"cell_type":"code","source":["# Checking if a given Character is a Vowel. If Vowel, function would return 1 else it would return 0\ndef vowelCheckFunction(char):\n  vowels=['a','e','i','o','u']\n  if char in vowels:\n    return 1\n  else:\n    return 0\n\n\nrdd=sc.parallelize(['b','d','m','t','e','u'],2)\nrdd2=rdd.map(lambda x:(x,vowelCheckFunction(x)))\nprint(rdd2.collect())\n  \nprint(rdd2.keys().collect()) # Retrieving Keys from RDD\nprint(rdd2.values().collect()) # Retrieving Values from RDD\n\n\n# Aggregating Data within RDD:\n# Mean Life in Hours by FilamentType\n# Mean Life in Hours by Bulb Power\n# Mean Life in Hours by FilamentType and Bulb Power\nfilDataSingle=[['filamentA','100W',605],\n['filamentB','100W',683],\n['filamentB','100W',691],\n['filamentB','200W',561],\n['filamentA','200W',530],\n['filamentA','100W',619],\n['filamentB','100W',686],\n['filamentB','200W',600],\n['filamentB','100W',696],\n['filamentA','200W',579],\n['filamentA','200W',520],\n['filamentA','100W',622],\n['filamentA','100W',668],\n['filamentB','200W',569],\n['filamentB','200W',555],\n['filamentA','200W',541]]\nfilDataRdd=sc.parallelize(filDataSingle,2)\nprint(filDataRdd.count())\nfilDataRdd.take(3)\nfilDataPairedRDD1=filDataRdd.map(lambda x:(x[0],x[2]))\nprint(filDataPairedRDD1.count())\nprint(\"\\nGet Num of Partitions:{0}\".format(filDataPairedRDD1.getNumPartitions()))\nfor i in filDataPairedRDD1.collect():\n  print(i)\nprint(\"\\nCompute Mean Life in Hours by Filament Type\")\nfilDataPairedRDD11=filDataPairedRDD1.map(lambda x:(x[0],(x[1],1)))\nfilDataPairedRDD11.take(2)\nfilDataPairedRDD12=filDataPairedRDD11.reduceByKey(lambda l1,l2:(l1[0]+l2[0],l1[1]+l2[1]))\nMean_By_Filament_Type=filDataPairedRDD12.map(lambda l:(l[0],(float(l[1][0]/l[1][1])),l[1][1]))\nfor i in Mean_By_Filament_Type.collect():\n  print(i)\n\nprint(\"\\nCompute Mean Life in Hours by Bulb Power\")\nfilDataPairRDD2=filDataRdd.map(lambda x:(x[1],x[2]))\nfilDataPairRDD21=filDataPairRDD2.map(lambda x:(x[0],(x[1],1)))\nfilDataPairRDD22=filDataPairRDD21.reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1]))\nMean_Life_By_Bulb_Power=filDataPairRDD22.map(lambda x:(x[0],(float(x[1][0]/x[1][1]),x[1][1])))\nfor i in Mean_Life_By_Bulb_Power.collect():\n  print(i)\n\nprint(\"\\nCompute Mean Life in Hours by Bulb Power and Filament Type\")\nfilDataPairRDD3=filDataRdd.map(lambda x:((x[0],x[1]),x[2]))\nfilDataPairRDD31=filDataPairRDD3.map(lambda x:(x[0],(x[1],1)))\nfilDataPairRDD32=filDataPairRDD31.reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1]))\nMean_Life_By_BulbPower_And_FilamenType=filDataPairRDD32.map(lambda x:(x[0],(float(x[1][0]/x[1][1]),x[1][1])))\nfor i in Mean_Life_By_BulbPower_And_FilamenType.collect():\n  print(i)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(&#39;b&#39;, 0), (&#39;d&#39;, 0), (&#39;m&#39;, 0), (&#39;t&#39;, 0), (&#39;e&#39;, 1), (&#39;u&#39;, 1)]\n[&#39;b&#39;, &#39;d&#39;, &#39;m&#39;, &#39;t&#39;, &#39;e&#39;, &#39;u&#39;]\n[0, 0, 0, 0, 1, 1]\n16\n16\n\nGet Num of Partitions:2\n(&#39;filamentA&#39;, 605)\n(&#39;filamentB&#39;, 683)\n(&#39;filamentB&#39;, 691)\n(&#39;filamentB&#39;, 561)\n(&#39;filamentA&#39;, 530)\n(&#39;filamentA&#39;, 619)\n(&#39;filamentB&#39;, 686)\n(&#39;filamentB&#39;, 600)\n(&#39;filamentB&#39;, 696)\n(&#39;filamentA&#39;, 579)\n(&#39;filamentA&#39;, 520)\n(&#39;filamentA&#39;, 622)\n(&#39;filamentA&#39;, 668)\n(&#39;filamentB&#39;, 569)\n(&#39;filamentB&#39;, 555)\n(&#39;filamentA&#39;, 541)\n\nCompute Mean Life in Hours by Filament Type\n(&#39;filamentB&#39;, 630.125, 8)\n(&#39;filamentA&#39;, 585.5, 8)\n\nCompute Mean Life in Hours by Bulb Power\n(&#39;100W&#39;, (658.75, 8))\n(&#39;200W&#39;, (556.875, 8))\n\nCompute Mean Life in Hours by Bulb Power and Filament Type\n((&#39;filamentB&#39;, &#39;100W&#39;), (689.0, 4))\n((&#39;filamentA&#39;, &#39;200W&#39;), (542.5, 4))\n((&#39;filamentA&#39;, &#39;100W&#39;), (628.5, 4))\n((&#39;filamentB&#39;, &#39;200W&#39;), (571.25, 4))\n</div>"]}}],"execution_count":52},{"cell_type":"code","source":["from pyspark.sql.types import * \nfrom pyspark.sql.functions import *\nfrom datetime import *\nstudents=spark.read.csv(\"/FileStore/tables/studentData_dateofbirth.csv\",header=True)\nsubjects=spark.read.csv(\"/FileStore/tables/Subjects_1.csv\",header=True)\nspark.sql(\"DROP table students_cached\")\nspark.sql(\"DROP table subjects_cached\")\nspark.sql(\"cache table students_cached as select * from students\")\nspark.sql(\"cache table subjects_cached as select * from subjects\")\nval1=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\nprint(\"Time Taken before cached query execution: {0}\".format(val1))\nspark.sql(\"select st.name,st.gender,st.dateofbirth,sb.subject,sb.marks from students_cached st join subjects_cached sb on st.studentId=sb.studentId\").show()\nval2=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\nprint(\"Time Taken after cached query execution: {0}\".format(val2))\n#print(\"Time taken:{0}-{1}:{2}\".format(val2,val2,(val2-val1)))\nval1=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\nprint(\"Time Taken before non-cached query execution: {0}\".format(val1))\nspark.sql(\"select st.name,st.gender,st.dateofbirth,sb.subject,sb.marks from students st join subjects sb on st.studentId=sb.studentId\").show()\nval2=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\nprint(\"Time Taken after non-cached query execution: {0}\".format(val2))\n#print(\"Time taken:{0}-{1}:{2}\".format(val2,val2,(val2-val1)))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Time Taken before cached query execution: 2019-12-28 19:45:25.660705\n+-----+------+-----------+-------+-----+\n name|gender|dateofbirth|subject|marks|\n+-----+------+-----------+-------+-----+\nRobin|     M| 1981-12-13| Python|   75|\nJulie|     F| 1988-09-05|   Java|   76|\nRobin|     M| 1981-12-13|   Java|   81|\nMaria|     F| 1986-06-06| Python|   85|\nJulie|     F| 1988-09-05|   Ruby|   72|\n  Bob|     M| 1987-05-04|    C++|   78|\n  Bob|     M| 1987-05-04| Python|   84|\nMaria|     F| 1986-06-06|   Java|   83|\n+-----+------+-----------+-------+-----+\n\nTime Taken after cached query execution: 2019-12-28 19:45:25.831941\nTime Taken before non-cached query execution: 2019-12-28 19:45:25.832197\n+-----+------+-----------+-------+-----+\n name|gender|dateofbirth|subject|marks|\n+-----+------+-----------+-------+-----+\nRobin|     M| 1981-12-13|   Java|   81|\nRobin|     M| 1981-12-13| Python|   75|\nMaria|     F| 1986-06-06|   Java|   83|\nMaria|     F| 1986-06-06| Python|   85|\nJulie|     F| 1988-09-05|   Ruby|   72|\nJulie|     F| 1988-09-05|   Java|   76|\n  Bob|     M| 1987-05-04| Python|   84|\n  Bob|     M| 1987-05-04|    C++|   78|\n+-----+------+-----------+-------+-----+\n\nTime Taken after non-cached query execution: 2019-12-28 19:45:26.112131\n</div>"]}}],"execution_count":53},{"cell_type":"code","source":["from pyspark.sql.types import * \nfrom pyspark.sql.functions import *\nstudents=spark.read.csv(\"/FileStore/tables/studentData_dateofbirth.csv\",header=True)\nsubjects=spark.read.csv(\"/FileStore/tables/Subjects_1.csv\",header=True)\nstudents.createOrReplaceTempView(\"students\")\nsubjects.createOrReplaceTempView(\"subjects\")\nprint(\"Using DISTRIBUTE BY and SORT BY clauses\")\nstudents_dist_sort=spark.sql(\"select * from students distribute by studentId SORT BY studentId\")\nsubjects_dist_sort=spark.sql(\"select * from subjects distribute by studentId SORT BY studentId\")\nstudents_dist_sort.createOrReplaceTempView(\"students_dist_sort\")\nsubjects_dist_sort.createOrReplaceTempView(\"subjects_dist_sort\")\nspark.sql(\"select st.name, st.gender,st.dateofbirth,sb.subject,sb.marks from students_dist_sort st join subjects_dist_sort sb on st.studentId=sb.studentId\").show()\nprint(\"Using Cluster By\")\nstudents_clust=spark.sql(\"select * from students cluster by studentId\")\nsubjects_clust=spark.sql(\"select * from subjects cluster by studentId\")\nstudents_clust.createOrReplaceTempView(\"students_clust\")\nsubjects_clust.createOrReplaceTempView(\"subjects_clust\")\nspark.sql(\"select st.name,st.gender,st.dateofbirth,sb.subject,sb.marks from students_clust st join subjects_clust sb ON st.studentId=sb.studentId\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Using DISTRIBUTE BY and SORT BY clauses\n+-----+------+-----------+-------+-----+\n name|gender|dateofbirth|subject|marks|\n+-----+------+-----------+-------+-----+\nMaria|     F| 1986-06-06| Python|   85|\nMaria|     F| 1986-06-06|   Java|   83|\n  Bob|     M| 1987-05-04|    C++|   78|\n  Bob|     M| 1987-05-04| Python|   84|\nJulie|     F| 1988-09-05|   Java|   76|\nJulie|     F| 1988-09-05|   Ruby|   72|\nRobin|     M| 1981-12-13| Python|   75|\nRobin|     M| 1981-12-13|   Java|   81|\n+-----+------+-----------+-------+-----+\n\nUsing Cluster By\n+-----+------+-----------+-------+-----+\n name|gender|dateofbirth|subject|marks|\n+-----+------+-----------+-------+-----+\nMaria|     F| 1986-06-06| Python|   85|\nMaria|     F| 1986-06-06|   Java|   83|\n  Bob|     M| 1987-05-04|    C++|   78|\n  Bob|     M| 1987-05-04| Python|   84|\nJulie|     F| 1988-09-05|   Java|   76|\nJulie|     F| 1988-09-05|   Ruby|   72|\nRobin|     M| 1981-12-13| Python|   75|\nRobin|     M| 1981-12-13|   Java|   81|\n+-----+------+-----------+-------+-----+\n\n</div>"]}}],"execution_count":54},{"cell_type":"code","source":["\n\nstudents=spark.read.csv(\"/FileStore/tables/studentData_dateofbirth.csv\",header=True)\nsubjects=spark.read.csv(\"/FileStore/tables/Subjects_1.csv\",header=True)\n# converting DataFrames to RDD\nstudents_rdd=students.rdd.map(lambda x:(x[0],(x[1],x[2]))).repartition(2)\nsubjects_rdd=subjects.rdd.map(lambda y:(y[0],y[1])).repartition(2)\nprint(students_rdd.take(2))\nprint(subjects_rdd.take(2))\n# Performing INNER JOIN \nprint(\"\\n Performing Inner Join against RDDs\")\nfor i in students_rdd.join(subjects_rdd).collect():\n  print(i)\nprint(\"\\n Performing Left Outer Join between RDDs\")  \nfor i in students_rdd.leftOuterJoin(subjects_rdd).collect():\n  print(i)\nprint(\"\\n Performing Right Outer Join between RDDs\")\nfor i in students_rdd.rightOuterJoin(subjects_rdd).collect():\n  print(i)\nprint(\"\\n Performing Outer Join between RDDs\")\nfor i in students_rdd.fullOuterJoin(subjects_rdd).collect():\n  print(i)\n  \n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(&#39;si1&#39;, (&#39;Robin&#39;, &#39;M&#39;)), (&#39;si2&#39;, (&#39;Maria&#39;, &#39;F&#39;))]\n[(&#39;si1&#39;, &#39;Python&#39;), (&#39;si3&#39;, &#39;Java&#39;)]\n\n Performing Inner Join against RDDs\n(&#39;si4&#39;, ((&#39;Bob&#39;, &#39;M&#39;), &#39;C++&#39;))\n(&#39;si4&#39;, ((&#39;Bob&#39;, &#39;M&#39;), &#39;Python&#39;))\n(&#39;si3&#39;, ((&#39;Julie&#39;, &#39;F&#39;), &#39;Java&#39;))\n(&#39;si3&#39;, ((&#39;Julie&#39;, &#39;F&#39;), &#39;Ruby&#39;))\n(&#39;si1&#39;, ((&#39;Robin&#39;, &#39;M&#39;), &#39;Python&#39;))\n(&#39;si1&#39;, ((&#39;Robin&#39;, &#39;M&#39;), &#39;Java&#39;))\n(&#39;si2&#39;, ((&#39;Maria&#39;, &#39;F&#39;), &#39;Python&#39;))\n(&#39;si2&#39;, ((&#39;Maria&#39;, &#39;F&#39;), &#39;Java&#39;))\n\n Performing Left Outer Join between RDDs\n(&#39;si4&#39;, ((&#39;Bob&#39;, &#39;M&#39;), &#39;C++&#39;))\n(&#39;si4&#39;, ((&#39;Bob&#39;, &#39;M&#39;), &#39;Python&#39;))\n(&#39;si6&#39;, ((&#39;William&#39;, &#39;M&#39;), None))\n(&#39;si3&#39;, ((&#39;Julie&#39;, &#39;F&#39;), &#39;Java&#39;))\n(&#39;si3&#39;, ((&#39;Julie&#39;, &#39;F&#39;), &#39;Ruby&#39;))\n(&#39;si1&#39;, ((&#39;Robin&#39;, &#39;M&#39;), &#39;Python&#39;))\n(&#39;si1&#39;, ((&#39;Robin&#39;, &#39;M&#39;), &#39;Java&#39;))\n(&#39;si2&#39;, ((&#39;Maria&#39;, &#39;F&#39;), &#39;Python&#39;))\n(&#39;si2&#39;, ((&#39;Maria&#39;, &#39;F&#39;), &#39;Java&#39;))\n\n Performing Right Outer Join between RDDs\n(&#39;si4&#39;, ((&#39;Bob&#39;, &#39;M&#39;), &#39;C++&#39;))\n(&#39;si4&#39;, ((&#39;Bob&#39;, &#39;M&#39;), &#39;Python&#39;))\n(&#39;si3&#39;, ((&#39;Julie&#39;, &#39;F&#39;), &#39;Java&#39;))\n(&#39;si3&#39;, ((&#39;Julie&#39;, &#39;F&#39;), &#39;Ruby&#39;))\n(&#39;si5&#39;, (None, &#39;C&#39;))\n(&#39;si1&#39;, ((&#39;Robin&#39;, &#39;M&#39;), &#39;Python&#39;))\n(&#39;si1&#39;, ((&#39;Robin&#39;, &#39;M&#39;), &#39;Java&#39;))\n(&#39;si2&#39;, ((&#39;Maria&#39;, &#39;F&#39;), &#39;Python&#39;))\n(&#39;si2&#39;, ((&#39;Maria&#39;, &#39;F&#39;), &#39;Java&#39;))\n\n Performing Outer Join between RDDs\n(&#39;si4&#39;, ((&#39;Bob&#39;, &#39;M&#39;), &#39;C++&#39;))\n(&#39;si4&#39;, ((&#39;Bob&#39;, &#39;M&#39;), &#39;Python&#39;))\n(&#39;si6&#39;, ((&#39;William&#39;, &#39;M&#39;), None))\n(&#39;si3&#39;, ((&#39;Julie&#39;, &#39;F&#39;), &#39;Java&#39;))\n(&#39;si3&#39;, ((&#39;Julie&#39;, &#39;F&#39;), &#39;Ruby&#39;))\n(&#39;si5&#39;, (None, &#39;C&#39;))\n(&#39;si1&#39;, ((&#39;Robin&#39;, &#39;M&#39;), &#39;Python&#39;))\n(&#39;si1&#39;, ((&#39;Robin&#39;, &#39;M&#39;), &#39;Java&#39;))\n(&#39;si2&#39;, ((&#39;Maria&#39;, &#39;F&#39;), &#39;Python&#39;))\n(&#39;si2&#39;, ((&#39;Maria&#39;, &#39;F&#39;), &#39;Java&#39;))\n</div>"]}}],"execution_count":55},{"cell_type":"code","source":["def rankContribution(uri,rank):\n\tnumberOfUris =len(uri)\n\trankContribution=float(rank)/numberOfUris \n\tnewrank=[]\n\tfor uri_1 in uri:\n\t\tnewrank.append((uri_1,rankContribution))\n\treturn newrank\n# Our function rankContribution will return the contribution to page rankfor the list or URIs (first variable) \t\t\t\n\npage_links=[('a',('b','c','d')),('b',('d','e')),('c',('b')),('d',('a','c'))]\npageLinksRDD=sc.parallelize(page_links,2)\npageRankRDD=pageLinksRDD.map(lambda x:(x[0],1),2)\nprint(pageRankRDD.take(2))\nnumIter=20\ns=0.85\nfor i in range(numIter):\n  pageLinks=pageLinksRDD.join(pageRankRDD)\n  contributedRDD=pageLinks.map(lambda x:rankContribution(x[1][0],x[1][1]))\n  sumRanks=contributedRDD.reduceByKey(lambda x,y:x+y)\n  pageRanksRDD=sumRanks.map(lambda x:(x[0],(1-s)+s*x[1]))\nprint(pageRanksRDD.collect())\n  \n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3059094560901907&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     19</span>   sumRanks<span class=\"ansi-blue-fg\">=</span>contributedRDD<span class=\"ansi-blue-fg\">.</span>reduceByKey<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">,</span>y<span class=\"ansi-blue-fg\">:</span>x<span class=\"ansi-blue-fg\">+</span>y<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     20</span>   pageRanksRDD<span class=\"ansi-blue-fg\">=</span>sumRanks<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">-</span>s<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">+</span>s<span class=\"ansi-blue-fg\">*</span>x<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">---&gt; 21</span><span class=\"ansi-red-fg\"> </span>print<span class=\"ansi-blue-fg\">(</span>pageRanksRDD<span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     22</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    829</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-credential passthrough clusters:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    830</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 831</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>collectAndServe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    832</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    833</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     61</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     65</span>             s <span class=\"ansi-blue-fg\">=</span> e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">.</span>toString<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 117.0 failed 1 times, most recent failure: Lost task 0.0 in stage 117.0 (TID 341, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 470, in process\n    out_iter = func(split_index, iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2542, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2542, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 353, in func\n    return f(iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1904, in combineLocally\n    merger.mergeValues(iterator)\n  File &#34;/databricks/spark/python/pyspark/shuffle.py&#34;, line 238, in mergeValues\n    for k, v in iterator:\nValueError: not enough values to unpack (expected 2, got 1)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:534)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:670)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:653)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:488)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:528)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1526)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:534)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2360)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2348)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2347)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2347)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1101)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1101)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1101)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2579)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2527)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2515)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:896)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2280)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2302)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2321)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2346)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:975)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:370)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:974)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:247)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor202.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 470, in process\n    out_iter = func(split_index, iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2542, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2542, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 353, in func\n    return f(iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1904, in combineLocally\n    merger.mergeValues(iterator)\n  File &#34;/databricks/spark/python/pyspark/shuffle.py&#34;, line 238, in mergeValues\n    for k, v in iterator:\nValueError: not enough values to unpack (expected 2, got 1)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:534)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:670)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:653)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:488)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:528)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1526)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:534)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":56},{"cell_type":"code","source":["filamentData=sc.textFile(\"/FileStore/tables/filamentData.csv\",2)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[81]: [[&#34;&#39;filamentA&#39;,&#39;100W&#39;,605&#34;,\n  &#34;&#39;filamentB&#39;,&#39;100W&#39;,683&#34;,\n  &#34;&#39;filamentB&#39;,&#39;100W&#39;,691&#34;,\n  &#34;&#39;filamentB&#39;,&#39;200W&#39;,561&#34;,\n  &#34;&#39;filamentA&#39;,&#39;200W&#39;,530&#34;,\n  &#34;&#39;filamentA&#39;,&#39;100W&#39;,619&#34;,\n  &#34;&#39;filamentB&#39;,&#39;100W&#39;,686&#34;,\n  &#34;&#39;filamentB&#39;,&#39;200W&#39;,600&#34;],\n [&#34;&#39;filamentB&#39;,&#39;100W&#39;,696&#34;,\n  &#34;&#39;filamentA&#39;,&#39;200W&#39;,579&#34;,\n  &#34;&#39;filamentA&#39;,&#39;200W&#39;,520&#34;,\n  &#34;&#39;filamentA&#39;,&#39;100W&#39;,622&#34;,\n  &#34;&#39;filamentA&#39;,&#39;100W&#39;,668&#34;,\n  &#34;&#39;filamentB&#39;,&#39;200W&#39;,569&#34;,\n  &#34;&#39;filamentB&#39;,&#39;200W&#39;,555&#34;,\n  &#34;&#39;filamentA&#39;,&#39;200W&#39;,541&#34;]]</div>"]}}],"execution_count":57},{"cell_type":"code","source":["def f(iterator): yield sum(iterator)\nrdd = sc.parallelize([1, 2, 3, 4], 2)\nrdd.mapPartitions(f).collect()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3688534833374012&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">def</span> f<span class=\"ansi-blue-fg\">(</span>iterator<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-green-fg\">yield</span> sum<span class=\"ansi-blue-fg\">(</span>iterator<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> rdd <span class=\"ansi-blue-fg\">=</span> sc<span class=\"ansi-blue-fg\">.</span>parallelize<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">4</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>rdd<span class=\"ansi-blue-fg\">.</span>mapPartitions<span class=\"ansi-blue-fg\">(</span>f<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    829</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-credential passthrough clusters:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    830</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 831</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>collectAndServe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    832</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    833</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     61</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     65</span>             s <span class=\"ansi-blue-fg\">=</span> e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">.</span>toString<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 95.0 failed 1 times, most recent failure: Lost task 0.0 in stage 95.0 (TID 389, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 504, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;&lt;command-3688534833374012&gt;&#34;, line 1, in f\n  File &#34;/databricks/spark/python/pyspark/sql/functions.py&#34;, line 44, in _\n    jc = getattr(sc._jvm.functions, name)(col._jc if isinstance(col, Column) else col)\nAttributeError: &#39;NoneType&#39; object has no attribute &#39;_jvm&#39;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:534)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:670)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:653)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:488)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:975)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:975)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:528)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1526)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:534)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2360)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2348)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2347)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2347)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1101)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1101)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1101)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2579)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2527)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2515)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:896)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2280)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2302)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2321)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2346)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:975)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:370)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:974)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:247)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor407.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 504, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;&lt;command-3688534833374012&gt;&#34;, line 1, in f\n  File &#34;/databricks/spark/python/pyspark/sql/functions.py&#34;, line 44, in _\n    jc = getattr(sc._jvm.functions, name)(col._jc if isinstance(col, Column) else col)\nAttributeError: &#39;NoneType&#39; object has no attribute &#39;_jvm&#39;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:534)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:670)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:653)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:488)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:975)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:975)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:528)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1526)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:534)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":58},{"cell_type":"code","source":["testDF =sqlContext.createDataFrame([(1,\"111\"), (2,\"111\"), (3,\"222\"), (4,\"222\"), (5,\"222\"), (6,\"111\"), (7,\"333\"), (8,\"444\")], [\"id\", \"d_id\"])\ntestDF.schema\ntestDF.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----+\n id|d_id|\n+---+----+\n  1| 111|\n  2| 111|\n  3| 222|\n  4| 222|\n  5| 222|\n  6| 111|\n  7| 333|\n  8| 444|\n+---+----+\n\n</div>"]}}],"execution_count":59},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\ndef amount_spent(quantity,price):\n  return quantity*price\n\ncustomer_name=StructField(\"cutomer_name\",StringType(),True)\ndate=StructField(\"date\",StringType(),True)\ncategory=StructField(\"category\",StringType(),True)\nproduct_name=StructField(\"product_name\",StringType(),True)\nquantity=StructField(\"quantity\",IntegerType(),True)\nprice=StructField(\"price\",DoubleType(),True)\ncolumnList=[customer_name,date,category,product_name,quantity,price]\ncustomerSchema=StructType(columnList)\n\ncustomersRDD = sc.parallelize([(\"Geoffrey\", \"2016-04-22\", \"A\", \"apples\", 1, 50.00),\n(\"Geoffrey\", \"2016-05-03\", \"B\", \"Lamp\", 2, 38.00),\n(\"Geoffrey\", \"2016-05-03\", \"D\", \"Solar Pannel\", 1, 29.00),\n(\"Geoffrey\", \"2016-05-03\", \"A\", \"apples\", 3, 50.00),\n(\"Geoffrey\", \"2016-05-03\", \"C\", \"Rice\", 5, 15.00),\n(\"Geoffrey\", \"2016-06-05\", \"A\", \"apples\", 5, 50.00),\n(\"Geoffrey\", \"2016-06-05\", \"A\", \"bananas\", 5, 55.00),\n(\"Geoffrey\", \"2016-06-15\", \"Y\", \"Motor skate\", 7, 68.00),\n(\"Geoffrey\", \"2016-06-15\", \"E\", \"Book: The noose\", 1, 125.00),\n(\"Yann\", \"2016-04-22\", \"B\", \"Lamp\", 1, 38.00),\n(\"Yann\", \"2016-05-03\", \"Y\", \"Motor skate\", 1, 68.00),\n(\"Yann\", \"2016-05-03\", \"D\", \"Recycle bin\", 5, 27.00),\n(\"Yann\", \"2016-05-03\", \"C\", \"Rice\", 15, 15.00),\n(\"Yann\", \"2016-04-02\", \"A\", \"bananas\", 3, 55.00),\n(\"Yann\", \"2016-04-02\", \"B\", \"Lamp\", 2, 38.00),\n(\"Yann\", \"2016-04-03\", \"E\", \"Book: Crime and Punishment\", 5, 100.00),\n(\"Yann\", \"2016-04-13\", \"E\", \"Book: The noose\", 5, 125.00),\n(\"Yann\", \"2016-04-27\", \"D\", \"Solar Pannel\", 5, 29.00),\n(\"Yann\", \"2016-05-27\", \"D\", \"Recycle bin\", 5, 27.00),\n(\"Yann\", \"2016-05-27\", \"A\", \"bananas\", 3, 55.00),\n(\"Yann\", \"2016-05-01\", \"Y\", \"Motor skate\", 1, 68.00),\n(\"Yann\", \"2016-06-07\", \"Z\", \"space ship\", 1, 227.00),\n(\"Yoshua\", \"2016-02-07\", \"Z\", \"space ship\", 2, 227.00),\n(\"Yoshua\", \"2016-02-14\", \"A\", \"bananas\", 9, 55.00),\n(\"Yoshua\", \"2016-02-14\", \"B\", \"Lamp\", 2, 38.00),\n(\"Yoshua\", \"2016-02-14\", \"A\", \"apples\", 10, 55.00),\n(\"Yoshua\", \"2016-03-07\", \"Z\", \"space ship\", 5, 227.00),\n(\"Yoshua\", \"2016-04-07\", \"Y\", \"Motor skate\", 4, 68.00),\n(\"Yoshua\", \"2016-04-07\", \"D\", \"Recycle bin\", 5, 27.00),\n(\"Yoshua\", \"2016-04-07\", \"C\", \"Rice\", 5, 15.00),\n(\"Yoshua\", \"2016-04-07\", \"A\", \"bananas\", 9, 55.00),\n(\"Jurgen\", \"2016-05-01\", \"Z\", \"space ship\", 1, 227.00),\n(\"Jurgen\", \"2016-05-01\", \"A\", \"bananas\", 5, 55.00),\n(\"Jurgen\", \"2016-05-08\", \"A\", \"bananas\", 5, 55.00),\n(\"Jurgen\", \"2016-05-08\", \"Y\", \"Motor skate\", 1, 68.00),\n(\"Jurgen\", \"2016-06-05\", \"A\", \"bananas\", 5, 55.00),\n(\"Jurgen\", \"2016-06-05\", \"C\", \"Rice\", 5, 15.00),\n(\"Jurgen\", \"2016-06-05\", \"Y\", \"Motor skate\", 2, 68.00),\n(\"Jurgen\", \"2016-06-05\", \"D\", \"Recycle bin\", 5, 27.00),\n])\n#toDF([\"customer_name\", \"date\", \"category\", \"product_name\", \"quantity\", \"price\"])\n\ncustomerDF=spark.createDataFrame(customersRDD,schema=customerSchema)\ncustomerDF.show()\ncustomerDF.createOrReplaceTempView(\"customers\")\n\nassert(spark.sql(\"select * from customers where dayofweek(cast(date) as date)==1\").count()==0)\namount_spent_udf=udf(amount_spent,DoubleType())\ncustomerDF_02=customerDF.withColumn(\"amount_spent\",amount_spent_udf(customerDF.quantity,customerDF.price))\n\n# What is the cumulative sum of spending of each customer throughout time\n\n\n\n              "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o197.sql.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input &#39;from&#39; expecting &lt;EOF&gt;(line 1, pos 9)\n\n== SQL ==\nselect * from customers where dayofweek(cast(date) as date)==1\n---------^^^\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:241)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:53)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:69)\n\tat com.databricks.sql.parser.DatabricksSqlParser$$anonfun$parsePlan$1.apply(DatabricksSqlParser.scala:64)\n\tat com.databricks.sql.parser.DatabricksSqlParser$$anonfun$parsePlan$1.apply(DatabricksSqlParser.scala:61)\n\tat com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:84)\n\tat com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:61)\n\tat org.apache.spark.sql.SparkSession$$anonfun$6.apply(SparkSession.scala:694)\n\tat org.apache.spark.sql.SparkSession$$anonfun$6.apply(SparkSession.scala:694)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:693)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">ParseException</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1068020922031906&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     60</span> customerDF<span class=\"ansi-blue-fg\">.</span>createOrReplaceTempView<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;customers&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     61</span> \n<span class=\"ansi-green-fg\">---&gt; 62</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">assert</span><span class=\"ansi-blue-fg\">(</span>spark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;select * from customers where dayofweek(cast(date) as date)==1&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>count<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">==</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     63</span> amount_spent_udf<span class=\"ansi-blue-fg\">=</span>udf<span class=\"ansi-blue-fg\">(</span>amount_spent<span class=\"ansi-blue-fg\">,</span>DoubleType<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span> customerDF_02<span class=\"ansi-blue-fg\">=</span>customerDF<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;amount_spent&#34;</span><span class=\"ansi-blue-fg\">,</span>amount_spent_udf<span class=\"ansi-blue-fg\">(</span>customerDF<span class=\"ansi-blue-fg\">.</span>quantity<span class=\"ansi-blue-fg\">,</span>customerDF<span class=\"ansi-blue-fg\">.</span>price<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">sql</span><span class=\"ansi-blue-fg\">(self, sqlQuery)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    834</span>         <span class=\"ansi-blue-fg\">[</span>Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row1&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row2&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row3&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    835</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 836</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jsparkSession<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span>sqlQuery<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_wrapped<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    837</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    838</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">2.0</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     72</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.parser.ParseException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 73</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> ParseException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     74</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.streaming.StreamingQueryException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     75</span>                 <span class=\"ansi-green-fg\">raise</span> StreamingQueryException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">ParseException</span>: &#34;\\nmismatched input &#39;from&#39; expecting &lt;EOF&gt;(line 1, pos 9)\\n\\n== SQL ==\\nselect * from customers where dayofweek(cast(date) as date)==1\\n---------^^^\\n&#34;</div>"]}}],"execution_count":60},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom datetime import *\ndf1 = sqlContext.createDataFrame([(\"11/25/1991\",\"11/24/1991\",\"11/30/1991\"), \n                            (\"11/25/1391\",\"11/24/1992\",\"11/30/1992\")], schema=['first', 'second', 'third'])\ndef castDate(date):\n  dates=[]\n  dates.append(datetime.strptime(date.first,'%m/%d/%Y').strftime(\"%Y-%m-%d\"))\n  dates.append(datetime.strptime(date.second,'%m/%d/%Y').strftime(\"%Y-%m-%d\"))\n  dates.append(datetime.strptime(date.third,'%m/%d/%Y').strftime(\"%Y-%m-%d\"))\n  return dates\n\ncastDateUDF=udf(castDate,DateType())\nrdd1=df1.rdd.map(lambda x:castDate(x)).flatMap(lambda x:x)\nrdd1_list=rdd1.collect()\nschema=StructType([StructField(\"date_col\",DateType(),True)])\n\n#df2=spark.createDataFrame(rdd1_list,schema)\n#df2.show()\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":61},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n    .master(\"spark://sparkmaster:7077\") \\\n    .appName(\"My Spark Application\") \\\n    .config(\"spark.submit.deployMode\", \"client\") \\\n    .getOrCreate()\nnumlines = spark.sparkContext.textFile(\"/FileStore/tables/UCBAdmissions.csv\") \\\n    .count()\nprint(\"The total number of lines is \" + str(numlines))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The total number of lines is 25\n</div>"]}}],"execution_count":62},{"cell_type":"code","source":["\nmyconf=sc._conf\nfor i in myconf.getAll():\n  if \"master\" in str(i):\n    print(i)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(&#39;spark.master&#39;, &#39;local[8]&#39;)\n</div>"]}}],"execution_count":63},{"cell_type":"code","source":["locwtemps = sc.parallelize(['Hayward,71|69|71|71|72',\n                            'Baumholder,46|42|40|37|39',\n                            'Alexandria,50|48|51|53|44',\n                            'Melbourne,88|101|85|77|74'],numSlices=4)\n# locwtemps.map(lambda x:x.split(',')).mapValues(lambda x:x.split('|')).map(lambda x:(x[0],[int(y) for y in x[1]])).map(lambda x:(x[0],(sum(x[1])/len(x[1])))).collect()\n#locwtemps.map(lambda x:x.split(',')).flatMapValues(lambda x:x.split('|')).map(lambda x:(x[0],int(x[1]))).collect()\n#locwtemps.map(lambda x:x.split(',')).mapValues(lambda x:x.split('|')).flatMapValues(lambda x:[int(y) for y in x]).map(lambda x:(x[0],(x[1],1))).foldByKey((0,0),lambda x,y:((x[0]+y[0]),(x[1]+y[1]))).map(lambda x:(x[0],float(x[1][0])/float(x[1][1]))).collect()\n#locwtemps.map(lambda x:x.split(',')).flatMapValues(lambda x:x.split('|')).mapValues(lambda x:int(x)).foldByKey(200,lambda x,y:y if x > y else x).collect()\n#groupByKey().collect()\n\n#locwtemps.map(lambda x:x.split(',')).mapValues(lambda x:x.split('|')).flatMapValues(lambda x:[int(y) for y in x]).groupByKey().mapValues(lambda x:float(sum(x))/float(len(x))).collect()\n\nlocwtemps.map(lambda x:x.split(',')).flatMapValues(lambda x:x.split('|')).mapValues(lambda x:int(x)).foldByKey(0,lambda x,y: x if x > y else y).map(lambda x:(x[1],x[0])).sortByKey(ascending=False).map(lambda x:(x[1],x[0])).collect()\n#flatMapValues(lambda x:[int(y) for y in x])\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[56]: [(&#39;Melbourne&#39;, 101), (&#39;Hayward&#39;, 72), (&#39;Alexandria&#39;, 53), (&#39;Baumholder&#39;, 46)]</div>"]}}],"execution_count":64},{"cell_type":"code","source":["import re\ndoc=sc.textFile(\"/FileStore/tables/shakespearePlays.txt\",4)\n#doc.collect()\n# REMOVE EMPTY LINES FROM THE FILE\n# USING WHITESPACE AS DELIMITER SPLIT LINES IN THE FILE\ndoc1=doc.filter(lambda x:len(x)>0) \\\n   .flatMap(lambda line:line.split(' ')) \\\n   .map(lambda x:(len(x),x)) \\\n   .sortByKey(ascending=False) \\\n   .map(lambda word:(word[1],word[0])) \ndoc2=sc.parallelize(doc1.take(5))\ndoc2.collect()\n   \ndoc2.saveAsTextFile(\"/FileStore/tables/counts/ShakespearePlays3/\")\n   \n\n  \n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":65},{"cell_type":"code","source":["stores = sc.parallelize([(100, 'Boca Raton'),\n                         (101, 'Columbia'),\n                         (102, 'Cambridge'),\n                         (103, 'Naperville')])\n# stores schema (store_id, store_location)\nsalespeople = sc.parallelize([(1, 'Henry', 100),\n                              (2, 'Karen', 100),\n                              (3, 'Paul', 101),\n                              (4, 'Jimmy', 102),\n                              (5, 'Janice', None)])\n# salespeople schema (salesperson_id, salesperson_name, store_id)\nsalespeople.keyBy(lambda x:(x[2])) \\\n           .leftOuterJoin(stores) \\\n           .filter(lambda x:(x[1][1] is None)) \\\n           .map(lambda x: (\"SalesPerson \"+ str(x[1][0][1]) +\" has no store allocated\")) \\\n           .collect()\n\n\n#print(stores.collect())\n#print(salespeople.collect())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[148]: [&#39;SalesPerson Janice has no store allocated&#39;]</div>"]}}],"execution_count":66},{"cell_type":"code","source":["File uploaded to /FileStore/tables/status.csv\n File uploaded to /FileStore/tables/weather.csv\n File uploaded to /FileStore/tables/stations.csv\n File uploaded to /FileStore/tables/trips.csv\n  Objective: you will use this data to return the average number of bikes available by the hour for one week (February 22 to February 28) for stations located in the San Jose area only"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["stations=sc.textFile(\"/FileStore/tables/stations.csv\")\n\nprint(sc.textFile(\"/FileStore/tables/status.csv\").take(2))\nstation_status=sc.textFile(\"/FileStore/tables/status.csv\",30) \\\n           .map(lambda x:x.split(',')) \\\n         .map(lambda x:(x[0],x[1],x[2],x[3].replace('\"',''))) \\\n         .map(lambda x:(x[0],x[1],x[2],x[3].split('-'))) \\\n         .map(lambda x:(x[0],x[1],x[2],int(x[3][0]),str(x[3][1]),x[3][2].split(' '))) \\\n         .map(lambda x:(x[0],x[1],x[2],x[3],x[4],int(x[5][0]),x[5][1].split(':')))\n         \n# File Station Data only for the required location \nstations_sanJose=stations.map(lambda x:x.split(',')) \\\n                         .filter(lambda x:('san jose' in x[1].lower() or 'san jose' in x[5].lower()))\n\n# Extract station_id's pertaining to San_Jose Location into a List\nstations_id_sanJose=stations_sanJose.map(lambda x:x[0]).distinct().collect()\n\n#Filter station status data only for Required Dates (22nd Feb 2015 to 28th Feb 2015)\nstation_status_1=station_status.filter(lambda x:(x[3]==2015 and x[4]=='02' and x[5]>=22 and x[5]<=28))\n\n# Filter station status data for only San Jose Location\nstation_status_2=station_status_1.filter(lambda x:x[1] in stations_id_sanJose)\n\n# Extract only station_id, available_bikes, hour from station_status RDD\nstations_status_3=station_status_2.map(lambda x:(x[0],(x[2],x[6][0])))\n# Extract only station_id, station_name from station RDD\nstation_sanJose_3=stations_sanJose.map(lambda x:(x[0],(x[1])))\n\nresult=stations_status_3.join(station_sanJose_3)\n\nresult.map(lambda x:(x[1][0][0],x[1][0][1])) \\\n            .mapValues(lambda x:(x,1))  \\\n            .reduceByKey(lambda x,y:(int(x[0])+int(y[0]),int(x[1])+int(y[1]))) \\\n            .map(lambda x:(round((x[1][0]/x[1][1]),2),x[0])) \\\n            .sortByKey(ascending=False) \\\n            .map(lambda x:(x[1],x[0])).collect()\n            \n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;2,San Jose Diridon Caltrain Station,37.329732,-121.901782,27,San Jose,8/6/2013&#39;, &#39;3,San Jose Civic Center,37.330698,-121.888979,15,San Jose,8/5/2013&#39;]\n[&#39;10,9,6,&#34;2015-02-28 23:59:01&#34;&#39;, &#39;10,9,6,&#34;2015-02-28 23:58:02&#34;&#39;]\n\n\n\n[(&#39;10&#39;, &#39;9&#39;, &#39;6&#39;, 2015, &#39;02&#39;, 28, [&#39;23&#39;, &#39;59&#39;, &#39;01&#39;])]\n\n\n[[&#39;2&#39;, &#39;San Jose Diridon Caltrain Station&#39;, &#39;37.329732&#39;, &#39;-121.901782&#39;, &#39;27&#39;, &#39;San Jose&#39;, &#39;8/6/2013&#39;]]\n\n\n\n[(&#39;10&#39;, (&#39;6&#39;, &#39;23&#39;))]\n\n\n[(&#39;2&#39;, &#39;San Jose Diridon Caltrain Station&#39;)]\n[(&#39;10&#39;, ((&#39;6&#39;, &#39;23&#39;), &#39;San Jose City Hall&#39;))]\nOut[256]: [(&#39;4&#39;, 12.83),\n (&#39;6&#39;, 12.25),\n (&#39;17&#39;, 12.18),\n (&#39;10&#39;, 12.07),\n (&#39;18&#39;, 11.9),\n (&#39;8&#39;, 11.61),\n (&#39;15&#39;, 11.61),\n (&#39;5&#39;, 11.52),\n (&#39;13&#39;, 11.45),\n (&#39;7&#39;, 11.43),\n (&#39;12&#39;, 11.31),\n (&#39;16&#39;, 11.3),\n (&#39;9&#39;, 11.12),\n (&#39;11&#39;, 11.09),\n (&#39;14&#39;, 10.27),\n (&#39;19&#39;, 10.21),\n (&#39;20&#39;, 9.48),\n (&#39;0&#39;, 9.33),\n (&#39;1&#39;, 9.17),\n (&#39;2&#39;, 9.06),\n (&#39;3&#39;, 8.2)]</div>"]}}],"execution_count":68},{"cell_type":"code","source":["cities1 = sc.parallelize([('Hayward',(37.668819,-122.080795)),\n                          ('Baumholder',(49.6489,7.3975)),\n                          ('Alexandria',(38.820450,-77.050552)),\n                          ('Melbourne', (37.663712,144.844788))])\ncities2 = sc.parallelize([('Boulder Creek',(64.0708333,-148.2236111)),\n                          ('Hayward',(37.668819,-122.080795)),\n                          ('Alexandria',(38.820450,-77.050552)),\n                          ('Arlington', (38.878337,-77.100703))])\n#cities1.collect()\n#cities2.collect()\nprint(cities1.subtractByKey(cities2).collect())\nprint(cities2.subtractByKey(cities1).collect())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(&#39;Baumholder&#39;, (49.6489, 7.3975)), (&#39;Melbourne&#39;, (37.663712, 144.844788))]\n[(&#39;Boulder Creek&#39;, (64.0708333, -148.2236111)), (&#39;Arlington&#39;, (38.878337, -77.100703))]\n</div>"]}}],"execution_count":69},{"cell_type":"code","source":["# Reading Data from stations.csv file\nwith open(\"/dbfs/FileStore/tables/stations.csv\",'r') as fileObj:\n  record=fileObj.read().split('\\n')\n\n# Extracting Station_ID and Station_Name from station dataset \nstation_detail=dict[]\nfor i in record:\n  if len(i)>0:\n    data=i.split(',')\n    station_id=int(data[0])\n    station_name=data[5]\n    id_name=(station_id,station_name)\n    station_detail.append({station_id:station_name)\n\n# Loading Data to Broadcast Variable\nstation_detail_bd=sc.broadcast(station_detail)\n\nstation_status=sc.textFile(\"/FileStore/tables/status.csv\")\nstation_status.take(2)\n\n# Find Stations in each City which has more number of available bikes \n# Find Stations in each City which has least number of available bikes \nstation_status_1=station_status.map(lambda x:x.split(','))\\\n              .map(lambda x:(x[0],x[1],x[2],x[3].replace('\"','').split('-'))) \\\n              .map(lambda x:(int(x[0]),int(x[1]),int(x[2]),int(x[3][0]),int(x[3][1]),x[3][2].split(' '))) \\\n              .map(lambda x:(x[0],x[1],x[2],x[3],x[4],int(x[5][0]),x[5][1].split(','))) \\\n              .map(lambda x:(x[0],x[1],x[2],x[3],x[4],x[5],str(x[6]).replace('[','').replace(']','').replace('\\\"','').replace(\"\\'\",'').split(':'))) \\\n              .map(lambda x:(x[0],x[1],x[2],x[3],x[4],x[5],int(x[6][0]),int(x[6][1]),int(x[6][2])))\n# Filter Station_ID, Available Bikes, Available Docks\nstation_status_2=station_status_1.map(lambda x:(x[0],(x[2],x[1])))\n#print(station_status_2.take(4))\n# SCHEMA\n# station_detail_bd=[station_id,station_name]\n# station_status_1=[station_ID,available bikes,available docks]\n#for i in station_detail_bd.value:\n#  print(i)\n\n\n\n#for i in station_detail_bd.value:\n#  print(i[1])\nx=10\nprint(getLocation(10))\ndef getLocation(x):\n  for i in station_detail_bd.value:\n      print(\"\\n -{0}\".format(i[0]))\n      print(\"\\n -{1}\".format(x))\n      if (int(i[0])==int(x)):\n        print(\"Within Function\")\n        val=i[1]\n      else:\n        val=-1\n  return val\n# station_status_1.take(2)\n#station_status_2.map(lambda x:(x[1][0],x[0])).take(10)\n#station_status_2.map(lambda x:(x[1][0],getLocation(x[0]))).take(10)\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">IndexError</span>                                Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3907370955433624&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     41</span> <span class=\"ansi-red-fg\">#  print(i[1])</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     42</span> x<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">10</span>\n<span class=\"ansi-green-fg\">---&gt; 43</span><span class=\"ansi-red-fg\"> </span>print<span class=\"ansi-blue-fg\">(</span>getLocation<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">10</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     44</span> <span class=\"ansi-green-fg\">def</span> getLocation<span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     45</span>   <span class=\"ansi-green-fg\">for</span> i <span class=\"ansi-green-fg\">in</span> station_detail_bd<span class=\"ansi-blue-fg\">.</span>value<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">&lt;command-3907370955433624&gt;</span> in <span class=\"ansi-cyan-fg\">getLocation</span><span class=\"ansi-blue-fg\">(x)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     44</span> <span class=\"ansi-green-fg\">def</span> getLocation<span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     45</span>   <span class=\"ansi-green-fg\">for</span> i <span class=\"ansi-green-fg\">in</span> station_detail_bd<span class=\"ansi-blue-fg\">.</span>value<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 46</span><span class=\"ansi-red-fg\">       </span>print<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;\\n -{0}&#34;</span><span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span>i<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     47</span>       print<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;\\n -{1}&#34;</span><span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     48</span>       <span class=\"ansi-green-fg\">if</span> <span class=\"ansi-blue-fg\">(</span>int<span class=\"ansi-blue-fg\">(</span>i<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">==</span>int<span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">IndexError</span>: tuple index out of range</div>"]}}],"execution_count":70},{"cell_type":"code","source":["# Reading Data from stations.csv file into RDD\nwith open(\"/dbfs/FileStore/tables/stations.csv\",'r') as fileObj:\n  record=fileObj.read().split('\\n')\n\n# Extracting Station_ID and Station_Name from station dataset \nstation_detail=dict()\nfor i in record:\n  if len(i)>0:\n    data=i.split(',')\n    station_id=int(data[0])\n    station_name=data[5]\n    id_name=(station_id,station_name)\n    station_detail.update({station_id:station_name})\nprint(type(station_detail))\nstation_detail_bd=sc.broadcast(station_detail)\n\n# Read Data from Station Status File into RDD\nstation_status=sc.textFile(\"FileStore/tables/status.csv\")\nstation_status_1=station_status.map(lambda x:x.split(','))                 \\\n              .map(lambda x:(int(x[0]),int(x[1]),int(x[2]),x[3].replace(\"\\\"\",'').replace('\\'','').split('-')))       \\\n              .map(lambda x:(x[0],x[1],x[2],int(x[3][0]),int(x[3][1]),int(x[3][2].split(' ')[0]),x[3][2].split(' ')[1]))      \\\n              .map(lambda x:(x[0],x[1],x[2],x[3],x[4],x[5],int(x[6].split(':')[0]),int(x[6].split(':')[1]),int(x[6].split(':')[2])))\n              \nstation=station_detail_bd.value\nprint(type(station))\n# Read Only one Month's Data (FEB_22 to FEB_28 2015)\nstation_status_2=station_status_1.filter(lambda x:(x[3]==2015 and x[4]==2 and x[5]>=22 and x[5]<=28))\nprint(station_status_2.take(2))\n# station_Name, Time,Available_Bikes\n#station_status_2.map(lambda x:(x[0],x[1])).take(4)\nstation_status_2.map(lambda x:((station[x[0]],x[6]),x[2])).mapValues(lambda x:(x,1)).reduceByKey(lambda x,y:((x[0]+y[0]),(x[1]+y[1]))) \\\n                .mapValues(lambda x:round((float(x[0]/x[1])),2)) \\\n                .map(lambda x:(x[1],x[0]))   \\\n                .sortByKey(ascending=True)   \\\n                .map(lambda x:(x[1],x[0]))    \\\n                .take(100)\n\n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &#39;dict&#39;&gt;\n&lt;class &#39;dict&#39;&gt;\n[(10, 9, 6, 2015, 2, 28, 23, 59, 1), (10, 9, 6, 2015, 2, 28, 23, 58, 2)]\nOut[61]: [((&#39;Palo Alto&#39;, 2), 7.3),\n ((&#39;Palo Alto&#39;, 1), 7.31),\n ((&#39;Palo Alto&#39;, 3), 7.31),\n ((&#39;Palo Alto&#39;, 0), 7.32),\n ((&#39;Palo Alto&#39;, 7), 7.33),\n ((&#39;Palo Alto&#39;, 4), 7.33),\n ((&#39;Palo Alto&#39;, 6), 7.33),\n ((&#39;Palo Alto&#39;, 5), 7.36),\n ((&#39;Palo Alto&#39;, 8), 7.38),\n ((&#39;Palo Alto&#39;, 9), 7.42),\n ((&#39;Palo Alto&#39;, 19), 7.45),\n ((&#39;Palo Alto&#39;, 16), 7.45),\n ((&#39;Palo Alto&#39;, 23), 7.46),\n ((&#39;Palo Alto&#39;, 22), 7.47),\n ((&#39;Palo Alto&#39;, 20), 7.48),\n ((&#39;Palo Alto&#39;, 15), 7.49),\n ((&#39;Palo Alto&#39;, 21), 7.49),\n ((&#39;Palo Alto&#39;, 18), 7.5),\n ((&#39;Palo Alto&#39;, 10), 7.52),\n ((&#39;Palo Alto&#39;, 17), 7.53),\n ((&#39;Palo Alto&#39;, 14), 7.54),\n ((&#39;Palo Alto&#39;, 13), 7.56),\n ((&#39;Palo Alto&#39;, 11), 7.64),\n ((&#39;Palo Alto&#39;, 12), 7.68),\n ((&#39;San Jose&#39;, 14), 8.96),\n ((&#39;San Jose&#39;, 22), 8.98),\n ((&#39;San Jose&#39;, 15), 8.99),\n ((&#39;San Jose&#39;, 23), 9.0),\n ((&#39;San Jose&#39;, 13), 9.0),\n ((&#39;San Jose&#39;, 21), 9.01),\n ((&#39;San Jose&#39;, 11), 9.01),\n ((&#39;San Jose&#39;, 19), 9.02),\n ((&#39;San Jose&#39;, 20), 9.02),\n ((&#39;San Jose&#39;, 0), 9.02),\n ((&#39;San Jose&#39;, 1), 9.03),\n ((&#39;San Jose&#39;, 18), 9.03),\n ((&#39;San Jose&#39;, 2), 9.03),\n ((&#39;San Jose&#39;, 12), 9.04),\n ((&#39;San Jose&#39;, 10), 9.05),\n ((&#39;San Jose&#39;, 5), 9.06),\n ((&#39;San Jose&#39;, 16), 9.06),\n ((&#39;San Jose&#39;, 4), 9.06),\n ((&#39;San Jose&#39;, 3), 9.07),\n ((&#39;San Jose&#39;, 6), 9.08),\n ((&#39;San Jose&#39;, 17), 9.09),\n ((&#39;San Jose&#39;, 9), 9.13),\n ((&#39;Mountain View&#39;, 22), 9.14),\n ((&#39;San Jose&#39;, 8), 9.14),\n ((&#39;Mountain View&#39;, 23), 9.15),\n ((&#39;San Jose&#39;, 7), 9.17),\n ((&#39;Mountain View&#39;, 4), 9.17),\n ((&#39;Mountain View&#39;, 3), 9.17),\n ((&#39;Mountain View&#39;, 5), 9.17),\n ((&#39;Mountain View&#39;, 21), 9.17),\n ((&#39;Mountain View&#39;, 0), 9.18),\n ((&#39;Mountain View&#39;, 2), 9.19),\n ((&#39;Mountain View&#39;, 6), 9.19),\n ((&#39;Mountain View&#39;, 20), 9.19),\n ((&#39;Mountain View&#39;, 1), 9.19),\n ((&#39;Mountain View&#39;, 11), 9.19),\n ((&#39;Mountain View&#39;, 7), 9.2),\n ((&#39;Mountain View&#39;, 12), 9.25),\n ((&#39;Mountain View&#39;, 10), 9.27),\n ((&#39;Mountain View&#39;, 9), 9.27),\n ((&#39;Mountain View&#39;, 19), 9.27),\n ((&#39;Mountain View&#39;, 13), 9.33),\n ((&#39;Mountain View&#39;, 8), 9.35),\n ((&#39;Mountain View&#39;, 18), 9.37),\n ((&#39;Mountain View&#39;, 14), 9.44),\n ((&#39;Mountain View&#39;, 16), 9.44),\n ((&#39;Redwood City&#39;, 14), 9.45),\n ((&#39;Redwood City&#39;, 13), 9.45),\n ((&#39;Mountain View&#39;, 15), 9.45),\n ((&#39;Redwood City&#39;, 15), 9.46),\n ((&#39;Mountain View&#39;, 17), 9.46),\n ((&#39;Redwood City&#39;, 12), 9.47),\n ((&#39;Redwood City&#39;, 16), 9.49),\n ((&#39;Redwood City&#39;, 11), 9.53),\n ((&#39;Redwood City&#39;, 17), 9.54),\n ((&#39;Redwood City&#39;, 22), 9.56),\n ((&#39;Redwood City&#39;, 3), 9.56),\n ((&#39;Redwood City&#39;, 9), 9.56),\n ((&#39;Redwood City&#39;, 23), 9.56),\n ((&#39;Redwood City&#39;, 10), 9.57),\n ((&#39;Redwood City&#39;, 1), 9.57),\n ((&#39;Redwood City&#39;, 0), 9.58),\n ((&#39;Redwood City&#39;, 2), 9.58),\n ((&#39;Redwood City&#39;, 6), 9.58),\n ((&#39;Redwood City&#39;, 21), 9.58),\n ((&#39;Redwood City&#39;, 8), 9.59),\n ((&#39;Redwood City&#39;, 5), 9.59),\n ((&#39;Redwood City&#39;, 4), 9.6),\n ((&#39;Redwood City&#39;, 20), 9.6),\n ((&#39;Redwood City&#39;, 7), 9.61),\n ((&#39;Redwood City&#39;, 19), 9.63),\n ((&#39;Redwood City&#39;, 18), 9.65),\n ((&#39;San Francisco&#39;, 23), 10.19),\n ((&#39;San Francisco&#39;, 3), 10.2),\n ((&#39;San Francisco&#39;, 1), 10.2),\n ((&#39;San Francisco&#39;, 4), 10.21)]</div>"]}}],"execution_count":71},{"cell_type":"code","source":["counter=sc.accumulator(0)\ndef incr(x):\n  global counter\n  counter+=1\n  x+=1\n  return x\nsc.range(1,11).map(lambda x:incr(x)).collect()\nprint(\"Total Number of records processed: {0}\".format(str(counter.value)))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Total Number of records processed: 10\n</div>"]}}],"execution_count":72},{"cell_type":"code","source":["from pyspark import SparkContext,SparkConf\nconf=SparkConf()\nconf.set(\"spark.default.parallelism\",\"5\")\n# sc=SparkContext(conf=conf)\nstations_statusRDD=sc.textFile(\"/FileStore/tables/status.csv\",10)\nprint(stations_statusRDD.count())\nprint(stations_statusRDD.getNumPartitions())\nprint(stations_statusRDD.take(2))\n\nstations_statusRDD_2=stations_statusRDD.map(lambda x:x.split(',')).map(lambda x:(x[0],(x[2],x[1])))\nstations_statusRDD_2.reduceByKey(lambda x,y:((int(x[0])+int(y[0])),(int(x[1])+int(y[1])))).take(2)\nprint(stations_statusRDD_2.getNumPartitions())\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ValueError</span>                                Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3065538824793546&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> conf<span class=\"ansi-blue-fg\">=</span>SparkConf<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> conf<span class=\"ansi-blue-fg\">.</span>set<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;spark.default.parallelism&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#34;5&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span>sc<span class=\"ansi-blue-fg\">=</span>SparkContext<span class=\"ansi-blue-fg\">(</span>conf<span class=\"ansi-blue-fg\">=</span>conf<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> stations_statusRDD<span class=\"ansi-blue-fg\">=</span>sc<span class=\"ansi-blue-fg\">.</span>textFile<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/FileStore/tables/status.csv&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-cyan-fg\">10</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> print<span class=\"ansi-blue-fg\">(</span>stations_statusRDD<span class=\"ansi-blue-fg\">.</span>count<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansi-cyan-fg\">__init__</span><span class=\"ansi-blue-fg\">(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    131</span>                     &#34; note this option will be removed in Spark 3.0&#34;)\n<span class=\"ansi-green-intense-fg ansi-bold\">    132</span> \n<span class=\"ansi-green-fg\">--&gt; 133</span><span class=\"ansi-red-fg\">         </span>SparkContext<span class=\"ansi-blue-fg\">.</span>_ensure_initialized<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> gateway<span class=\"ansi-blue-fg\">=</span>gateway<span class=\"ansi-blue-fg\">,</span> conf<span class=\"ansi-blue-fg\">=</span>conf<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    134</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    135</span>             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansi-cyan-fg\">_ensure_initialized</span><span class=\"ansi-blue-fg\">(cls, instance, gateway, conf)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    335</span>                         <span class=\"ansi-blue-fg\">&#34; created by %s at %s:%s &#34;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    336</span>                         % (currentAppName, currentMaster,\n<span class=\"ansi-green-fg\">--&gt; 337</span><span class=\"ansi-red-fg\">                             callsite.function, callsite.file, callsite.linenum))\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    338</span>                 <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    339</span>                     SparkContext<span class=\"ansi-blue-fg\">.</span>_active_spark_context <span class=\"ansi-blue-fg\">=</span> instance\n\n<span class=\"ansi-red-fg\">ValueError</span>: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /local_disk0/tmp/1578213161298-0/PythonShell.py:1164 </div>"]}}],"execution_count":73},{"cell_type":"code","source":["nums1=[-1,-2]\nnums2=[3]\n\nnums=[]\n        \nfor i in nums1:\n    nums.append(i)\nfor j in nums2:\n    nums.append(j)\n    temp=0\n        \nprint(\"Length of nums:{0}\".format(len(nums)))\nfor i in nums:\n  print(i)\nfor i in range(0,len(nums)-1):\n    for j in range(i+1,len(nums)-1):\n        if nums[i] >= nums[j]:\n            temp=nums[i]\n            nums[i]=nums[j]\n            nums[j]=temp\n            print(\"nums[i]={0},nums[j]={1}\".format(nums[i],nums[j]))\n                    \nlen_nums=len(nums)\nprint(\"Printing Elements of the list\")        \nfor i in nums:\n    print(i)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Length of nums:3\n-1\n-2\n3\nnums[i]=-2,nums[j]=-1\nPrinting Elements of the list\n-2\n-1\n3\n</div>"]}}],"execution_count":74}],"metadata":{"name":"Learning_1","notebookId":1976539813094791},"nbformat":4,"nbformat_minor":0}
