{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nrdd = sc.parallelize([1, 2, 3, 4], 2)\ndef f(iterator): yield sum(iterator)\nrdd.mapPartitions(f).collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2451590036240535&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> rdd <span class=\"ansi-blue-fg\">=</span> sc<span class=\"ansi-blue-fg\">.</span>parallelize<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">4</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> <span class=\"ansi-green-fg\">def</span> f<span class=\"ansi-blue-fg\">(</span>iterator<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-green-fg\">yield</span> sum<span class=\"ansi-blue-fg\">(</span>iterator<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\"> </span>rdd<span class=\"ansi-blue-fg\">.</span>mapPartitions<span class=\"ansi-blue-fg\">(</span>f<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    829</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-credential passthrough clusters:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    830</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 831</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>collectAndServe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    832</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    833</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     61</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     65</span>             s <span class=\"ansi-blue-fg\">=</span> e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">.</span>toString<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 46.0 failed 1 times, most recent failure: Lost task 0.0 in stage 46.0 (TID 98, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 508, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;&lt;command-2451590036240535&gt;&#34;, line 4, in f\n  File &#34;/databricks/spark/python/pyspark/sql/functions.py&#34;, line 44, in _\n    jc = getattr(sc._jvm.functions, name)(col._jc if isinstance(col, Column) else col)\nAttributeError: &#39;NoneType&#39; object has no attribute &#39;_jvm&#39;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:676)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:659)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:997)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:997)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:537)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:543)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2362)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2350)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2349)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2349)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2582)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2517)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2280)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2302)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2321)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2346)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:997)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:392)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:996)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:247)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 508, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;&lt;command-2451590036240535&gt;&#34;, line 4, in f\n  File &#34;/databricks/spark/python/pyspark/sql/functions.py&#34;, line 44, in _\n    jc = getattr(sc._jvm.functions, name)(col._jc if isinstance(col, Column) else col)\nAttributeError: &#39;NoneType&#39; object has no attribute &#39;_jvm&#39;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:676)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:659)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:997)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:997)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:537)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:543)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark import SparkContext\ntemp=spark.range(100)\nprint(type(temp))\ntemp.rdd.getNumPartitions()\nfor i in temp.take(10):\n  print(i)\n  \n# Example of mapPartition\ndef compute_length(x):\n  print(type(x))\n  yield len(x)\ntemp.repartition(5).rdd.mapPartitions(lambda x:compute_length(x)).collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3485556643975441&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     11</span>   print<span class=\"ansi-blue-fg\">(</span>type<span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     12</span>   <span class=\"ansi-green-fg\">yield</span> len<span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">---&gt; 13</span><span class=\"ansi-red-fg\"> </span>temp<span class=\"ansi-blue-fg\">.</span>repartition<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">5</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">.</span>mapPartitions<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span>compute_length<span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    829</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-credential passthrough clusters:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    830</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 831</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>collectAndServe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    832</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    833</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     61</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     65</span>             s <span class=\"ansi-blue-fg\">=</span> e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">.</span>toString<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 52.0 failed 1 times, most recent failure: Lost task 1.0 in stage 52.0 (TID 110, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 508, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;&lt;command-3485556643975441&gt;&#34;, line 12, in compute_length\nTypeError: object of type &#39;itertools.chain&#39; has no len()\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:676)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:659)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:997)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:997)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:537)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:543)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2362)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2350)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2349)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2349)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2582)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2517)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2280)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2302)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2321)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2346)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:997)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:392)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:996)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:247)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor325.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 508, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;&lt;command-3485556643975441&gt;&#34;, line 12, in compute_length\nTypeError: object of type &#39;itertools.chain&#39; has no len()\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:676)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:659)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:997)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:997)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:537)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:543)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["\nfrom pyspark.sql.types import *\nfrom pyspark.sql import SparkSession\nfrom pyspark import SparkContext\n\nstudentData=spark.read.csv(\"/FileStore/tables/studentData.csv\",header=True, inferSchema=True)\nstudentData.createOrReplaceTempView(\"student\")\nspark.sql(\"select * from student\").show()\nstudentData.printSchema()\nstudentId=StructField(\"StudentId\",StringType(),True)\nname=StructField(\"name\",StringType(), True)\ngender=StructField(\"gender\",StringType(),True)\ncolumns=[studentId,name,gender]\nstudentSchema=StructType(columns)\n\nstudentData1=spark.read.csv(\"/FileStore/tables/studentData.csv\",header=True,schema=studentSchema)\nstudentData1.createOrReplaceTempView(\"student1\")\nspark.sql(\"select * from student1\").show()\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+-------+------+\nStudentId|   name|gender|\n+---------+-------+------+\n      si1|  Robin|     M|\n      si2|  Maria|     F|\n      si3|  Julie|     F|\n      si4|    Bob|     M|\n      si6|William|     M|\n+---------+-------+------+\n\nroot\n-- StudentId: string (nullable = true)\n-- name: string (nullable = true)\n-- gender: string (nullable = true)\n\n+---------+-------+------+\nStudentId|   name|gender|\n+---------+-------+------+\n      si1|  Robin|     M|\n      si2|  Maria|     F|\n      si3|  Julie|     F|\n      si4|    Bob|     M|\n      si6|William|     M|\n+---------+-------+------+\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql import SparkSession\nfrom pyspark import SparkContext,HiveContext\n\n# File uploaded to /FileStore/tables/baseball_reference_2016_clean.csv\n# File uploaded to /FileStore/tables/baseball_reference_2016_scrape.csv\n\nbaseball_ref_df=spark.read.csv(\"/FileStore/tables/baseball_reference_2016_clean.csv\",inferSchema=True,header=True).repartition(5)\nprint(baseball_ref_df.rdd.getNumPartitions())\nbaseball_ref_scrape_df=spark.read.csv(\"/FileStore/tables/baseball_reference_2016_scrape.csv\",inferSchema=True,header=True)\n                                     \nbaseball_ref_df.createOrReplaceTempView(\"bb\")\n# spark.sql(\"select * from baseball_ref_df\").show()\n\nbaseball_ref_df.printSchema()\n# spark.sql(\"select home_team,count(*) from bb group by home_team\").show()\n#spark.sql(\"select distinct field_type from bb\").show()\n# on grass, on turf\n#spark.sql(\"select distinct game_type from bb\").show()\n# Day Game, Night Game \n#spark.sql(\"select distinct season from bb\").show()\n# post season, regular season\n#spark.sql(\"select distinct day_of_week from bb\").show()\n# Home Team Win By day-of-week \n# Home Team Win By game_type\n# Home Team Win By season \n# Home Team Win By field_type \n# spark.sql(\"select day_of_week,count(home_team_win) from bb where home_team_win=1 group by 1 order by 2 desc\").show()\n\n# Identify which team won more games during Night Game \n#bb_df1=spark.sql(\"select CASE WHEN home_team_win=1 THEN home_team ELSE away_team END as winning_team, game_type, count(*) as win_count from bb group by 1,2 order by 3 DESC\")\nprint(\"bb_df1 no. of partitions : {0}\".format(bb_df1.rdd.getNumPartitions()))\n#bb_df1.createOrReplaceTempView(\"bb_df1\")\n# Identify Team which got more victories by game_Type\n#spark.sql(\"select game_type, winning_team,win_count from (select winning_team,game_type,win_count, RANK() OVER(PARTITION BY game_type ORDER BY win_count DESC) as rank from bb_df1)tmp where rank=1\").show()\n# Identify Team which got least victories by game_type \n#spark.sql(\"select game_type, winning_team,win_count from (select winning_team,game_type, win_count, RANK() OVER(PARTITION BY game_type ORDER BY win_count ASC) as rank from bb_df1)tmp where rank=1\").show()\n\nbaseball_ref_df.repartitionByRange(4,\"_c0\")\nprint(baseball_ref_df.rdd.getNumPartitions())\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">5\nroot\n-- _c0: integer (nullable = true)\n-- attendance: double (nullable = true)\n-- away_team: string (nullable = true)\n-- away_team_errors: integer (nullable = true)\n-- away_team_hits: integer (nullable = true)\n-- away_team_runs: integer (nullable = true)\n-- date: timestamp (nullable = true)\n-- field_type: string (nullable = true)\n-- game_type: string (nullable = true)\n-- home_team: string (nullable = true)\n-- home_team_errors: integer (nullable = true)\n-- home_team_hits: integer (nullable = true)\n-- home_team_runs: integer (nullable = true)\n-- start_time: string (nullable = true)\n-- venue: string (nullable = true)\n-- day_of_week: string (nullable = true)\n-- temperature: double (nullable = true)\n-- wind_speed: double (nullable = true)\n-- wind_direction: string (nullable = true)\n-- sky: string (nullable = true)\n-- total_runs: integer (nullable = true)\n-- game_hours_dec: double (nullable = true)\n-- season: string (nullable = true)\n-- home_team_win: integer (nullable = true)\n-- home_team_loss: integer (nullable = true)\n-- home_team_outcome: string (nullable = true)\n\nbb_df1 no. of partitions : 32\n5\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark import SparkContext,HiveContext\ndata = \\\n  [(\"Thin\", \"Cell Phone\", 6000),\n  (\"Normal\", \"Tablet\", 1500),\n  (\"Mini\", \"Tablet\", 5500),\n  (\"Ultra thin\", \"Cell Phone\", 5500),\n  (\"Very thin\", \"Cell Phone\", 6000),\n  (\"Big\", \"Tablet\", 2500),\n  (\"Bendable\", \"Cell Phone\", 3000),\n  (\"Foldable\", \"Cell Phone\", 3000),\n  (\"Pro\", \"Tablet\", 4500),\n  (\"Pro2\", \"Tablet\", 6500)]\ndf = spark.createDataFrame(data, [\"product\", \"category\", \"revenue\"])\ndf.registerTempTable(\"productRevenue\")\nspark.sql(\"select * from productRevenue\").show()\n\n# Find Best Selling and Second Best Selling Prouducts by Category\nspark.sql(\"select product, category,revenue from \\\n              (select product, category, revenue,dense_rank() OVER(PARTITION BY category ORDER BY revenue DESC) as rank  \\\n            from productRevenue)tmp where rank in (1,2)\").show()\n\n# What is the difference between the revenue of a product and the revenue of the best selling product in the same category as this product?\nspark.sql(\"select product, category, revenue, (max_revenue_by_cat-revenue) as revenue_diff from (select product, category, revenue,first_value(revenue) OVER(PARTITION BY category ORDER BY revenue DESC) as max_revenue_by_cat from productRevenue)tmp\").show()\n\n#result3=spark.sql(\"select  product, category,revenue_totals,agg_revenue,avg_revenue from (select /*+ REPARTITION(5) */ product, category, COLLECT_LIST(revenue) OVER(PARTITION BY category order by revenue DESC) as revenue_totals, SUM(revenue) OVER(PARTITION BY category order by revenue DESC) as agg_revenue, AVG(revenue) OVER(PARTITION BY category order by revenue DESC) as avg_revenue from productRevenue)tmp\")\n\nspark.sql(\"select  product, category,revenue_totals,agg_revenue,avg_revenue from (select /*+ REPARTITION(5) */ product, category, COLLECT_LIST(revenue) OVER(PARTITION BY category order by revenue DESC) as revenue_totals, SUM(revenue) OVER(PARTITION BY category order by revenue DESC) as agg_revenue, AVG(revenue) OVER(PARTITION BY category order by revenue DESC) as avg_revenue from productRevenue)tmp\").explain()\n#print(result3.rdd.getNumPartitions())\n# SELECT /*+ REPARTITION(numPartitions) */\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+----------+-------+\n   product|  category|revenue|\n+----------+----------+-------+\n      Thin|Cell Phone|   6000|\n    Normal|    Tablet|   1500|\n      Mini|    Tablet|   5500|\nUltra thin|Cell Phone|   5500|\n Very thin|Cell Phone|   6000|\n       Big|    Tablet|   2500|\n  Bendable|Cell Phone|   3000|\n  Foldable|Cell Phone|   3000|\n       Pro|    Tablet|   4500|\n      Pro2|    Tablet|   6500|\n+----------+----------+-------+\n\n+----------+----------+-------+\n   product|  category|revenue|\n+----------+----------+-------+\n      Pro2|    Tablet|   6500|\n      Mini|    Tablet|   5500|\n      Thin|Cell Phone|   6000|\n Very thin|Cell Phone|   6000|\nUltra thin|Cell Phone|   5500|\n+----------+----------+-------+\n\n+----------+----------+-------+------------+\n   product|  category|revenue|revenue_diff|\n+----------+----------+-------+------------+\n      Pro2|    Tablet|   6500|           0|\n      Mini|    Tablet|   5500|        1000|\n       Pro|    Tablet|   4500|        2000|\n       Big|    Tablet|   2500|        4000|\n    Normal|    Tablet|   1500|        5000|\n      Thin|Cell Phone|   6000|           0|\n Very thin|Cell Phone|   6000|           0|\nUltra thin|Cell Phone|   5500|         500|\n  Bendable|Cell Phone|   3000|        3000|\n  Foldable|Cell Phone|   3000|        3000|\n+----------+----------+-------+------------+\n\n== Physical Plan ==\nExchange RoundRobinPartitioning(5), [id=#889]\n+- Window [product#3285, category#3286, collect_list(revenue#3287L, 0, 0) windowspecdefinition(category#3286, revenue#3287L DESC NULLS LAST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS revenue_totals#3757, sum(revenue#3287L) windowspecdefinition(category#3286, revenue#3287L DESC NULLS LAST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS agg_revenue#3758L, avg(revenue#3287L) windowspecdefinition(category#3286, revenue#3287L DESC NULLS LAST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS avg_revenue#3759], [category#3286], [revenue#3287L DESC NULLS LAST]\n   +- Sort [category#3286 ASC NULLS FIRST, revenue#3287L DESC NULLS LAST], false, 0\n      +- Exchange hashpartitioning(category#3286, 200), [id=#886]\n         +- *(1) Scan ExistingRDD[product#3285,category#3286,revenue#3287L]\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["from pyspark.sql.functions import explode\n\ndata_df=spark.read.json(\"/FileStore/tables/Sample_json.json\",multiLine=True)\ndata_df.createOrReplaceTempView(\"data_df\")\ndf1=spark.sql(\"select explode(data) as data_exploded from data_df\")\ndf1.select(df1.data_exploded[0],df1.data_exploded[1],df1.data_exploded[2]).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------+----------------+----------------+\ndata_exploded[0]|data_exploded[1]|data_exploded[2]|\n+----------------+----------------+----------------+\n               1|      Nick Young|          Lakers|\n               2|    Lou Williams|          Lakers|\n              13|    Lebron James|       Cavaliers|\n               4|    Kyrie Irving|       Cavaliers|\n               5|      Chris Paul|          Lakers|\n               6|   Blake Griffin|          Lakers|\n              20|    Kevin Durant|        Warriors|\n              17|   Stephen Curry|        Warriors|\n               9|   Dwight Howard|          Lakers|\n              10|     Kyle Korver|          Lakers|\n              11|      Kyle Lowry|       Cavaliers|\n               2|   DeMar DeRozan|       Cavaliers|\n+----------------+----------------+----------------+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom datetime import datetime, time\nfrom random import randint,random,choices\n\nspark=SparkSession.builder.appName(\"Test Spark Streaming Application\").getOrCreate()\n\nMerchant_Name=[\"Master\",\"VISA\",\"Discover\"]\n\nevent_datetime=datetime.now()\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n\ntrans_df=spark.readStream.format(\"rate\").option(\"rowsPerSecond\",1).load() \n\"\"\"trans_df=spark.readStream.format(\"rate\").withColumn(\"transaction_id\",randint(0,1000)).withColumn(\"transaction_card_type\",random.choice(Merchant_Name)) \\\n\t\t\t\t.withColumn(\"transaction_amount\",round(random.uniform(10.0,450.00),2)) \\\n\t\t\t\t.withColumn(\"transaction_time\",event_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")) \\\n\t\t\t\t.option(\"rowsPerSecond\",100).option(\"numPartitions\",10).load()\n\"\"\"\nprint(trans_df.isStreaming)\n#print(trans_df.id())\n#print(trans_df.name())\nprint(trans_df.explain())\n#print(trans_df.lastProgress())\n\n# trans_df.createOrReplaceTempView(\"df\")\n                            \n# trans_agg=trans_df.groupBy(trans_df.transaction_card_type).agg({\"transaction_amount\":\"sum\"})                            \n                            \n# trans_agg=spark.sql(\"select transaction_card_type, SUM(transaction_amount) from df group by transaction_card_type\")\n#query=trans_agg.writeStream.format(\"console\").outputMode(\"complete\").trigger(Trigger.ProcessingTime(\"10 second\")).start()\nquery=trans_df.writeStream.format(\"console\").outputMode(\"update\").start()\nprint(query.id)\nprint(query.name)\nprint(query.explain())\n#print(query.lastProgress())\n\nquery.awaitTermination()\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\njava.io.IOException: Connection failed\n\tat com.databricks.rpc.Jetty9Client$$anon$1.handleError(Jetty9Client.scala:468)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onFailure(Jetty9Client.scala:395)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyFailure(ResponseNotifier.java:177)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyFailure(ResponseNotifier.java:169)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpExchange.notifyFailureComplete(HttpExchange.java:268)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpExchange.abort(HttpExchange.java:240)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpConversation.abort(HttpConversation.java:149)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpRequest.abort(HttpRequest.java:768)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpDestination.abort(HttpDestination.java:453)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpDestination.failed(HttpDestination.java:234)\n\tat shaded.v9_4.org.eclipse.jetty.client.AbstractConnectionPool$1.failed(AbstractConnectionPool.java:140)\n\tat shaded.v9_4.org.eclipse.jetty.util.Promise$Wrapper.failed(Promise.java:136)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpClient$1$1.failed(HttpClient.java:612)\n\tat shaded.v9_4.org.eclipse.jetty.client.AbstractConnectorHttpClientTransport.connectFailed(AbstractConnectorHttpClientTransport.java:138)\n\tat shaded.v9_4.org.eclipse.jetty.client.AbstractConnectorHttpClientTransport$ClientSelectorManager.connectionFailed(AbstractConnectorHttpClientTransport.java:188)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$Connect.failed(ManagedSelector.java:822)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:254)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.access$1400(ManagedSelector.java:62)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.processSelected(ManagedSelector.java:543)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:401)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:360)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:184)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)\n\tat shaded.v9_4.org.eclipse.jetty.io.SelectorManager.doFinishConnect(SelectorManager.java:355)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:232)\n\t... 11 more"]}}],"execution_count":7},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark import SparkContext \n\nspark=SparkSession.builder.appName(\"Test Applicaation\").getOrCreate() \n\nspark.read.format(\"com.databricks.spark.xml\").option(\"rowTag\",\"person\").load(\"/FileStore/tables/person_data.xml\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-242110151241212&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> spark<span class=\"ansi-blue-fg\">=</span>SparkSession<span class=\"ansi-blue-fg\">.</span>builder<span class=\"ansi-blue-fg\">.</span>appName<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Test Applicaation&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>getOrCreate<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n<span class=\"ansi-green-fg\">----&gt; 6</span><span class=\"ansi-red-fg\"> </span>spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;com.databricks.spark.xml&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;rowTag&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#34;person&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/FileStore/tables/person_data.xml&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">load</span><span class=\"ansi-blue-fg\">(self, path, format, schema, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    164</span>         self<span class=\"ansi-blue-fg\">.</span>options<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">**</span>options<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    165</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">,</span> basestring<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 166</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    167</span>         <span class=\"ansi-green-fg\">elif</span> path <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    168</span>             <span class=\"ansi-green-fg\">if</span> type<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">!=</span> list<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     61</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     65</span>             s <span class=\"ansi-blue-fg\">=</span> e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">.</span>toString<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o347.load.\n: java.lang.ClassNotFoundException: Failed to find data source: com.databricks.spark.xml. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:733)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:280)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:214)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: com.databricks.spark.xml.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23$$anonfun$apply$14.apply(DataSource.scala:710)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23$$anonfun$apply$14.apply(DataSource.scala:710)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23.apply(DataSource.scala:710)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23.apply(DataSource.scala:710)\n\tat scala.util.Try.orElse(Try.scala:84)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:710)\n\t... 13 more\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["# Python Program to demonstrate \n# Basic Array Charactertistics \nimport numpy as np \n# Creating Array Object\narr=np.array([[1,2,3],[4,5,6]])\n\n# Print Type of array Object \nprint(\"Array is of Type: {0}\".format(type(arr)))\n\n# Printing Array Dimensions (axes)\nprint(\"No. of Dimensions: {0}\".format(arr.ndim))\n\n# ndim returns the Number of the dimension in the array\n# shape returns the share of the array\nprint(\"Share of Array: {0}\".format(arr.shape))\n# size returns the size of the array \nprint(\"Size of Array: {0}\".format(arr.size))\n# dtype returns the type of elements in the array\nprint(\"Dtype of Array: {0}\".format(arr.dtype))\n\n# Printing Shape of Array\nprint(\"Shape of Array: {0}\".format(arr.shape))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Array is of Type: &lt;class &#39;numpy.ndarray&#39;&gt;\nNo. of Dimensions: 2\nShare of Array: (2, 3)\nSize of Array: 6\nDtype of Array: int64\nShape of Array: (2, 3)\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["events = spark.read.json(\"/databricks-datasets/structured-streaming/events/\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["events.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/events\")\nspark.sql(\"DROP TABLE IF EXISTS events\")\nspark.sql(\"CREATE TABLE events USING DELTA LOCATION '/mnt/delta/events/'\")\nspark.sql(\"select * from events\").show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+----------+\naction|time      |\n+------+----------+\nClose |1469571524|\nOpen  |1469571526|\nClose |1469571532|\nOpen  |1469571535|\nOpen  |1469571536|\nOpen  |1469571536|\nOpen  |1469571537|\nClose |1469571538|\nOpen  |1469571539|\nClose |1469571550|\nOpen  |1469571551|\nClose |1469571553|\nOpen  |1469571553|\nClose |1469571555|\nClose |1469571559|\nOpen  |1469571559|\nOpen  |1469571559|\nOpen  |1469571564|\nOpen  |1469571570|\nClose |1469571575|\n+------+----------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark import SparkContext\n\nspark=SparkSession.builder.appName(\"filamentDetail\").getOrCreate()\n\n#spark.read.format(\"csv\").option(\"header\",True).load(\"/FileStore/tables/filamentData.csv\").groupBy(\"FilamentType\").agg(\"LifeInHours\":\"mean\").show()\n#groupBy(\"_c0\").agg({\"_c2\":\"mean\"}).show()\n\n#window=Window.partitionBy(\"Gender\")\nswimmers=spark.read.format(\"csv\").option(\"header\",True).load(\"/FileStore/tables/swimmerData.csv\")\nswimmers.createOrReplaceTempView(\"swimmersTbl\")\nspark.sql(\"select a.id, a.Gender,a.Occupation,a.swimTimeInSecond from (select id, Gender, Occupation, swimTimeInSecond, rank() over(partition by Gender ORDER BY swimTimeInSecond DESC) as rank from swimmersTbl)a where a.rank <=2\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+------+----------+----------------+\n id|Gender|Occupation|swimTimeInSecond|\n+---+------+----------+----------------+\nid9|Female|Programmer|           16.83|\nid7|Female|Programmer|            16.8|\nid8|  Male|   Manager|           17.11|\nid1|  Male|Programmer|           16.73|\n+---+------+----------+----------------+\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark import SparkContext\n\nspark=SparkSession.builder.appName(\"filamentDetail\").getOrCreate()\n\nswimmers=spark.read.format(\"csv\").option(\"header\",True).load(\"/FileStore/tables/swimmerData.csv\")\n\n\n#window=Window.partitionBy(\"Gender\", \"Occupation\").orderBy(\"swimTimeInSecond\")\n\n\n#window=Window.partitionBy(\"Gender\")\n#swimmersdf.withColumn(\"swimTimeInSecondsByGender\",collect_set(swimmersdf[\"swimTimeInSecond\"]) over window).show()\nswimmers.createOrReplaceTempView(\"swimmersTbl\")\n# spark.sql(\"select Gender, collect_set(swimTimeInSecond) as swimTimeInSecondsByGender from swimmersTbl group by Gender\").show(truncate=False)\n\nspark.sql(\"select Gender, Occupation, sum(swimTimeInSecond) OVER(PARTITION BY Gender, Occupation ORDER BY swimTimeInSecond) as cum_total from swimmersTbl\").show(truncate=False)\n\n\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+-----------+------------------+\nGender|Occupation |cum_total         |\n+------+-----------+------------------+\nMale  |Programmer |15.65             |\nMale  |Programmer |31.61             |\nMale  |Programmer |48.34             |\nFemale|Programmer |16.8              |\nFemale|Programmer |33.629999999999995|\nMale  |RiskAnalyst|15.27             |\nMale  |RiskAnalyst|31.009999999999998|\nFemale|Manager    |15.56             |\nMale  |Manager    |15.15             |\nMale  |Manager    |32.26             |\nFemale|RiskAnalyst|15.9              |\nFemale|RiskAnalyst|32.24             |\n+------+-----------+------------------+\n\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark import SparkContext\nfrom pyspark.sql.functions import collect_set, explode, length,col\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import IntegerType\n\nspark=SparkSession.builder.appName(\"filamentDetail\").getOrCreate()\n\nflightsdf=spark.read.format(\"csv\").option(\"header\",True).load(\"/FileStore/tables/flights.csv\")\nflightsdf.show(10,truncate=False)\n# Year, Month, DayofMonth, Origin, Dest, FlightNum\n# Departure Delays\n#flightsdf.select(\"Year\",\"Month\",\"DayofMonth\",)\n# Top 10 routes which have most flights\nflightsdf.createOrReplaceTempView(\"flightsTbl\")\n# spark.sql(\"select Origin, Dest, count(distinct FlightNum) as total_flights from flightsTbl group by Origin, Dest order by total_flights DESC\").show(truncate=False)\n#spark.sql(\"select top 10 Origin, Dest, collect_set(FlightNum) as flight_nos, size(collect_set(FlightNum)) as flight_counts from flightsTbl group by Origin, Dest order by size(flight_nos) #DESC\").show(truncate=False)\n\n\n#spark.sql(\"select Year, Month, FlightNum, ArrDelay, WeatherDelay from flightsTbl\").show()\n#spark.sql(\"select Year, Month, FlightNum, SUM(ArrDelay) OVER(PARTITION BY Year, Month) as ArrivalDelay from flightsTbl\").show()\n#spark.sql(\"select distinct  Year, Month from flightsTbl\").show()\n\n# ArrDelay, DepDelay, \n#Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay\n\n\n#spark.sql(\"select  distinct Origin, Dest from flightsTbl where Cancelled = 'NA'\").show()\n#spark.sql(\"select distinct ArrDelay,DepDelay from flightsTbl\").show()\n#DepDelay\n\n# List top 10 routes which has the most ArrivalDelays\n#spark.sql(\"select a.Origin, a.Dest, a.total_arrival_delay from (select distinct Origin, Dest, sum(ArrDelay) OVER(PARTITION BY Origin, Dest) as total_arrival_delay from flightsTbl where #ArrDelay <> 'NA')a  order by a.total_arrival_delay DESC\").show()\n          \nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import col\n#window = Window.\\\n#              partitionBy('col1','col2',\\\n#                          'col3','col4').\\\n#              orderBy(df['col5'].desc())\n#df = df.withColumn(\"rank_based_on_col5\",rank().over(window))\n#df_w_least = df.filter(\"rank_based_on_col5=1\")\n\nflightsdf.filter(col(\"ArrDelay\") != 'NA').select(\"Origin\",\"Dest\",col(\"ArrDelay\").cast(IntegerType()).alias(\"ArrivalDelay\")).groupBy(\"Origin\",\"Dest\").agg({\"ArrivalDelay\":\"sum\"}).withColumnRenamed(\"sum(ArrivalDelay)\",\"Total_Arrival_Delay\").orderBy(\"Total_Arrival_Delay\",ascending=False).limit(10).show()\n\n#.withColumn(\"total_arrival_delay\",flightsdf[\"ArrDelay\"].cast(IntegerType())).show()\n\n#show()#\n\n\n#.withColumn(\"total_arrival_delay\",sum(flightsdf[\"ArrDelay\"].cast(IntegerType())).Window.partitionBy('Origin','Dest')).show()\n#withColumn(\"total_arrival_delay\",sum(\"Arr_Delay\").Window.partitionBy('Origin','Dest')).show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\nYear|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n2008|1    |3         |4        |2003   |1955      |2211   |2225      |WN           |335      |N712SW |128              |150           |116    |-14     |8       |IAD   |TPA |810     |4     |8      |0        |null            |0       |NA          |NA          |NA      |NA           |NA               |\n2008|1    |3         |4        |754    |735       |1002   |1000      |WN           |3231     |N772SW |128              |145           |113    |2       |19      |IAD   |TPA |810     |5     |10     |0        |null            |0       |NA          |NA          |NA      |NA           |NA               |\n2008|1    |3         |4        |628    |620       |804    |750       |WN           |448      |N428WN |96               |90            |76     |14      |8       |IND   |BWI |515     |3     |17     |0        |null            |0       |NA          |NA          |NA      |NA           |NA               |\n2008|1    |3         |4        |926    |930       |1054   |1100      |WN           |1746     |N612SW |88               |90            |78     |-6      |-4      |IND   |BWI |515     |3     |7      |0        |null            |0       |NA          |NA          |NA      |NA           |NA               |\n2008|1    |3         |4        |1829   |1755      |1959   |1925      |WN           |3920     |N464WN |90               |90            |77     |34      |34      |IND   |BWI |515     |3     |10     |0        |null            |0       |2           |0           |0       |0            |32               |\n2008|1    |3         |4        |1940   |1915      |2121   |2110      |WN           |378      |N726SW |101              |115           |87     |11      |25      |IND   |JAX |688     |4     |10     |0        |null            |0       |NA          |NA          |NA      |NA           |NA               |\n2008|1    |3         |4        |1937   |1830      |2037   |1940      |WN           |509      |N763SW |240              |250           |230    |57      |67      |IND   |LAS |1591    |3     |7      |0        |null            |0       |10          |0           |0       |0            |47               |\n2008|1    |3         |4        |1039   |1040      |1132   |1150      |WN           |535      |N428WN |233              |250           |219    |-18     |-1      |IND   |LAS |1591    |7     |7      |0        |null            |0       |NA          |NA          |NA      |NA           |NA               |\n2008|1    |3         |4        |617    |615       |652    |650       |WN           |11       |N689SW |95               |95            |70     |2       |2       |IND   |MCI |451     |6     |19     |0        |null            |0       |NA          |NA          |NA      |NA           |NA               |\n2008|1    |3         |4        |1620   |1620      |1639   |1655      |WN           |810      |N648SW |79               |95            |70     |-16     |0       |IND   |MCI |451     |3     |6      |0        |null            |0       |NA          |NA          |NA      |NA           |NA               |\n+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\nonly showing top 10 rows\n\n+------+----+-------------------+\nOrigin|Dest|Total_Arrival_Delay|\n+------+----+-------------------+\n   LAS| SFO|               7576|\n   LAS| LAX|               7532|\n   LAX| OAK|               7256|\n   LAX| SFO|               6981|\n   LAX| LAS|               6622|\n   SFO| LAS|               6395|\n   SAN| SFO|               6382|\n   SFO| SAN|               6279|\n   SAN| OAK|               6162|\n   LAX| PHX|               5777|\n+------+----+-------------------+\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark import SparkContext\nspark=SparkSession.builder.appName(\"test\").master(\"local\").getOrCreate()\nspark"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.172.232.11:41208\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.5</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":16}],"metadata":{"name":"Learning_3","notebookId":2451590036240534},"nbformat":4,"nbformat_minor":0}
