{"cells":[{"cell_type":"code","source":["from pyspark import SparkContext, SparkConf\nfrom pyspark.sql.types import *\nfor i in spark.sparkContext.getConf().getAll():\n  print('{0} - {1}'.format(i,len(i[0])))\n  \ntemp=spark.range(1,10,2,4)  \nprint(type(temp))\n\nfor i in temp.collect():\n  print(i)\n  \ntemp=spark.range(1,10,2,4)\nprint(temp.rdd.getNumPartitions())\nprint(temp.collect())  \n  \n\n#rdd=sc.parallelize([{\"name\":\"Ram\",\"age\":32,\"sex\":\"M\"},{\"name\":\"Tamy\",\"age\":35,\"sex\":\"F\"},{\"name\":\"Angel\",\"age\":34,\"sex\":\"F\"}])\n#print(spark.createDataFrame(rdd).collect())\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(&#39;spark.databricks.preemption.enabled&#39;, &#39;true&#39;) - 35\n(&#39;spark.hadoop.fs.abfs.impl&#39;, &#39;shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem&#39;) - 25\n(&#39;spark.driver.tempDirectory&#39;, &#39;/local_disk0/tmp&#39;) - 26\n(&#39;spark.hadoop.fs.adl.impl.disable.cache&#39;, &#39;true&#39;) - 38\n(&#39;spark.hadoop.parquet.block.size.row.check.max&#39;, &#39;10&#39;) - 45\n(&#39;spark.hadoop.fs.s3a.connection.maximum&#39;, &#39;200&#39;) - 38\n(&#39;spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2&#39;, &#39;0&#39;) - 60\n(&#39;spark.shuffle.reduceLocality.enabled&#39;, &#39;false&#39;) - 36\n(&#39;spark.sql.streaming.checkpointFileManagerClass&#39;, &#39;com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager&#39;) - 46\n(&#39;spark.executor.extraClassPath&#39;, &#39;/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/api-base--api-base_java-spark_2.4_2.11_deploy.jar:/databricks/jars/api-base--api-base-spark_2.4_2.11_deploy.jar:/databricks/jars/api--rpc--rpc_parser-spark_2.4_2.11_deploy.jar:/databricks/jars/chauffeur-api--api--endpoints--endpoints-spark_2.4_2.11_deploy.jar:/databricks/jars/chauffeur-api--chauffeur-api-spark_2.4_2.11_deploy.jar:/databricks/jars/common--client--client-spark_2.4_2.11_deploy.jar:/databricks/jars/common--common-spark_2.4_2.11_deploy.jar:/databricks/jars/common--credentials--credentials-spark_2.4_2.11_deploy.jar:/databricks/jars/common--hadoop--hadoop-spark_2.4_2.11_deploy.jar:/databricks/jars/common--jetty--client--client-spark_2.4_2.11_deploy.jar:/databricks/jars/common--lazy--lazy-spark_2.4_2.11_deploy.jar:/databricks/jars/common--libcommon_resources.jar:/databricks/jars/common--path--path-spark_2.4_2.11_deploy.jar:/databricks/jars/common--rate-limiter--rate-limiter-spark_2.4_2.11_deploy.jar:/databricks/jars/common--storage--storage-spark_2.4_2.11_deploy.jar:/databricks/jars/daemon--data--client--client-spark_2.4_2.11_deploy.jar:/databricks/jars/daemon--data--client--conf--conf-spark_2.4_2.11_deploy.jar:/databricks/jars/daemon--data--client--utils-spark_2.4_2.11_deploy.jar:/databricks/jars/daemon--data--data-common--data-common-spark_2.4_2.11_deploy.jar:/databricks/jars/dbfs--utils--dbfs-utils-spark_2.4_2.11_deploy.jar:/databricks/jars/extern--acl--auth--auth-spark_2.4_2.11_deploy.jar:/databricks/jars/extern--extern-spark_2.4_2.11_deploy.jar:/databricks/jars/extern--libaws-regions.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-client-common_deploy.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-shim-common_deploy.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-shim-hive1_deploy.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-shim-hive2_deploy.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-shim-loader_deploy.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-spark-client_deploy.jar:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/jsonutil--jsonutil-spark_2.4_2.11_deploy.jar:/databricks/jars/libraries--api--managedLibraries--managedLibraries-spark_2.4_2.11_deploy.jar:/databricks/jars/libraries--api--typemappers--typemappers-spark_2.4_2.11_deploy.jar:/databricks/jars/libraries--libraries-spark_2.4_2.11_deploy.jar:/databricks/jars/logging--log4j-mod--log4j-mod-spark_2.4_2.11_deploy.jar:/databricks/jars/logging--utils--logging-utils-spark_2.4_2.11_deploy.jar:/databricks/jars/s3commit--client--client-spark_2.4_2.11_deploy.jar:/databricks/jars/s3commit--common--common-spark_2.4_2.11_deploy.jar:/databricks/jars/s3--s3-spark_2.4_2.11_deploy.jar:/databricks/jars/----scalapb_090--com.fasterxml.jackson.core__jackson-core__2.9.9_shaded.jar:/databricks/jars/----scalapb_090--com.google.android__annotations__4.1.1.4_shaded.jar:/databricks/jars/----scalapb_090--com.google.api.grpc__proto-google-common-protos__1.12.0_shaded.jar:/databricks/jars/----scalapb_090--com.google.code.gson__gson__2.7_shaded.jar:/databricks/jars/----scalapb_090--com.google.errorprone__error_prone_annotations__2.3.2_shaded.jar:/databricks/jars/----scalapb_090--com.google.guava__guava__20.0_shaded.jar:/databricks/jars/----scalapb_090--com.google.protobuf__protobuf-java__3.7.1_shaded.jar:/databricks/jars/----scalapb_090--com.google.protobuf__protobuf-java-util__3.7.1_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__fastparse_2.11__2.1.2_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__sourcecode_2.11__0.1.6_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-api__1.22.1_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-context__1.22.1_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-core__1.22.1_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-netty__1.22.1_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-protobuf__1.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-protobuf-lite__1.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-stub__1.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.opencensus__opencensus-api__0.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.opencensus__opencensus-contrib-grpc-metrics__0.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.perfmark__perfmark-api__0.16.0_shaded.jar:/databricks/jars/----scalapb_090--org.codehaus.mojo__animal-sniffer-annotations__1.17_shaded.jar:/databricks/jars/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.11_deploy_shaded.jar:/databricks/jars/secret-manager--api--api-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--common--spark-common-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--conf-reader--conf-reader_lib-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--dbutils--dbutils-api-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--antlr--parser-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--common--driver-common-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--display--display-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--driver-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--events-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--ml--ml-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--secret-redaction-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--spark--resources-resources.jar:/databricks/jars/spark--maven-trees--hive-exec-with-glue--hive-12679-patch_deploy.jar:/databricks/jars/spark--maven-trees--hive-exec-with-glue--hive-exec_shaded.jar:/databricks/jars/spark--maven-trees--spark_2.4--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.8.10.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-glue--com.amazonaws__aws-java-sdk-glue__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.carrotsearch--hppc--com.carrotsearch__hppc__0.7.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.chuusai--shapeless_2.11--com.chuusai__shapeless_2.11__2.3.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.7.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.databricks--dbml-local_2.11--com.databricks__dbml-local_2.11__0.5.0-db8-spark2.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.databricks--dbml-local_2.11-tests--com.databricks__dbml-local_2.11-tests__0.5.0-db8-spark2.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.databricks--Rserve--com.databricks__Rserve__1.8-3.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.databricks.scalapb--compilerplugin_2.11--com.databricks.scalapb__compilerplugin_2.11__0.4.15-9.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.databricks.scalapb--scalapb-runtime_2.11--com.databricks.scalapb__scalapb-runtime_2.11__0.4.15-9.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.esotericsoftware--kryo-shaded--com.esotericsoftware__kryo-shaded__4.0.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.esotericsoftware--minlog--com.esotericsoftware__minlog__1.3.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml--classmate--com.fasterxml__classmate__1.0.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.6.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.6.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.6.7.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.6.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.datatype--jackson-datatype-joda--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.6.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.module--jackson-module-paranamer--com.fasterxml.jackson.module__jackson-module-paranamer__2.6.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.module--jackson-module-scala_2.11--com.fasterxml.jackson.module__jackson-module-scala_2.11__2.6.7.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil--jniloader--com.github.fommil__jniloader__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--core--com.github.fommil.netlib__core__1.1.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--native_ref-java--com.github.fommil.netlib__native_ref-java__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--native_ref-java-natives--com.github.fommil.netlib__native_ref-java-natives__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--native_system-java--com.github.fommil.netlib__native_system-java__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--native_system-java-natives--com.github.fommil.netlib__native_system-java-natives__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--netlib-native_ref-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_ref-linux-x86_64-natives__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--netlib-native_system-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_system-linux-x86_64-natives__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.luben--zstd-jni--com.github.luben__zstd-jni__1.3.2-2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.rwl--jtransforms--com.github.rwl__jtransforms__2.4.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__2.0.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.googlecode.javaewah--JavaEWAH--com.googlecode.javaewah__JavaEWAH__0.3.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.google.guava--guava--com.google.guava__guava__15.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.h2database--h2--com.h2database__h2__1.3.174.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.jcraft--jsch--com.jcraft__jsch__0.1.50.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.2.8.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.microsoft.sqlserver--mssql-jdbc--com.microsoft.sqlserver__mssql-jdbc__6.2.2.jre8.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-dbcp--commons-dbcp--commons-dbcp__commons-dbcp__1.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-io--commons-io--commons-io__commons-io__2.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-net--commons-net--commons-net__commons-net__3.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-pool--commons-pool--commons-pool__commons-pool__1.5.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.ning--compress-lzf--com.ning__compress-lzf__1.0.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.sun.mail--javax.mail--com.sun.mail__javax.mail__1.5.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.trueaccord.lenses--lenses_2.11--com.trueaccord.lenses__lenses_2.11__0.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.twitter--chill_2.11--com.twitter__chill_2.11__0.9.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.twitter--chill-java--com.twitter__chill-java__0.9.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.twitter--parquet-hadoop-bundle--com.twitter__parquet-hadoop-bundle__1.6.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.twitter--util-app_2.11--com.twitter__util-app_2.11__6.23.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.twitter--util-core_2.11--com.twitter__util-core_2.11__6.23.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.twitter--util-jvm_2.11--com.twitter__util-jvm_2.11__6.23.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.typesafe--config--com.typesafe__config__1.2.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-api_2.11--com.typesafe.scala-logging__scala-logging-api_2.11__2.1.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-slf4j_2.11--com.typesafe.scala-logging__scala-logging-slf4j_2.11__2.1.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.univocity--univocity-parsers--com.univocity__univocity-parsers__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.vlkan--flatbuffers--com.vlkan__flatbuffers__1.2.0-3f79e055.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.zaxxer--HikariCP--com.zaxxer__HikariCP__3.1.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.airlift--aircompressor--io.airlift__aircompressor__0.10.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-ganglia--io.dropwizard.metrics__metrics-ganglia__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-graphite--io.dropwizard.metrics__metrics-graphite__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-log4j--io.dropwizard.metrics__metrics-log4j__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.netty--netty-all--io.netty__netty-all__4.1.42.Final.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.netty--netty--io.netty__netty__3.9.9.Final.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.activation--activation--javax.activation__activation__1.1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.annotation--javax.annotation-api--javax.annotation__javax.annotation-api__1.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.transaction--jta--javax.transaction__jta__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.validation--validation-api--javax.validation__validation-api__1.1.0.Final.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.ws.rs--javax.ws.rs-api--javax.ws.rs__javax.ws.rs-api__2.0.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.xml.bind--jaxb-api--javax.xml.bind__jaxb-api__2.2.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.xml.stream--stax-api--javax.xml.stream__stax-api__1.0-2.jar:/databricks/jars/spark--maven-trees--spark_2.4--javolution--javolution--javolution__javolution__5.5.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--jline--jline--jline__jline__2.14.6.jar:/databricks/jars/spark--maven-trees--spark_2.4--joda-time--joda-time--joda-time__joda-time__2.9.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--junit--junit--junit__junit__4.12.jar:/databricks/jars/spark--maven-trees--spark_2.4--liball_deps_2.11.jar:/databricks/jars/spark--maven-trees--spark_2.4--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:/databricks/jars/spark--maven-trees--spark_2.4--log4j--log4j--log4j__log4j__1.2.17.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.hydromatic--eigenbase-properties--net.hydromatic__eigenbase-properties__1.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.razorvine--pyrolite--net.razorvine__pyrolite__4.13.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.sf.opencsv--opencsv--net.sf.opencsv__opencsv__2.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.sf.supercsv--super-csv--net.sf.supercsv__super-csv__2.2.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.snowflake--snowflake-ingest-sdk--net.snowflake__snowflake-ingest-sdk__0.9.6.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.snowflake--snowflake-jdbc--net.snowflake__snowflake-jdbc__3.9.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.snowflake--spark-snowflake_2.11--net.snowflake__spark-snowflake_2.11__2.5.3-spark_2.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.sourceforge.f2j--arpack_combined_all--net.sourceforge.f2j__arpack_combined_all__0.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.acplt--oncrpc--org.acplt__oncrpc__1.0.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.antlr--antlr4-runtime--org.antlr__antlr4-runtime__4.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.ant--ant-jsch--org.apache.ant__ant-jsch__1.9.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.arrow--arrow-format--org.apache.arrow__arrow-format__0.10.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.arrow--arrow-memory--org.apache.arrow__arrow-memory__0.10.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.arrow--arrow-vector--org.apache.arrow__arrow-vector__0.10.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.avro--avro-ipc--org.apache.avro__avro-ipc__1.8.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.avro--avro-mapred-hadoop2--org.apache.avro__avro-mapred-hadoop2__1.8.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.calcite--calcite-avatica--org.apache.calcite__calcite-avatica__1.2.0-incubating.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.calcite--calcite-core--org.apache.calcite__calcite-core__1.2.0-incubating.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.calcite--calcite-linq4j--org.apache.calcite__calcite-linq4j__1.2.0-incubating.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.8.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.commons--commons-crypto--org.apache.commons__commons-crypto__1.0.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.derby--derby--org.apache.derby__derby__10.12.1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-client--org.apache.hadoop__hadoop-client__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-hdfs--org.apache.hadoop__hadoop-hdfs__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-app--org.apache.hadoop__hadoop-mapreduce-client-app__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-common--org.apache.hadoop__hadoop-mapreduce-client-common__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-core--org.apache.hadoop__hadoop-mapreduce-client-core__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-jobclient--org.apache.hadoop__hadoop-mapreduce-client-jobclient__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-shuffle--org.apache.hadoop__hadoop-mapreduce-client-shuffle__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-yarn-api--org.apache.hadoop__hadoop-yarn-api__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-yarn-client--org.apache.hadoop__hadoop-yarn-client__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-yarn-common--org.apache.hadoop__hadoop-yarn-common__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-yarn-server-common--org.apache.hadoop__hadoop-yarn-server-common__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.6.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.10.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.ivy--ivy--org.apache.ivy__ivy__2.4.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.orc--orc-core-nohive--org.apache.orc__orc-core-nohive__1.5.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.orc--orc-mapreduce-nohive--org.apache.orc__orc-mapreduce-nohive__1.5.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.orc--orc-shims--org.apache.orc__orc-shims__1.5.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-column--org.apache.parquet__parquet-column__1.10.1.2-databricks4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-common--org.apache.parquet__parquet-common__1.10.1.2-databricks4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-encoding--org.apache.parquet__parquet-encoding__1.10.1.2-databricks4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-format--org.apache.parquet__parquet-format__2.4.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-hadoop--org.apache.parquet__parquet-hadoop__1.10.1.2-databricks4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-jackson--org.apache.parquet__parquet-jackson__1.10.1.2-databricks4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.9.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.xbean--xbean-asm6-shaded--org.apache.xbean__xbean-asm6-shaded__4.8.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-jaxrs--org.codehaus.jackson__jackson-jaxrs__1.9.13.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-xc--org.codehaus.jackson__jackson-xc__1.9.13.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.janino--commons-compiler--org.codehaus.janino__commons-compiler__3.0.10.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.janino--janino--org.codehaus.janino__janino__3.0.10.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__3.2.6.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__3.2.10.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__3.2.9.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-client--org.eclipse.jetty__jetty-client__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-continuation--org.eclipse.jetty__jetty-continuation__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-http--org.eclipse.jetty__jetty-http__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-io--org.eclipse.jetty__jetty-io__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-jndi--org.eclipse.jetty__jetty-jndi__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-plus--org.eclipse.jetty__jetty-plus__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-proxy--org.eclipse.jetty__jetty-proxy__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-security--org.eclipse.jetty__jetty-security__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-server--org.eclipse.jetty__jetty-server__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlet--org.eclipse.jetty__jetty-servlet__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlets--org.eclipse.jetty__jetty-servlets__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-util--org.eclipse.jetty__jetty-util__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-webapp--org.eclipse.jetty__jetty-webapp__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-xml--org.eclipse.jetty__jetty-xml__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.fusesource.leveldbjni--leveldbjni-all--org.fusesource.leveldbjni__leveldbjni-all__1.8.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2.external--aopalliance-repackaged--org.glassfish.hk2.external__aopalliance-repackaged__2.4.0-b34.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2.external--javax.inject--org.glassfish.hk2.external__javax.inject__2.4.0-b34.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2--hk2-api--org.glassfish.hk2__hk2-api__2.4.0-b34.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2--hk2-locator--org.glassfish.hk2__hk2-locator__2.4.0-b34.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2--hk2-utils--org.glassfish.hk2__hk2-utils__2.4.0-b34.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2--osgi-resource-locator--org.glassfish.hk2__osgi-resource-locator__1.0.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.bundles.repackaged--jersey-guava--org.glassfish.jersey.bundles.repackaged__jersey-guava__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.containers--jersey-container-servlet-core--org.glassfish.jersey.containers__jersey-container-servlet-core__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.containers--jersey-container-servlet--org.glassfish.jersey.containers__jersey-container-servlet__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.core--jersey-client--org.glassfish.jersey.core__jersey-client__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.core--jersey-common--org.glassfish.jersey.core__jersey-common__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.core--jersey-server--org.glassfish.jersey.core__jersey-server__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.media--jersey-media-jaxb--org.glassfish.jersey.media__jersey-media-jaxb__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.hamcrest--hamcrest-core--org.hamcrest__hamcrest-core__1.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.hamcrest--hamcrest-library--org.hamcrest__hamcrest-library__1.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.hibernate--hibernate-validator--org.hibernate__hibernate-validator__5.1.1.Final.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.iq80.snappy--snappy--org.iq80.snappy__snappy__0.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.javassist--javassist--org.javassist__javassist__3.18.1-GA.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.jboss.logging--jboss-logging--org.jboss.logging__jboss-logging__3.1.3.GA.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.jdbi--jdbi--org.jdbi__jdbi__2.63.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.joda--joda-convert--org.joda__joda-convert__1.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.jodd--jodd-core--org.jodd__jodd-core__3.5.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.json4s--json4s-ast_2.11--org.json4s__json4s-ast_2.11__3.5.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.json4s--json4s-core_2.11--org.json4s__json4s-core_2.11__3.5.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.json4s--json4s-jackson_2.11--org.json4s__json4s-jackson_2.11__3.5.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.json4s--json4s-scalap_2.11--org.json4s__json4s-scalap_2.11__3.5.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.lz4--lz4-java--org.lz4__lz4-java__1.4.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.mariadb.jdbc--mariadb-java-client--org.mariadb.jdbc__mariadb-java-client__2.1.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.mockito--mockito-core--org.mockito__mockito-core__1.10.19.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.objenesis--objenesis--org.objenesis__objenesis__2.5.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.postgresql--postgresql--org.postgresql__postgresql__42.1.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.roaringbitmap--RoaringBitmap--org.roaringbitmap__RoaringBitmap__0.7.45.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.roaringbitmap--shims--org.roaringbitmap__shims__0.7.45.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.rocksdb--rocksdbjni--org.rocksdb__rocksdbjni__6.2.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.rosuda.REngine--REngine--org.rosuda.REngine__REngine__2.1.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.scalacheck--scalacheck_2.11--org.scalacheck__scalacheck_2.11__1.12.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.scalactic--scalactic_2.11--org.scalactic__scalactic_2.11__3.0.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-parser-combinators_2.11--org.scala-lang.modules__scala-parser-combinators_2.11__1.1.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-xml_2.11--org.scala-lang.modules__scala-xml_2.11__1.0.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.scala-lang--scala-compiler_2.11--org.scala-lang__scala-compiler__2.11.12.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.scala-lang--scala-library_2.11--org.scala-lang__scala-library__2.11.12.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.scala-lang--scala-reflect_2.11--org.scala-lang__scala-reflect__2.11.12.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.scalanlp--breeze_2.11--org.scalanlp__breeze_2.11__0.13.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.scalanlp--breeze-macros_2.11--org.scalanlp__breeze-macros_2.11__0.13.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.scala-sbt--test-interface--org.scala-sbt__test-interface__1.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.scalatest--scalatest_2.11--org.scalatest__scalatest_2.11__3.0.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.slf4j--jcl-over-slf4j--org.slf4j__jcl-over-slf4j__1.7.16.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.slf4j--jul-to-slf4j--org.slf4j__jul-to-slf4j__1.7.16.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.16.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.16.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.spark-project.hive--hive-beeline--org.spark-project.hive__hive-beeline__1.2.1.spark2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.spark-project.hive--hive-cli--org.spark-project.hive__hive-cli__1.2.1.spark2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.spark-project.hive--hive-jdbc--org.spark-project.hive__hive-jdbc__1.2.1.spark2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.spark-project.hive--hive-metastore--org.spark-project.hive__hive-metastore__1.2.1.spark2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.spark-project.spark--unused--org.spark-project.spark__unused__1.0.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.spire-math--spire_2.11--org.spire-math__spire_2.11__0.13.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.spire-math--spire-macros_2.11--org.spire-math__spire-macros_2.11__0.13.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.springframework--spring-core--org.springframework__spring-core__4.1.4.RELEASE.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.springframework--spring-test--org.springframework__spring-test__4.1.4.RELEASE.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.tukaani--xz--org.tukaani__xz__1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.typelevel--machinist_2.11--org.typelevel__machinist_2.11__0.6.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.typelevel--macro-compat_2.11--org.typelevel__macro-compat_2.11__1.1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.xerial--sqlite-jdbc--org.xerial__sqlite-jdbc__3.8.11.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.yaml--snakeyaml--org.yaml__snakeyaml__1.16.jar:/databricks/jars/spark--maven-trees--spark_2.4--oro--oro--oro__oro__2.0.8.jar:/databricks/jars/spark--maven-trees--spark_2.4--software.amazon.ion--ion-java--software.amazon.ion__ion-java__1.0.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--stax--stax-api--stax__stax-api__1.0.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--xmlenc--xmlenc--xmlenc__xmlenc__0.52.jar:/databricks/jars/spark--sql-extension--sql-extension-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--versions--2.4--avro_2.11_deploy_shaded.jar:/databricks/jars/spark--versions--2.4--catalyst_2.11_deploy.jar:/databricks/jars/spark--versions--2.4--com.fasterxml.jackson.core__jackson-annotations__2.9.0_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--com.fasterxml.jackson.core__jackson-core__2.9.4_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--com.fasterxml.jackson.core__jackson-databind__2.9.4_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--com.fasterxml.jackson.jaxrs__jackson-jaxrs-base__2.9.4_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--com.fasterxml.jackson.jaxrs__jackson-jaxrs-json-provider__2.9.4_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--com.fasterxml.jackson.module__jackson-module-jaxb-annotations__2.9.4_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--com.github.shyiko__mysql-binlog-connector-java__0.16.1_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--com.google.guava__guava__20.0_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--core_2.11_deploy.jar:/databricks/jars/spark--versions--2.4--debezium-connector-mysql_2.11_deploy_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--ganglia-lgpl_2.11_deploy.jar:/databricks/jars/spark--versions--2.4--graphx_2.11_deploy.jar:/databricks/jars/spark--versions--2.4--hive_2.11_deploy_shaded.jar:/databricks/jars/spark--versions--2.4--hive-thriftserver_2.11_deploy.jar:/databricks/jars/spark--versions--2.4--io.confluent__common-utils__4.0.0_shaded-for-avro.jar:/databricks/jars/spark--versions--2.4--io.confluent__kafka-schema-registry-client__4.0.0_shaded-for-avro.jar:/databricks/jars/spark--versions--2.4--io.debezium__debezium-core__0.8.3.Final_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--io.debezium__debezium-ddl-parser__0.8.3.Final_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--io.debezium__debezium-embedded__0.8.3.Final_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--javax.annotation__javax.annotation-api__1.2_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--javax.servlet__javax.servlet-api__3.1.0_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--javax.validation__validation-api__1.1.0.Final_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--javax.ws.rs__javax.ws.rs-api__2.0.1_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--kafka-clients_only_shaded.jar:/databricks/jars/spark--versions--2.4--libspark-sql-parser-compiled.jar:/databricks/jars/spark--versions--2.4--mllib_2.11_deploy_shaded.jar:/databricks/jars/spark--versions--2.4--mllib-local_2.11_deploy.jar:/databricks/jars/spark--versions--2.4--mysql-cdc_2.11_deploy_shaded.jar:/databricks/jars/spark--versions--2.4--net.sourceforge.argparse4j__argparse4j__0.7.0_shaded-for-mysql-cdc.jar:/databricks/jars/spark--ve\n*** WARNING: skipped 16025 bytes of output ***\n\n\n(&#39;spark.databricks.clusterUsageTags.driverNodeType&#39;, &#39;dev-tier-node&#39;) - 48\n(&#39;spark.hadoop.spark.sql.sources.outputCommitterClass&#39;, &#39;com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter&#39;) - 51\n(&#39;spark.databricks.clusterUsageTags.instanceBootstrapType&#39;, &#39;ssh&#39;) - 55\n(&#39;spark.repl.class.uri&#39;, &#39;spark://10.172.226.154:40802/classes&#39;) - 20\n(&#39;spark.streaming.driver.writeAheadLog.allowBatching&#39;, &#39;true&#39;) - 50\n(&#39;spark.databricks.clusterSource&#39;, &#39;UI&#39;) - 30\n(&#39;spark.hadoop.hive.server2.transport.mode&#39;, &#39;http&#39;) - 40\n(&#39;spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled&#39;, &#39;false&#39;) - 57\n(&#39;spark.databricks.driverNodeTypeId&#39;, &#39;dev-tier-node&#39;) - 33\n(&#39;spark.sql.parquet.compression.codec&#39;, &#39;snappy&#39;) - 35\n(&#39;spark.databricks.passthrough.glue.executorServiceFactoryClassName&#39;, &#39;com.databricks.backend.daemon.driver.GlueClientExecutorServiceFactory&#39;) - 65\n(&#39;spark.databricks.clusterUsageTags.hailEnabled&#39;, &#39;false&#39;) - 45\n(&#39;spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled&#39;, &#39;false&#39;) - 59\n(&#39;spark.databricks.clusterUsageTags.containerType&#39;, &#39;LXC&#39;) - 47\n(&#39;spark.eventLog.enabled&#39;, &#39;false&#39;) - 22\n(&#39;spark.hadoop.fs.wasb.impl&#39;, &#39;shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem&#39;) - 25\n(&#39;spark.executor.tempDirectory&#39;, &#39;/local_disk0/tmp&#39;) - 28\n(&#39;spark.databricks.clusterUsageTags.clusterOwnerUserId&#39;, &#39;6647443176029732&#39;) - 52\n(&#39;spark.databricks.clusterUsageTags.clusterAllTags&#39;, &#39;[{&#34;key&#34;:&#34;Name&#34;,&#34;value&#34;:&#34;ce2-worker&#34;}]&#39;) - 48\n(&#39;spark.databricks.workerNodeTypeId&#39;, &#39;dev-tier-node&#39;) - 33\n(&#39;spark.driver.port&#39;, &#39;40802&#39;) - 17\n(&#39;spark.hadoop.mapred.output.committer.class&#39;, &#39;com.databricks.backend.daemon.data.client.DirectOutputCommitter&#39;) - 42\n(&#39;spark.hadoop.hive.server2.thrift.http.port&#39;, &#39;10000&#39;) - 42\n(&#39;spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version&#39;, &#39;2&#39;) - 60\n(&#39;spark.sql.allowMultipleContexts&#39;, &#39;false&#39;) - 31\n(&#39;spark.databricks.clusterUsageTags.clusterEbsVolumeSize&#39;, &#39;0&#39;) - 54\n(&#39;spark.home&#39;, &#39;/databricks/spark&#39;) - 10\n(&#39;spark.databricks.clusterUsageTags.clusterTargetWorkers&#39;, &#39;0&#39;) - 54\n(&#39;spark.sql.warehouse.dir&#39;, &#39;/user/hive/warehouse&#39;) - 23\n(&#39;spark.hadoop.hive.server2.idle.operation.timeout&#39;, &#39;7200000&#39;) - 48\n(&#39;spark.task.reaper.enabled&#39;, &#39;true&#39;) - 25\n(&#39;spark.databricks.passthrough.s3a.tokenProviderClassName&#39;, &#39;com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider&#39;) - 55\n(&#39;spark.storage.memoryFraction&#39;, &#39;0.5&#39;) - 28\n(&#39;spark.databricks.session.share&#39;, &#39;false&#39;) - 30\n(&#39;spark.databricks.clusterUsageTags.clusterResourceClass&#39;, &#39;default&#39;) - 54\n(&#39;spark.databricks.clusterUsageTags.clusterFirstOnDemand&#39;, &#39;0&#39;) - 54\n(&#39;spark.driver.maxResultSize&#39;, &#39;4g&#39;) - 26\n(&#39;spark.databricks.clusterUsageTags.clusterName&#39;, &#39;test_2&#39;) - 45\n(&#39;spark.databricks.delta.multiClusterWrites.enabled&#39;, &#39;true&#39;) - 49\n(&#39;spark.databricks.clusterUsageTags.clusterSku&#39;, &#39;STANDARD_SKU&#39;) - 44\n(&#39;spark.worker.cleanup.enabled&#39;, &#39;false&#39;) - 28\n(&#39;spark.databricks.clusterUsageTags.enableCredentialPassthrough&#39;, &#39;false&#39;) - 61\n(&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType&#39;, &#39;ebs_volume_type: GENERAL_PURPOSE_SSD\\n&#39;) - 62\n(&#39;spark.databricks.clusterUsageTags.enableJdbcAutoStart&#39;, &#39;true&#39;) - 53\n(&#39;spark.databricks.sparkContextId&#39;, &#39;4070508562953178873&#39;) - 31\n(&#39;spark.hadoop.spark.thriftserver.closeSessionHeaderName&#39;, &#39;X-Databricks-SqlService-CloseSession&#39;) - 54\n(&#39;spark.databricks.passthrough.glue.credentialsProviderFactoryClassName&#39;, &#39;com.databricks.backend.daemon.driver.aws.DatabricksCredentialProviderFactory&#39;) - 69\n(&#39;spark.hadoop.fs.wasb.impl.disable.cache&#39;, &#39;true&#39;) - 39\n(&#39;spark.databricks.clusterUsageTags.clusterEbsVolumeType&#39;, &#39;GENERAL_PURPOSE_SSD&#39;) - 54\n(&#39;spark.databricks.clusterUsageTags.clusterLogDestination&#39;, &#39;&#39;) - 55\n(&#39;spark.cleaner.referenceTracking.blocking&#39;, &#39;false&#39;) - 40\n(&#39;spark.hadoop.parquet.page.size.check.estimate&#39;, &#39;false&#39;) - 45\n(&#39;spark.r.sql.derby.temp.dir&#39;, &#39;/tmp/RtmpsHOFBM&#39;) - 26\n(&#39;spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class&#39;, &#39;com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory&#39;) - 65\n(&#39;spark.databricks.delta.preview.enabled&#39;, &#39;true&#39;) - 38\n(&#39;spark.databricks.clusterUsageTags.isSingleUserCluster&#39;, &#39;false&#39;) - 53\n(&#39;spark.databricks.clusterUsageTags.clusterState&#39;, &#39;Pending&#39;) - 46\n(&#39;spark.databricks.tahoe.logStore.azure.class&#39;, &#39;com.databricks.tahoe.store.AzureLogStore&#39;) - 43\n(&#39;spark.hadoop.fs.azure.skip.metrics&#39;, &#39;true&#39;) - 34\n(&#39;spark.databricks.clusterUsageTags.sparkVersion&#39;, &#39;6.2.x-scala2.11&#39;) - 46\n(&#39;spark.scheduler.mode&#39;, &#39;FAIR&#39;) - 20\n(&#39;spark.master&#39;, &#39;local[8]&#39;) - 12\n(&#39;spark.databricks.delta.logStore.crossCloud.fatal&#39;, &#39;true&#39;) - 48\n(&#39;spark.databricks.clusterUsageTags.clusterWorkers&#39;, &#39;0&#39;) - 48\n(&#39;spark.repl.class.outputDir&#39;, &#39;/local_disk0/tmp/repl/spark-4070508562953178873-646dfc5f-e332-4f4b-bea9-b4264ea1a7a5&#39;) - 26\n(&#39;spark.files.fetchFailure.unRegisterOutputOnHost&#39;, &#39;true&#39;) - 47\n(&#39;spark.hadoop.fs.s3n.impl&#39;, &#39;com.databricks.s3a.S3AFileSystem&#39;) - 24\n(&#39;spark.databricks.clusterUsageTags.driverContainerPrivateIp&#39;, &#39;10.172.226.154&#39;) - 58\n(&#39;spark.databricks.clusterUsageTags.enableSqlAclsOnly&#39;, &#39;false&#39;) - 51\n(&#39;spark.databricks.clusterUsageTags.clusterEbsVolumeCount&#39;, &#39;0&#39;) - 55\n(&#39;spark.databricks.clusterUsageTags.clusterNumSshKeys&#39;, &#39;0&#39;) - 51\n(&#39;spark.databricks.tahoe.logStore.aws.class&#39;, &#39;com.databricks.tahoe.store.S3LockBasedLogStore&#39;) - 41\n(&#39;spark.speculation.quantile&#39;, &#39;0.9&#39;) - 26\n(&#39;spark.shuffle.manager&#39;, &#39;SORT&#39;) - 21\n(&#39;spark.files.overwrite&#39;, &#39;true&#39;) - 21\n(&#39;spark.sql.hive.metastore.sharedPrefixes&#39;, &#39;org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks&#39;) - 39\n(&#39;spark.databricks.io.directoryCommit.enableLogicalDelete&#39;, &#39;false&#39;) - 55\n(&#39;spark.task.reaper.killTimeout&#39;, &#39;60s&#39;) - 29\n(&#39;spark.r.numRBackendThreads&#39;, &#39;1&#39;) - 26\n(&#39;spark.hadoop.fs.wasbs.impl.disable.cache&#39;, &#39;true&#39;) - 40\n(&#39;spark.hadoop.parquet.block.size.row.check.min&#39;, &#39;10&#39;) - 45\n(&#39;spark.hadoop.hive.server2.use.SSL&#39;, &#39;true&#39;) - 33\n(&#39;spark.hadoop.fs.abfss.impl.disable.cache&#39;, &#39;true&#39;) - 40\n(&#39;spark.sql.hive.metastore.version&#39;, &#39;0.13.0&#39;) - 32\n(&#39;spark.shuffle.service.port&#39;, &#39;4048&#39;) - 26\n(&#39;spark.databricks.acl.client&#39;, &#39;com.databricks.spark.sql.acl.client.SparkSqlAclClient&#39;) - 27\n(&#39;spark.databricks.clusterUsageTags.clusterAvailability&#39;, &#39;ON_DEMAND&#39;) - 53\n(&#39;spark.hadoop.hive.warehouse.subdir.inherit.perms&#39;, &#39;false&#39;) - 48\n(&#39;spark.streaming.driver.writeAheadLog.closeFileAfterWrite&#39;, &#39;true&#39;) - 56\n(&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb&#39;, &#39;0&#39;) - 64\n(&#39;spark.hadoop.hive.server2.keystore.path&#39;, &#39;/databricks/keys/jetty-ssl-driver-keystore.jks&#39;) - 39\n(&#39;spark.ui.port&#39;, &#39;43605&#39;) - 13\n(&#39;spark.databricks.credential.redactor&#39;, &#39;com.databricks.logging.secrets.CredentialRedactorProxyImpl&#39;) - 36\n(&#39;spark.databricks.clusterUsageTags.clusterPinned&#39;, &#39;false&#39;) - 47\n(&#39;spark.sql.ui.retainedExecutions&#39;, &#39;100&#39;) - 31\n(&#39;spark.databricks.acl.provider&#39;, &#39;com.databricks.sql.acl.ReflectionBackedAclProvider&#39;) - 29\n(&#39;spark.extraListeners&#39;, &#39;com.databricks.backend.daemon.driver.DBCEventLoggingListener&#39;) - 20\n(&#39;spark.executor.memory&#39;, &#39;4800m&#39;) - 21\n(&#39;spark.databricks.clusterUsageTags.enableElasticDisk&#39;, &#39;false&#39;) - 51\n(&#39;spark.sql.parquet.cacheMetadata&#39;, &#39;true&#39;) - 31\n(&#39;spark.databricks.clusterUsageTags.driverContainerId&#39;, &#39;b16e7a40c8e141abbca31c0ba0f32beb&#39;) - 51\n(&#39;spark.hadoop.fs.adl.impl&#39;, &#39;com.databricks.adl.AdlFileSystem&#39;) - 24\n(&#39;spark.databricks.clusterUsageTags.clusterNodeType&#39;, &#39;dev-tier-node&#39;) - 49\n(&#39;spark.databricks.clusterUsageTags.enableLocalDiskEncryption&#39;, &#39;false&#39;) - 59\n(&#39;spark.databricks.tahoe.logStore.class&#39;, &#39;com.databricks.tahoe.store.DelegatingLogStore&#39;) - 37\n(&#39;spark.databricks.cloudProvider&#39;, &#39;AWS&#39;) - 30\n(&#39;spark.sql.hive.convertMetastoreParquet&#39;, &#39;true&#39;) - 38\n(&#39;spark.executor.id&#39;, &#39;driver&#39;) - 17\n(&#39;spark.databricks.passthrough.adls.tokenProviderClassName&#39;, &#39;com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider&#39;) - 56\n(&#39;spark.app.name&#39;, &#39;Databricks Shell&#39;) - 14\n(&#39;spark.driver.allowMultipleContexts&#39;, &#39;false&#39;) - 34\n(&#39;spark.databricks.clusterUsageTags.workerEnvironmentId&#39;, &#39;default-worker-env&#39;) - 53\n(&#39;spark.driver.host&#39;, &#39;10.172.226.154&#39;) - 17\n(&#39;spark.rdd.compress&#39;, &#39;true&#39;) - 18\n(&#39;spark.databricks.eventLog.dir&#39;, &#39;eventlogs&#39;) - 29\n(&#39;spark.sql.catalogImplementation&#39;, &#39;hive&#39;) - 31\n(&#39;spark.databricks.clusterUsageTags.clusterCreator&#39;, &#39;Webapp&#39;) - 48\n(&#39;spark.databricks.clusterUsageTags.driverInstancePrivateIp&#39;, &#39;10.172.234.143&#39;) - 57\n(&#39;spark.speculation&#39;, &#39;false&#39;) - 17\n(&#39;spark.hadoop.fs.s3a.multipart.size&#39;, &#39;10485760&#39;) - 34\n(&#39;spark.databricks.clusterUsageTags.cloudProvider&#39;, &#39;AWS&#39;) - 47\n(&#39;spark.hadoop.databricks.dbfs.client.version&#39;, &#39;v1&#39;) - 43\n(&#39;spark.hadoop.hive.server2.session.check.interval&#39;, &#39;60000&#39;) - 48\n(&#39;spark.sql.hive.convertCTAS&#39;, &#39;true&#39;) - 26\n(&#39;spark.metrics.conf&#39;, &#39;/databricks/spark/conf/metrics.properties&#39;) - 18\n(&#39;spark.hadoop.spark.sql.parquet.output.committer.class&#39;, &#39;org.apache.spark.sql.parquet.DirectParquetOutputCommitter&#39;) - 53\n(&#39;spark.hadoop.fs.s3a.fast.upload.default&#39;, &#39;true&#39;) - 39\n(&#39;spark.akka.frameSize&#39;, &#39;256&#39;) - 20\n(&#39;spark.hadoop.fs.s3a.fast.upload&#39;, &#39;true&#39;) - 31\n(&#39;spark.databricks.clusterUsageTags.clusterGeneration&#39;, &#39;0&#39;) - 51\n(&#39;spark.hadoop.fs.s3.impl&#39;, &#39;com.databricks.s3a.S3AFileSystem&#39;) - 23\n(&#39;spark.databricks.clusterUsageTags.driverPublicDns&#39;, &#39;ec2-35-165-181-116.us-west-2.compute.amazonaws.com&#39;) - 49\n(&#39;spark.hadoop.fs.abfs.impl.disable.cache&#39;, &#39;true&#39;) - 39\n(&#39;spark.hadoop.fs.wasbs.impl&#39;, &#39;shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem&#39;) - 26\n(&#39;spark.hadoop.hive.server2.keystore.password&#39;, &#39;gb1gQqZ9ZIHS&#39;) - 43\n(&#39;spark.speculation.multiplier&#39;, &#39;3&#39;) - 28\n(&#39;spark.storage.blockManagerTimeoutIntervalMs&#39;, &#39;300000&#39;) - 43\n(&#39;spark.databricks.overrideDefaultCommitProtocol&#39;, &#39;org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol&#39;) - 46\n(&#39;spark.databricks.clusterUsageTags.clusterNoDriverDaemon&#39;, &#39;false&#39;) - 55\n(&#39;spark.databricks.clusterUsageTags.instanceWorkerEnvId&#39;, &#39;default-worker-env&#39;) - 53\n(&#39;spark.hadoop.parquet.memory.pool.ratio&#39;, &#39;0.5&#39;) - 38\n(&#39;spark.executor.extraJavaOptions&#39;, &#39;-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=256m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Ddatabricks.serviceName=spark-executor-1&#39;) - 31\n(&#39;spark.sparkr.use.daemon&#39;, &#39;false&#39;) - 23\n(&#39;spark.scheduler.listenerbus.eventqueue.capacity&#39;, &#39;20000&#39;) - 47\n(&#39;spark.databricks.clusterUsageTags.clusterScalingType&#39;, &#39;fixed_size&#39;) - 52\n(&#39;spark.databricks.clusterUsageTags.clusterStateMessage&#39;, &#39;Starting Spark&#39;) - 53\n(&#39;spark.sql.hive.metastore.jars&#39;, &#39;/databricks/hive/*&#39;) - 29\n(&#39;spark.databricks.passthrough.adls.gen2.tokenProviderClassName&#39;, &#39;com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider&#39;) - 61\n(&#39;spark.hadoop.parquet.page.write-checksum.enabled&#39;, &#39;true&#39;) - 48\n(&#39;spark.hadoop.databricks.s3commit.client.sslTrustAll&#39;, &#39;false&#39;) - 51\n(&#39;spark.hadoop.fs.s3a.threads.max&#39;, &#39;136&#39;) - 31\n(&#39;spark.r.backendConnectionTimeout&#39;, &#39;604800&#39;) - 32\n(&#39;spark.serializer.objectStreamReset&#39;, &#39;100&#39;) - 34\n(&#39;spark.sql.sources.commitProtocolClass&#39;, &#39;com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol&#39;) - 37\n(&#39;spark.hadoop.hive.server2.idle.session.timeout&#39;, &#39;900000&#39;) - 46\n(&#39;spark.databricks.redactor&#39;, &#39;com.databricks.spark.util.DatabricksSparkLogRedactorProxy&#39;) - 25\n(&#39;spark.databricks.clusterUsageTags.autoTerminationMinutes&#39;, &#39;120&#39;) - 56\n(&#39;spark.databricks.clusterUsageTags.clusterPythonVersion&#39;, &#39;3&#39;) - 54\n(&#39;spark.databricks.clusterUsageTags.enableDfAcls&#39;, &#39;false&#39;) - 46\n(&#39;spark.hadoop.fs.abfss.impl&#39;, &#39;shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem&#39;) - 26\n(&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount&#39;, &#39;0&#39;) - 63\n(&#39;spark.shuffle.service.enabled&#39;, &#39;true&#39;) - 29\n(&#39;spark.hadoop.spark.driverproxy.customHeadersToProperties&#39;, &#39;X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds&#39;) - 56\n(&#39;spark.hadoop.parquet.page.verify-checksum.enabled&#39;, &#39;true&#39;) - 49\n(&#39;spark.hadoop.fs.s3a.multipart.threshold&#39;, &#39;104857600&#39;) - 39\n(&#39;spark.hadoop.fs.s3a.impl&#39;, &#39;com.databricks.s3a.S3AFileSystem&#39;) - 24\n(&#39;spark.databricks.clusterUsageTags.dataPlaneRegion&#39;, &#39;us-west-2&#39;) - 49\n(&#39;spark.rpc.message.maxSize&#39;, &#39;256&#39;) - 25\n(&#39;spark.logConf&#39;, &#39;true&#39;) - 13\n(&#39;spark.databricks.clusterUsageTags.enableJobsAutostart&#39;, &#39;true&#39;) - 53\n(&#39;spark.databricks.clusterUsageTags.clusterMetastoreAccessType&#39;, &#39;RDS_DIRECT&#39;) - 60\n(&#39;spark.hadoop.hive.server2.enable.doAs&#39;, &#39;false&#39;) - 37\n(&#39;eventLog.rolloverIntervalSeconds&#39;, &#39;3600&#39;) - 32\n(&#39;spark.shuffle.memoryFraction&#39;, &#39;0.2&#39;) - 28\n(&#39;spark.databricks.clusterUsageTags.driverInstanceId&#39;, &#39;i-012430e34554631d4&#39;) - 50\n(&#39;spark.databricks.clusterUsageTags.clusterOwnerOrgId&#39;, &#39;5741052354962972&#39;) - 51\n(&#39;spark.databricks.clusterUsageTags.containerZoneId&#39;, &#39;us-west-2c&#39;) - 49\n(&#39;spark.app.id&#39;, &#39;local-1578960722885&#39;) - 12\n(&#39;spark.databricks.clusterUsageTags.clusterId&#39;, &#39;0114-001139-ember136&#39;) - 43\n(&#39;spark.databricks.clusterUsageTags.region&#39;, &#39;us-west-2&#39;) - 40\n(&#39;spark.databricks.clusterUsageTags.clusterSpotBidPricePercent&#39;, &#39;100&#39;) - 60\n(&#39;spark.files.useFetchCache&#39;, &#39;false&#39;) - 25\n&lt;class &#39;pyspark.sql.dataframe.DataFrame&#39;&gt;\nRow(id=1)\nRow(id=3)\nRow(id=5)\nRow(id=7)\nRow(id=9)\n4\n[Row(id=1), Row(id=3), Row(id=5), Row(id=7), Row(id=9)]\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["\n%python\n# Use the Spark CSV datasource with options specifying:\n# - First line of file is a header\n# - Automatically infer the schema of the data\ndata = spark.read.csv(\"/databricks-datasets/samples/population-vs-price/data_geo.csv\", header=\"true\", inferSchema=\"true\")\ndata.createOrReplaceTempView(\"data\")\ndata_df=spark.sql(\"\"\"select `2014 rank` as rank_in_2014, City as City, State as State, `State Code` as State_Code,`2014 Population estimate` as Population_Estimate_2014,`2015 median sales price` as Median_Sales_Price_2015 from data\"\"\")\ndata_df.cache() # Cache data for faster reuse\nprint(data_df.take(1))\nprint(data_df.printSchema())\nprint('{0} - {1}'.format('2014 Rank',data.filter(data.rank_in_2014==None).count()))\nprint('{0} - {1}'.format('City',data_df.filter(data_df.City==None).count()))\nprint('{0} - {1}'.format('State',data_df.filter(data_df.State==None).count()))\nprint('{0} - {1}'.format('State Code',data_df.filter(data_df.State_Code==None).count()))\n#print(data_df.filter(data.City==None).count())\n#print(data.filter(data.City==None).count())\n# drop rows with missing values\n#data_df1 = data_df.dropna() \n#print(data_df1.count())\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2677037226184569&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span> print<span class=\"ansi-blue-fg\">(</span>data_df<span class=\"ansi-blue-fg\">.</span>take<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      9</span> print<span class=\"ansi-blue-fg\">(</span>data_df<span class=\"ansi-blue-fg\">.</span>printSchema<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">---&gt; 10</span><span class=\"ansi-red-fg\"> </span>print<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;{0} - {1}&#39;</span><span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;2014 Rank&#39;</span><span class=\"ansi-blue-fg\">,</span>data<span class=\"ansi-blue-fg\">.</span>filter<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">.</span>rank_in_2014<span class=\"ansi-blue-fg\">==</span><span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>count<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     11</span> print<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;{0} - {1}&#39;</span><span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;City&#39;</span><span class=\"ansi-blue-fg\">,</span>data_df<span class=\"ansi-blue-fg\">.</span>filter<span class=\"ansi-blue-fg\">(</span>data_df<span class=\"ansi-blue-fg\">.</span>City<span class=\"ansi-blue-fg\">==</span><span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>count<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     12</span> print<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;{0} - {1}&#39;</span><span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;State&#39;</span><span class=\"ansi-blue-fg\">,</span>data_df<span class=\"ansi-blue-fg\">.</span>filter<span class=\"ansi-blue-fg\">(</span>data_df<span class=\"ansi-blue-fg\">.</span>State<span class=\"ansi-blue-fg\">==</span><span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>count<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">__getattr__</span><span class=\"ansi-blue-fg\">(self, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1324</span>         <span class=\"ansi-green-fg\">if</span> name <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">in</span> self<span class=\"ansi-blue-fg\">.</span>columns<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1325</span>             raise AttributeError(\n<span class=\"ansi-green-fg\">-&gt; 1326</span><span class=\"ansi-red-fg\">                 &#34;&#39;%s&#39; object has no attribute &#39;%s&#39;&#34; % (self.__class__.__name__, name))\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1327</span>         jc <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>apply<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1328</span>         <span class=\"ansi-green-fg\">return</span> Column<span class=\"ansi-blue-fg\">(</span>jc<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AttributeError</span>: &#39;DataFrame&#39; object has no attribute &#39;rank_in_2014&#39;</div>"]}}],"execution_count":2},{"cell_type":"code","source":["from pyspark.sql.functions import *\ndate_val=spark.createDataFrame([('2015-12-12 11:11:11',)],['dt'])\nprint(date_val.select(year('dt').alias('year')).collect())\nprint(date_val.select(quarter('dt').alias('quarter')).collect())\nprint(date_val.select(month('dt').alias('month')).collect())\nprint(date_val.select(dayofmonth('dt').alias('dayofmonth')).collect())\nprint(date_val.select(dayofweek('dt').alias('dayofweek')).collect())\nprint(date_val.select(dayofyear('dt').alias('dayofyear')).collect())\nprint(date_val.select(hour('dt').alias('hour')).collect())\nprint(date_val.select(minute('dt').alias('minute')).collect())\nprint(date_val.select(second('dt').alias('second')).collect())\nprint(date_val.select(weekofyear('dt').alias('weekofyear')).collect())\n\n'''\n#O/P:\n[Row(year=2015)]\n[Row(quarter=4)]\n[Row(month=12)]\n[Row(dayofmonth=12)]\n[Row(dayofweek=7)]\n[Row(dayofyear=346)]\n[Row(hour=11)]\n[Row(minute=11)]\n[Row(second=11)]\n[Row(weekofyear=50)]\n'''\n\n# Date Functions in a DataFrame \n#to_date(), date_add(date,days), date_sub(date,days),datediff(date1,date2),add_months(date,<num_of_months>),months_between(<date1>,<date2>),to_timestamp(),next_day(date,dayOfWeek), last_day(date),\ndate_val1=spark.createDataFrame([('2019-12-29',)],['dt'])\ndate_val2=spark.createDataFrame([('2019-11-28',)],['dt'])\n# date_add() function\nprint(date_val1.select(date_add('dt',1).alias('Next Date')).collect())\n# [Row(Next Date=datetime.date(2019, 12, 30))]\n\n# date_sub() function\nprint(date_val1.select(date_sub('dt',2).alias('Day before yesterday\\'s Date')).collect())\n# [Row(Day before yesterday's Date=datetime.date(2019, 12, 27))]\n\n# datediff() function\ndates=spark.createDataFrame([('2019-12-29','2019-11-14'),],['dt1','dt2'])\nprint(dates.select(datediff('dt1','dt2')).collect())\n# [Row(datediff(dt1, dt2)=45)]\n\n# add_months() function\nprint(date_val1.select(add_months('dt',2)).collect())\n# [Row(add_months(dt, 2)=datetime.date(2020, 2, 29))]\n\n# to_date() function\nprint(date_val1.select(to_date('dt')).collect())\n# [Row(to_date(`dt`)=datetime.date(2019, 12, 29))]\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n                  \n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[Row(year=2015)]\n[Row(quarter=4)]\n[Row(month=12)]\n[Row(dayofmonth=12)]\n[Row(dayofweek=7)]\n[Row(dayofyear=346)]\n[Row(hour=11)]\n[Row(minute=11)]\n[Row(second=11)]\n[Row(weekofyear=50)]\n[Row(Next Date=datetime.date(2019, 12, 30))]\n[Row(Day before yesterday&#39;s Date=datetime.date(2019, 12, 27))]\n[Row(datediff(dt1, dt2)=45)]\n[Row(add_months(dt, 2)=datetime.date(2020, 2, 29))]\n[Row(to_date(`dt`)=datetime.date(2019, 12, 29))]\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nUCBAdmissions_RDD=sc.textFile(\"/FileStore/tables/UCBAdmissions.csv\",4)   \\\n                    .map(lambda x:x.replace('\"',''))            \\\n                    .map(lambda x:x.split(','))                   \\\n                    .map(lambda x:x[1:])                        \\\n                    .filter(lambda x:(x[0]!='Admit'))\n#print(sc.textFile(\"/FileStore/tables/UCBAdmissions.csv\",4).map(lambda x:x.replace('\"','')).map(lambda x:x.split(',')).map(lambda x:x[1:]).filter(lambda x:(x[0]!='\"Admit\"')).take(4))\nprint(UCBAdmissions_RDD.getNumPartitions())\nfor i in UCBAdmissions_RDD.take(4):\n  print(i)\nprint(type(UCBAdmissions_RDD))\n#for i in UCBAdmissions_RDD.collect():\n#  print(i)\n\nAdmit=StructField('Admit',StringType(),True)\nGender=StructField('Gender',StringType(),True)\nDept=StructField('Dept',StringType(),True)\nFreq=StructField('Freq',StringType(),True)\nUCBAdmission_Cols_List=[Admit,Gender,Dept,Freq]\nUCBAdmission_Schema=StructType(UCBAdmission_Cols_List)\nUCBAdmission_DF=spark.createDataFrame(UCBAdmissions_RDD,schema=UCBAdmission_Schema)\nprint(UCBAdmission_DF.printSchema())\nprint(\"Printing Number of partitions after creating DF from the RDD\")\nprint(UCBAdmission_DF.rdd.getNumPartitions())\nprint(\"The number of partitions seem to to be the same\")\nUCBAdmissions_DF1=UCBAdmission_DF.rdd.map(lambda x:(int(x[3]),(x[0],x[1],x[2])))\nfor i in UCBAdmissions_DF1.take(2):\n  print(i)\nprint(\"Repartitioning data within RDD using partitionBy. PartitionBy can be used on when we have Data as key/value pairs within RDD\") \npartition_count=0\n#for i in UCBAdmissions_DF1.sortByKey().partitionBy(3).glom().collect():\n#  partition_count+=1\n#  print(\"\\npartition_number:{0}\".format(partition_count))\n#  for j in i:\n#    print(j)\n#partition_count=0\n#for i in UCBAdmission_DF.rdd.sortBy(lambda x:int(x[3])).repartition(6).glom().collect():\n#  partition_count+=1\n#  print(\"\\n Partition_count - {0}\".format(partition_count))\n#  for j in i:\n#    print(j)\n\nprint(UCBAdmissions_DF1.collect())\ndef f(x):\n  print(\"\\n\")\n  for i in x:\n    print(\"\\n\")\n    for j in i:\n      print(i)\nUCBAdmissions_DF1.foreachPartition(f)\n  \n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">4\n[&#39;Admitted&#39;, &#39;Male&#39;, &#39;A&#39;, &#39;512&#39;]\n[&#39;Rejected&#39;, &#39;Male&#39;, &#39;A&#39;, &#39;313&#39;]\n[&#39;Admitted&#39;, &#39;Female&#39;, &#39;A&#39;, &#39;89&#39;]\n[&#39;Rejected&#39;, &#39;Female&#39;, &#39;A&#39;, &#39;19&#39;]\n&lt;class &#39;pyspark.rdd.PipelinedRDD&#39;&gt;\nroot\n-- Admit: string (nullable = true)\n-- Gender: string (nullable = true)\n-- Dept: string (nullable = true)\n-- Freq: string (nullable = true)\n\nNone\nPrinting Number of partitions after creating DF from the RDD\n4\nThe number of partitions seem to to be the same\n(512, (&#39;Admitted&#39;, &#39;Male&#39;, &#39;A&#39;))\n(313, (&#39;Rejected&#39;, &#39;Male&#39;, &#39;A&#39;))\nRepartitioning data within RDD using partitionBy. PartitionBy can be used on when we have Data as key/value pairs within RDD\n[(512, (&#39;Admitted&#39;, &#39;Male&#39;, &#39;A&#39;)), (313, (&#39;Rejected&#39;, &#39;Male&#39;, &#39;A&#39;)), (89, (&#39;Admitted&#39;, &#39;Female&#39;, &#39;A&#39;)), (19, (&#39;Rejected&#39;, &#39;Female&#39;, &#39;A&#39;)), (353, (&#39;Admitted&#39;, &#39;Male&#39;, &#39;B&#39;)), (207, (&#39;Rejected&#39;, &#39;Male&#39;, &#39;B&#39;)), (17, (&#39;Admitted&#39;, &#39;Female&#39;, &#39;B&#39;)), (8, (&#39;Rejected&#39;, &#39;Female&#39;, &#39;B&#39;)), (120, (&#39;Admitted&#39;, &#39;Male&#39;, &#39;C&#39;)), (205, (&#39;Rejected&#39;, &#39;Male&#39;, &#39;C&#39;)), (202, (&#39;Admitted&#39;, &#39;Female&#39;, &#39;C&#39;)), (391, (&#39;Rejected&#39;, &#39;Female&#39;, &#39;C&#39;)), (138, (&#39;Admitted&#39;, &#39;Male&#39;, &#39;D&#39;)), (279, (&#39;Rejected&#39;, &#39;Male&#39;, &#39;D&#39;)), (131, (&#39;Admitted&#39;, &#39;Female&#39;, &#39;D&#39;)), (244, (&#39;Rejected&#39;, &#39;Female&#39;, &#39;D&#39;)), (53, (&#39;Admitted&#39;, &#39;Male&#39;, &#39;E&#39;)), (138, (&#39;Rejected&#39;, &#39;Male&#39;, &#39;E&#39;)), (94, (&#39;Admitted&#39;, &#39;Female&#39;, &#39;E&#39;)), (299, (&#39;Rejected&#39;, &#39;Female&#39;, &#39;E&#39;)), (22, (&#39;Admitted&#39;, &#39;Male&#39;, &#39;F&#39;)), (351, (&#39;Rejected&#39;, &#39;Male&#39;, &#39;F&#39;)), (24, (&#39;Admitted&#39;, &#39;Female&#39;, &#39;F&#39;)), (317, (&#39;Rejected&#39;, &#39;Female&#39;, &#39;F&#39;))]\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["subjects=spark.read.csv('FileStore/tables/subjects.csv',header=True,inferSchema=True)\nstudents=spark.read.csv('/FileStore/tables/students.csv',header=True,inferSchema=True)\nprint(students.take(2))\nprint(subjects.take(2))\nprint(\"\\n\")\nfor i in students.join(subjects,students[\"studentid\"]==subjects[\"studentid\"],how='inner').take(4):\n  print(i)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[Row(studentid=&#39;      si3&#39;, gender=&#39;     F&#39;, name=&#39;  Julie&#39;), Row(studentid=&#39;      si2&#39;, gender=&#39;     F&#39;, name=&#39;  Maria&#39;)]\n[Row(id=6, marks=78.0, studentid=&#39;      si4&#39;, subjects=&#39;     C++&#39;), Row(id=9, marks=83.0, studentid=&#39;      si2&#39;, subjects=&#39;    Java&#39;)]\n\n\nRow(studentid=&#39;      si4&#39;, gender=&#39;     M&#39;, name=&#39;    Bob&#39;, id=6, marks=78.0, studentid=&#39;      si4&#39;, subjects=&#39;     C++&#39;)\nRow(studentid=&#39;      si2&#39;, gender=&#39;     F&#39;, name=&#39;  Maria&#39;, id=9, marks=83.0, studentid=&#39;      si2&#39;, subjects=&#39;    Java&#39;)\nRow(studentid=&#39;      si3&#39;, gender=&#39;     F&#39;, name=&#39;  Julie&#39;, id=5, marks=72.0, studentid=&#39;      si3&#39;, subjects=&#39;    Ruby&#39;)\nRow(studentid=&#39;      si2&#39;, gender=&#39;     F&#39;, name=&#39;  Maria&#39;, id=4, marks=85.0, studentid=&#39;      si2&#39;, subjects=&#39;  Python&#39;)\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["def f(x):\n    total=0\n    for rec in x:\n        total+=rec[0][1]\n    yield total\nkvrdd1 = sc.parallelize([((1,99),'A'),((1,101),'B'),((2,99),'C'),((2,101),'D')],2)\n\nprint(kvrdd1.lookup((1,99)))\nkvrdd1.mapPartitions(f).collect()\n# returns:\n# ((1, 99), 'A')\n# ((1, 101), 'B')\n# ((2, 99), 'C')\n# ((2, 101), 'D')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;A&#39;]\nOut[158]: [200, 200]</div>"]}}],"execution_count":7},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark=SparkSession.builder.appName('abc').getOrCreate()\nspark.sql(\"SET -v\").show(n=200, truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nkey                                                              |value                                                                                                                                                                                      |meaning                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n+-----------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nspark.databricks.credentials.assumed.role                        |&lt;undefined&gt;                                                                                                                                                                                |The role that will be assumed when trying to access the filesystem.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\nspark.databricks.delta.alterLocation.bypassSchemaCheck           |false                                                                                                                                                                                      |If true, Alter Table Set Location on Delta will go through even if the Delta table in the new location has a different schema from the original Delta table.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\nspark.databricks.delta.alterTable.rename.enabledOnAWS            |false                                                                                                                                                                                      |When true, allow ALTER TABLE ... RENAME TO ... on managed Delta tables on AWS.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\nspark.databricks.delta.autoCompact.enabled                       |&lt;undefined&gt;                                                                                                                                                                                |Whether to compact files after writes made into Delta tables from this session.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\nspark.databricks.delta.commitInfo.enabled                        |true                                                                                                                                                                                       |Whether to log commit information into the Delta log.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\nspark.databricks.delta.convert.metadataCheck.enabled             |true                                                                                                                                                                                       |\nIf enabled, during convert to delta, if there is a difference between the catalog table&#39;s\nproperties and the Delta table&#39;s configuration, we should error. If disabled, merge\nthe two configurations with the same semantics as update and merge.\n\nspark.databricks.delta.formatCheck.enabled                       |true                                                                                                                                                                                       |If true, checks the specified format and reject reads from / writes to Delta tables using the regular format.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\nspark.databricks.delta.optimize.incremental                      |true                                                                                                                                                                                       |Controls whether the OPTIMIZE command shall operate on &#34;unoptimized&#34; data only or just blindly rewrite all data files (matching the given predicate, if any). When enabled, the command&#39;s runtime should generally be proportional to the amount of data added since last time the command was run, but this is not a strong guarantee. Also note that this option takes effect no matter if the ZORDER BY clause is specified or not.                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\nspark.databricks.delta.optimizeWrite.enabled                     |&lt;undefined&gt;                                                                                                                                                                                |Whether to optimize writes made into Delta tables from this session.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\nspark.databricks.delta.retentionDurationCheck.enabled            |true                                                                                                                                                                                       |Adds a check preventing users from running vacuum with a very short retention period, which may end up corrupting the Delta Log.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\nspark.databricks.delta.schema.autoMerge.enabled                  |false                                                                                                                                                                                      |If true, enables schema merging on appends and on overwrites.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\nspark.databricks.delta.stalenessLimit                            |0ms                                                                                                                                                                                        |Setting a non-zero time limit will allow you to query the last loaded state of the Delta\ntable without blocking on a table update. You can use this configuration to reduce the\nlatency on queries when up-to-date results are not a requirement. Table updates will be\nscheduled on a separate scheduler pool in a FIFO queue, and will share cluster resources\nfairly with your query. If a table hasn&#39;t updated past this time limit, we will block\non a synchronous state update before running the query.\n\nspark.databricks.hive.metastore.client.pool.enabled              |true                                                                                                                                                                                       |When set to true, a pool of hive clients is used                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\nspark.databricks.hive.metastore.client.pool.log.fullStackTrace   |true                                                                                                                                                                                       |Whether or not to log the full stack trace when the object is borrowed                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\nspark.databricks.hive.metastore.client.pool.size                 |5                                                                                                                                                                                          |The size of the hive client pool. Only valid when the client pool is enabled                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\nspark.databricks.hive.metastore.client.pool.waitTime             |-1                                                                                                                                                                                         |The max time (ms) a thread waits for a hive client in the pool to become idle                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\nspark.databricks.hive.metastore.connection.maxIdleMillis         |60000                                                                                                                                                                                      |The max time a connection is idle before being closed. Keep this number small to reduce the physical connections, but an aggressive value may increase the connection latency.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\nspark.databricks.io.cache.parquet.enabled                        |true                                                                                                                                                                                       |Enables automatic caching of Parquet files, if spark.databricks.io.cache.enabled is set to true.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\nspark.databricks.io.cache.parquet.numWriters                     |1                                                                                                                                                                                          |The number of asynchronous workers writing Parquet page cache.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\nspark.databricks.io.cache.prefix                                 |dbfs,s3,hdfs,wasb,adl,abfs                                                                                                                                                                 |Only paths with the specified scheme prefixes (separated by comma) are cached. If a path doesn&#39;t have a scheme, use the default file system URI to decide.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n\n*** WARNING: skipped 92296 bytes of output ***\n\nspark.sql.sources.bucketing.maxBuckets                           |100000                                                                                                                                                                                     |The maximum number of buckets allowed. Defaults to 100000                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\nspark.sql.sources.default                                        |parquet                                                                                                                                                                                    |The default data source to use in input/output.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\nspark.sql.sources.parallelPartitionDiscovery.threshold           |32                                                                                                                                                                                         |The maximum number of paths allowed for listing files at driver side. If the number of detected paths exceeds this value during partition discovery, it tries to list the files with another Spark distributed job. This applies to Parquet, ORC, CSV, JSON and LibSVM data sources.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\nspark.sql.sources.partitionColumnTypeInference.enabled           |true                                                                                                                                                                                       |When true, automatically infer the data types for partitioned columns.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\nspark.sql.sources.partitionOverwriteMode                         |STATIC                                                                                                                                                                                     |When INSERT OVERWRITE a partitioned data source table, we currently support 2 modes: static and dynamic. In static mode, Spark deletes all the partitions that match the partition specification(e.g. PARTITION(a=1,b)) in the INSERT statement, before overwriting. In dynamic mode, Spark doesn&#39;t delete partitions ahead, and only overwrite those partitions that have data written into it at runtime. By default we use static mode to keep the same behavior of Spark prior to 2.3. Note that this config doesn&#39;t affect Hive serde tables, as they are always overwritten with dynamic mode. This can also be set as an output option for a data source using key partitionOverwriteMode (which takes precedence over this setting), e.g. dataframe.write.option(&#34;partitionOverwriteMode&#34;, &#34;dynamic&#34;).save(path).                                                              |\nspark.sql.statistics.fallBackToHdfs                              |false                                                                                                                                                                                      |If the table statistics are not available from table metadata enable fall back to hdfs. This is useful in determining if a table is small enough to use auto broadcast joins.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\nspark.sql.statistics.histogram.enabled                           |false                                                                                                                                                                                      |Generates histograms when computing column statistics if enabled. Histograms can provide better estimation accuracy. Currently, Spark only supports equi-height histogram. Note that collecting histograms takes extra cost. For example, collecting column statistics usually takes only one table scan, but generating equi-height histogram will cause an extra table scan.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\nspark.sql.statistics.size.autoUpdate.enabled                     |false                                                                                                                                                                                      |Enables automatic update for table size once table&#39;s data is changed. Note that if the total number of files of the table is very large, this can be expensive and slow down data change commands.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\nspark.sql.storeAssignmentPolicy                                  |&lt;undefined&gt;                                                                                                                                                                                |When inserting a value into a column with different data type, Spark will perform type coercion. Currently, we support 3 policies for the type coercion rules: ANSI, legacy and strict. With ANSI policy, Spark performs the type coercion as per ANSI SQL. In practice, the behavior is mostly the same as PostgreSQL. It disallows certain unreasonable type conversions such as converting `string` to `int` or `double` to `boolean`. With legacy policy, Spark allows the type coercion as long as it is a valid `Cast`, which is very loose. e.g. converting `string` to `int` or `double` to `boolean` is allowed. It is also the only behavior in Spark 2.x and it is compatible with Hive. With strict policy, Spark doesn&#39;t allow any possible precision loss or data truncation in type coercion, e.g. converting `double` to `int` or `decimal` to `double` is not allowed.|\nspark.sql.streaming.checkpointLocation                           |&lt;undefined&gt;                                                                                                                                                                                |The default location for storing checkpoint data for streaming queries.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\nspark.sql.streaming.metricsEnabled                               |false                                                                                                                                                                                      |Whether Dropwizard/Codahale metrics will be reported for active streaming queries.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\nspark.sql.streaming.multipleWatermarkPolicy                      |min                                                                                                                                                                                        |Policy to calculate the global watermark value when there are multiple watermark operators in a streaming query. The default value is &#39;min&#39; which chooses the minimum watermark reported across multiple operators. Other alternative value is&#39;max&#39; which chooses the maximum across multiple operators.Note: This configuration cannot be changed between query restarts from the same checkpoint location.                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\nspark.sql.streaming.noDataMicroBatches.enabled                   |true                                                                                                                                                                                       |Whether streaming micro-batch engine will execute batches without data for eager state management for stateful streaming queries.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\nspark.sql.streaming.numRecentProgressUpdates                     |100                                                                                                                                                                                        |The number of progress updates to retain for a streaming query                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\nspark.sql.streaming.streamingQueryListeners                      |&lt;undefined&gt;                                                                                                                                                                                |List of class names implementing StreamingQueryListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\nspark.sql.thriftserver.scheduler.pool                            |&lt;undefined&gt;                                                                                                                                                                                |Set a Fair Scheduler pool for a JDBC client session.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\nspark.sql.thriftserver.ui.retainedSessions                       |200                                                                                                                                                                                        |The number of SQL client sessions kept in the JDBC/ODBC web UI history.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\nspark.sql.thriftserver.ui.retainedStatements                     |200                                                                                                                                                                                        |The number of SQL statements kept in the JDBC/ODBC web UI history.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\nspark.sql.ui.retainedExecutions                                  |100                                                                                                                                                                                        |Number of executions to retain in the Spark UI.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\nspark.sql.variable.substitute                                    |true                                                                                                                                                                                       |This enables substitution using syntax like ${var} ${system:var} and ${env:var}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\nspark.sql.warehouse.dir                                          |/user/hive/warehouse                                                                                                                                                                       |The default location for managed databases and tables.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n+-----------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["def f(x):\n  for rec in x:\n    print(rec)\ntemp=sc.parallelize([((1,99),'A'),((1,101),'B'),((2,99),'C'),((2,101),'D')],2)\ntemp.foreachPartition(f)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["loremRDD=sc.textFile(\"/FileStore/tables/lorem.txt\")\nprint(loremRDD.collect())\nprint(\"\\n\")\nstorageLorem=loremRDD.getStorageLevel()\nprint(storageLorem)\nprint(storageLorem.useDisk)\nprint(storageLorem.useMemory)\nprint(storageLorem.useOffHeap)\nprint(storageLorem.deserialized)\nprint(storageLorem.replication)\n\n# print(loremRDD.getStorageLevel())\n\nfrom datetime import *\nUCBAdmissionsRDD=sc.textFile(\"/FileStore/tables/UCBAdmissions.csv\")\n\nprint(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\nUCBAdmissionsRDD.count()\nprint(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n#UCBAdmissionsRDD.cache()\nprint(UCBAdmissionsRDD.getStorageLevel())\nprint(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\nUCBAdmissionsRDD.count()\nUCBAdmissionsRDD.persist(StorageLevel(True, True, False, False, 2))\nprint(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2161052544768621&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     22</span> print<span class=\"ansi-blue-fg\">(</span>datetime<span class=\"ansi-blue-fg\">.</span>now<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>strftime<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;%Y-%m-%d %H:%M:%S&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     23</span> UCBAdmissionsRDD<span class=\"ansi-blue-fg\">.</span>count<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">---&gt; 24</span><span class=\"ansi-red-fg\"> </span>UCBAdmissionsRDD<span class=\"ansi-blue-fg\">.</span>persist<span class=\"ansi-blue-fg\">(</span>StorageLevel<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     25</span> print<span class=\"ansi-blue-fg\">(</span>datetime<span class=\"ansi-blue-fg\">.</span>now<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>strftime<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;%Y-%m-%d %H:%M:%S&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;StorageLevel&#39; is not defined</div>"]}}],"execution_count":10},{"cell_type":"code","source":["doc = sc.textFile(\"/FileStore/tables/shakespearePlays.txt\",4)\nwords = doc.flatMap(lambda x: x.split()) \\\n    .map(lambda x: (x,1)) \\\n    .reduceByKey(lambda x, y: x + y)\nwords.persist()\nwords.count()\n# returns: 33505\nwords.take(3)\n# returns: [('Quince', 8), ('Begin', 9), ('Just', 12)]\nprint(words.toDebugString().decode(\"utf-8\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(5) PythonRDD[106] at RDD at PythonRDD.scala:59 [Memory Serialized 1x Replicated]\n       CachedPartitions: 5; MemorySize: 562.0 B; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n  MapPartitionsRDD[105] at mapPartitions at PythonRDD.scala:172 [Memory Serialized 1x Replicated]\n  ShuffledRDD[104] at partitionBy at NativeMethodAccessorImpl.java:0 [Memory Serialized 1x Replicated]\n +-(5) PairwiseRDD[103] at reduceByKey at &lt;command-1500255913664174&gt;:4 [Memory Serialized 1x Replicated]\n  PythonRDD[102] at reduceByKey at &lt;command-1500255913664174&gt;:4 [Memory Serialized 1x Replicated]\n  /FileStore/tables/shakespearePlays.txt MapPartitionsRDD[101] at textFile at &lt;unknown&gt;:0 [Memory Serialized 1x Replicated]\n  /FileStore/tables/shakespearePlays.txt HadoopRDD[100] at textFile at &lt;unknown&gt;:0 [Memory Serialized 1x Replicated]\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["UCBAdmissionsRDD=sc.textFile(\"/FileStore/tables/UCBAdmissions.csv\",4)\nsc.setCheckpointDir(\"/FileStore/tables/Checkpoint\")\nUCBAdmissionsRDD.checkpoint()\nprint(UCBAdmissionsRDD.collect())\n\nprint(UCBAdmissionsRDD.getCheckpointFile())\nprint(UCBAdmissionsRDD.isCheckpointed())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;&#34;&#34;,&#34;Admit&#34;,&#34;Gender&#34;,&#34;Dept&#34;,&#34;Freq&#34;&#39;, &#39;&#34;1&#34;,&#34;Admitted&#34;,&#34;Male&#34;,&#34;A&#34;,512&#39;, &#39;&#34;2&#34;,&#34;Rejected&#34;,&#34;Male&#34;,&#34;A&#34;,313&#39;, &#39;&#34;3&#34;,&#34;Admitted&#34;,&#34;Female&#34;,&#34;A&#34;,89&#39;, &#39;&#34;4&#34;,&#34;Rejected&#34;,&#34;Female&#34;,&#34;A&#34;,19&#39;, &#39;&#34;5&#34;,&#34;Admitted&#34;,&#34;Male&#34;,&#34;B&#34;,353&#39;, &#39;&#34;6&#34;,&#34;Rejected&#34;,&#34;Male&#34;,&#34;B&#34;,207&#39;, &#39;&#34;7&#34;,&#34;Admitted&#34;,&#34;Female&#34;,&#34;B&#34;,17&#39;, &#39;&#34;8&#34;,&#34;Rejected&#34;,&#34;Female&#34;,&#34;B&#34;,8&#39;, &#39;&#34;9&#34;,&#34;Admitted&#34;,&#34;Male&#34;,&#34;C&#34;,120&#39;, &#39;&#34;10&#34;,&#34;Rejected&#34;,&#34;Male&#34;,&#34;C&#34;,205&#39;, &#39;&#34;11&#34;,&#34;Admitted&#34;,&#34;Female&#34;,&#34;C&#34;,202&#39;, &#39;&#34;12&#34;,&#34;Rejected&#34;,&#34;Female&#34;,&#34;C&#34;,391&#39;, &#39;&#34;13&#34;,&#34;Admitted&#34;,&#34;Male&#34;,&#34;D&#34;,138&#39;, &#39;&#34;14&#34;,&#34;Rejected&#34;,&#34;Male&#34;,&#34;D&#34;,279&#39;, &#39;&#34;15&#34;,&#34;Admitted&#34;,&#34;Female&#34;,&#34;D&#34;,131&#39;, &#39;&#34;16&#34;,&#34;Rejected&#34;,&#34;Female&#34;,&#34;D&#34;,244&#39;, &#39;&#34;17&#34;,&#34;Admitted&#34;,&#34;Male&#34;,&#34;E&#34;,53&#39;, &#39;&#34;18&#34;,&#34;Rejected&#34;,&#34;Male&#34;,&#34;E&#34;,138&#39;, &#39;&#34;19&#34;,&#34;Admitted&#34;,&#34;Female&#34;,&#34;E&#34;,94&#39;, &#39;&#34;20&#34;,&#34;Rejected&#34;,&#34;Female&#34;,&#34;E&#34;,299&#39;, &#39;&#34;21&#34;,&#34;Admitted&#34;,&#34;Male&#34;,&#34;F&#34;,22&#39;, &#39;&#34;22&#34;,&#34;Rejected&#34;,&#34;Male&#34;,&#34;F&#34;,351&#39;, &#39;&#34;23&#34;,&#34;Admitted&#34;,&#34;Female&#34;,&#34;F&#34;,24&#39;, &#39;&#34;24&#34;,&#34;Rejected&#34;,&#34;Female&#34;,&#34;F&#34;,317&#39;]\ndbfs:/FileStore/tables/Checkpoint/2b33e763-d66a-4a6b-be63-bc7e5eb3ac9c/rdd-119\nTrue\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["#from pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[4]\") \\\n\t\t\t\t\t.appName(\"HydrateStream\").config(\"spark.driver.allowMultipleContexts\", \"true\")   \\\n\t\t\t\t\t.config(\"fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")   \\\n                    .config(\"fs.azure.account.key.enterprisedatalakeprod.blob.core.windows.net\",   \\\n                    \"GacHjIy+ODWDKe4YEJtm20PFCpqGre8HQWCyw2G/XhnVexNnVFdtCCOK/N281OEZmczERmCu87/mrAJkybupvw==\")   \\\n\t\t\t\t\t.getOrCreate();"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["from pyspark.sql import SparkSession \nfrom pyspark.context import SparkContext \nfor i in spark.sparkContext.getConf().getAll():\n  print(i)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(&#39;spark.databricks.preemption.enabled&#39;, &#39;true&#39;)\n(&#39;spark.hadoop.fs.abfs.impl&#39;, &#39;shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem&#39;)\n(&#39;spark.driver.tempDirectory&#39;, &#39;/local_disk0/tmp&#39;)\n(&#39;spark.hadoop.fs.adl.impl.disable.cache&#39;, &#39;true&#39;)\n(&#39;spark.hadoop.parquet.block.size.row.check.max&#39;, &#39;10&#39;)\n(&#39;spark.hadoop.fs.s3a.connection.maximum&#39;, &#39;200&#39;)\n(&#39;spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2&#39;, &#39;0&#39;)\n(&#39;spark.shuffle.reduceLocality.enabled&#39;, &#39;false&#39;)\n(&#39;spark.sql.streaming.checkpointFileManagerClass&#39;, &#39;com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager&#39;)\n(&#39;spark.executor.extraClassPath&#39;, &#39;/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/api-base--api-base_java-spark_2.4_2.11_deploy.jar:/databricks/jars/api-base--api-base-spark_2.4_2.11_deploy.jar:/databricks/jars/api--rpc--rpc_parser-spark_2.4_2.11_deploy.jar:/databricks/jars/chauffeur-api--api--endpoints--endpoints-spark_2.4_2.11_deploy.jar:/databricks/jars/chauffeur-api--chauffeur-api-spark_2.4_2.11_deploy.jar:/databricks/jars/common--client--client-spark_2.4_2.11_deploy.jar:/databricks/jars/common--common-spark_2.4_2.11_deploy.jar:/databricks/jars/common--credentials--credentials-spark_2.4_2.11_deploy.jar:/databricks/jars/common--hadoop--hadoop-spark_2.4_2.11_deploy.jar:/databricks/jars/common--jetty--client--client-spark_2.4_2.11_deploy.jar:/databricks/jars/common--lazy--lazy-spark_2.4_2.11_deploy.jar:/databricks/jars/common--libcommon_resources.jar:/databricks/jars/common--path--path-spark_2.4_2.11_deploy.jar:/databricks/jars/common--rate-limiter--rate-limiter-spark_2.4_2.11_deploy.jar:/databricks/jars/common--storage--storage-spark_2.4_2.11_deploy.jar:/databricks/jars/daemon--data--client--client-spark_2.4_2.11_deploy.jar:/databricks/jars/daemon--data--client--conf--conf-spark_2.4_2.11_deploy.jar:/databricks/jars/daemon--data--client--utils-spark_2.4_2.11_deploy.jar:/databricks/jars/daemon--data--data-common--data-common-spark_2.4_2.11_deploy.jar:/databricks/jars/dbfs--utils--dbfs-utils-spark_2.4_2.11_deploy.jar:/databricks/jars/extern--acl--auth--auth-spark_2.4_2.11_deploy.jar:/databricks/jars/extern--extern-spark_2.4_2.11_deploy.jar:/databricks/jars/extern--libaws-regions.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-client-common_deploy.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-shim-common_deploy.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-shim-hive1_deploy.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-shim-hive2_deploy.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-shim-loader_deploy.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-spark-client_deploy.jar:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/jsonutil--jsonutil-spark_2.4_2.11_deploy.jar:/databricks/jars/libraries--api--managedLibraries--managedLibraries-spark_2.4_2.11_deploy.jar:/databricks/jars/libraries--api--typemappers--typemappers-spark_2.4_2.11_deploy.jar:/databricks/jars/libraries--libraries-spark_2.4_2.11_deploy.jar:/databricks/jars/logging--log4j-mod--log4j-mod-spark_2.4_2.11_deploy.jar:/databricks/jars/logging--utils--logging-utils-spark_2.4_2.11_deploy.jar:/databricks/jars/s3commit--client--client-spark_2.4_2.11_deploy.jar:/databricks/jars/s3commit--common--common-spark_2.4_2.11_deploy.jar:/databricks/jars/s3--s3-spark_2.4_2.11_deploy.jar:/databricks/jars/----scalapb_090--com.fasterxml.jackson.core__jackson-core__2.9.9_shaded.jar:/databricks/jars/----scalapb_090--com.google.android__annotations__4.1.1.4_shaded.jar:/databricks/jars/----scalapb_090--com.google.api.grpc__proto-google-common-protos__1.12.0_shaded.jar:/databricks/jars/----scalapb_090--com.google.code.gson__gson__2.7_shaded.jar:/databricks/jars/----scalapb_090--com.google.errorprone__error_prone_annotations__2.3.2_shaded.jar:/databricks/jars/----scalapb_090--com.google.guava__guava__20.0_shaded.jar:/databricks/jars/----scalapb_090--com.google.protobuf__protobuf-java__3.7.1_shaded.jar:/databricks/jars/----scalapb_090--com.google.protobuf__protobuf-java-util__3.7.1_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__fastparse_2.11__2.1.2_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__sourcecode_2.11__0.1.6_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-api__1.22.1_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-context__1.22.1_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-core__1.22.1_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-netty__1.22.1_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-protobuf__1.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-protobuf-lite__1.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-stub__1.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.opencensus__opencensus-api__0.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.opencensus__opencensus-contrib-grpc-metrics__0.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.perfmark__perfmark-api__0.16.0_shaded.jar:/databricks/jars/----scalapb_090--org.codehaus.mojo__animal-sniffer-annotations__1.17_shaded.jar:/databricks/jars/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.11_deploy_shaded.jar:/databricks/jars/secret-manager--api--api-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--common--spark-common-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--conf-reader--conf-reader_lib-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--dbutils--dbutils-api-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--antlr--parser-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--common--driver-common-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--display--display-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--driver-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--events-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--ml--ml-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--secret-redaction-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--spark--resources-resources.jar:/databricks/jars/spark--maven-trees--hive-exec-with-glue--hive-12679-patch_deploy.jar:/databricks/jars/spark--maven-trees--hive-exec-with-glue--hive-exec_shaded.jar:/databricks/jars/spark--maven-trees--spark_2.4--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.8.10.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-glue--com.amazonaws__aws-java-sdk-glue__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.carrotsearch--hppc--com.carrotsearch__hppc__0.7.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.chuusai--shapeless_2.11--com.chuusai__shapeless_2.11__2.3.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.7.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.databricks--dbml-local_2.11--com.databricks__dbml-local_2.11__0.5.0-db8-spark2.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.databricks--dbml-local_2.11-tests--com.databricks__dbml-local_2.11-tests__0.5.0-db8-spark2.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.databricks--Rserve--com.databricks__Rserve__1.8-3.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.databricks.scalapb--compilerplugin_2.11--com.databricks.scalapb__compilerplugin_2.11__0.4.15-9.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.databricks.scalapb--scalapb-runtime_2.11--com.databricks.scalapb__scalapb-runtime_2.11__0.4.15-9.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.esotericsoftware--kryo-shaded--com.esotericsoftware__kryo-shaded__4.0.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.esotericsoftware--minlog--com.esotericsoftware__minlog__1.3.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml--classmate--com.fasterxml__classmate__1.0.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.6.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.6.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.6.7.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.6.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.datatype--jackson-datatype-joda--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.6.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.module--jackson-module-paranamer--com.fasterxml.jackson.module__jackson-module-paranamer__2.6.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.module--jackson-module-scala_2.11--com.fasterxml.jackson.module__jackson-module-scala_2.11__2.6.7.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil--jniloader--com.github.fommil__jniloader__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--core--com.github.fommil.netlib__core__1.1.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--native_ref-java--com.github.fommil.netlib__native_ref-java__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--native_ref-java-natives--com.github.fommil.netlib__native_ref-java-natives__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--native_system-java--com.github.fommil.netlib__native_system-java__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--native_system-java-natives--com.github.fommil.netlib__native_system-java-natives__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--netlib-native_ref-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_ref-linux-x86_64-natives__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--netlib-native_system-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_system-linux-x86_64-natives__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.luben--zstd-jni--com.github.luben__zstd-jni__1.3.2-2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.rwl--jtransforms--com.github.rwl__jtransforms__2.4.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__2.0.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.googlecode.javaewah--JavaEWAH--com.googlecode.javaewah__JavaEWAH__0.3.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.google.guava--guava--com.google.guava__guava__15.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.h2database--h2--com.h2database__h2__1.3.174.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.jcraft--jsch--com.jcraft__jsch__0.1.50.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.2.8.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.microsoft.sqlserver--mssql-jdbc--com.microsoft.sqlserver__mssql-jdbc__6.2.2.jre8.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-dbcp--commons-dbcp--commons-dbcp__commons-dbcp__1.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-io--commons-io--commons-io__commons-io__2.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-net--commons-net--commons-net__commons-net__3.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-pool--commons-pool--commons-pool__commons-pool__1.5.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.ning--compress-lzf--com.ning__compress-lzf__1.0.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.sun.mail--javax.mail--com.sun.mail__javax.mail__1.5.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.trueaccord.lenses--lenses_2.11--com.trueaccord.lenses__lenses_2.11__0.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.twitter--chill_2.11--com.twitter__chill_2.11__0.9.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.twitter--chill-java--com.twitter__chill-java__0.9.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.twitter--parquet-hadoop-bundle--com.twitter__parquet-hadoop-bundle__1.6.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.twitter--util-app_2.11--com.twitter__util-app_2.11__6.23.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.twitter--util-core_2.11--com.twitter__util-core_2.11__6.23.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.twitter--util-jvm_2.11--com.twitter__util-jvm_2.11__6.23.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.typesafe--config--com.typesafe__config__1.2.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-api_2.11--com.typesafe.scala-logging__scala-logging-api_2.11__2.1.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-slf4j_2.11--com.typesafe.scala-logging__scala-logging-slf4j_2.11__2.1.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.univocity--univocity-parsers--com.univocity__univocity-parsers__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.vlkan--flatbuffers--com.vlkan__flatbuffers__1.2.0-3f79e055.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.zaxxer--HikariCP--com.zaxxer__HikariCP__3.1.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.airlift--aircompressor--io.airlift__aircompressor__0.10.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-ganglia--io.dropwizard.metrics__metrics-ganglia__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-graphite--io.dropwizard.metrics__metrics-graphite__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-log4j--io.dropwizard.metrics__metrics-log4j__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.netty--netty-all--io.netty__netty-all__4.1.42.Final.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.netty--netty--io.netty__netty__3.9.9.Final.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.activation--activation--javax.activation__activation__1.1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.annotation--javax.annotation-api--javax.annotation__javax.annotation-api__1.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.transaction--jta--javax.transaction__jta__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.validation--validation-api--javax.validation__validation-api__1.1.0.Final.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.ws.rs--javax.ws.rs-api--javax.ws.rs__javax.ws.rs-api__2.0.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.xml.bind--jaxb-api--javax.xml.bind__jaxb-api__2.2.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.xml.stream--stax-api--javax.xml.stream__stax-api__1.0-2.jar:/databricks/jars/spark--maven-trees--spark_2.4--javolution--javolution--javolution__javolution__5.5.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--jline--jline--jline__jline__2.14.6.jar:/databricks/jars/spark--maven-trees--spark_2.4--joda-time--joda-time--joda-time__joda-time__2.9.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--junit--junit--junit__junit__4.12.jar:/databricks/jars/spark--maven-trees--spark_2.4--liball_deps_2.11.jar:/databricks/jars/spark--maven-trees--spark_2.4--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:/databricks/jars/spark--maven-trees--spark_2.4--log4j--log4j--log4j__log4j__1.2.17.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.hydromatic--eigenbase-properties--net.hydromatic__eigenbase-properties__1.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.razorvine--pyrolite--net.razorvine__pyrolite__4.13.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.sf.opencsv--opencsv--net.sf.opencsv__opencsv__2.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.sf.supercsv--super-csv--net.sf.supercsv__super-csv__2.2.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.snowflake--snowflake-ingest-sdk--net.snowflake__snowflake-ingest-sdk__0.9.6.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.snowflake--snowflake-jdbc--net.snowflake__snowflake-jdbc__3.9.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.snowflake--spark-snowflake_2.11--net.snowflake__spark-snowflake_2.11__2.5.3-spark_2.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.sourceforge.f2j--arpack_combined_all--net.sourceforge.f2j__arpack_combined_all__0.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.acplt--oncrpc--org.acplt__oncrpc__1.0.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.antlr--antlr4-runtime--org.antlr__antlr4-runtime__4.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.ant--ant-jsch--org.apache.ant__ant-jsch__1.9.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.arrow--arrow-format--org.apache.arrow__arrow-format__0.10.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.arrow--arrow-memory--org.apache.arrow__arrow-memory__0.10.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.arrow--arrow-vector--org.apache.arrow__arrow-vector__0.10.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.avro--avro-ipc--org.apache.avro__avro-ipc__1.8.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.avro--avro-mapred-hadoop2--org.apache.avro__avro-mapred-hadoop2__1.8.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.calcite--calcite-avatica--org.apache.calcite__calcite-avatica__1.2.0-incubating.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.calcite--calcite-core--org.apache.calcite__calcite-core__1.2.0-incubating.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.calcite--calcite-linq4j--org.apache.calcite__calcite-linq4j__1.2.0-incubating.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.8.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.commons--commons-crypto--org.apache.commons__commons-crypto__1.0.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.derby--derby--org.apache.derby__derby__10.12.1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-client--org.apache.hadoop__hadoop-client__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-hdfs--org.apache.hadoop__hadoop-hdfs__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-app--org.apache.hadoop__hadoop-mapreduce-client-app__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-common--org.apache.hadoop__hadoop-mapreduce-client-common__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-core--org.apache.hadoop__hadoop-mapreduce-client-core__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-jobclient--org.apache.hadoop__hadoop-mapreduce-client-jobclient__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-shuffle--org.apache.hadoop__hadoop-mapreduce-client-shuffle__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-yarn-api--org.apache.hadoop__hadoop-yarn-api__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-yarn-client--org.apache.hadoop__hadoop-yarn-client__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-yarn-common--org.apache.hadoop__hadoop-yarn-common__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-yarn-server-common--org.apache.hadoop__hadoop-yarn-server-common__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.6.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.10.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.ivy--ivy--org.apache.ivy__ivy__2.4.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.orc--orc-core-nohive--org.apache.orc__orc-core-nohive__1.5.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.orc--orc-mapreduce-nohive--org.apache.orc__orc-mapreduce-nohive__1.5.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.orc--orc-shims--org.apache.orc__orc-shims__1.5.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-column--org.apache.parquet__parquet-column__1.10.1.2-databricks4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-common--org.apache.parquet__parquet-common__1.10.1.2-databricks4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-encoding--org.apache.parquet__parquet-encoding__1.10.1.2-databricks4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-format--org.apache.parquet__parquet-format__2.4.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-hadoop--org.apache.parquet__parquet-hadoop__1.10.1.2-databricks4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-jackson--org.apache.parquet__parquet-jackson__1.10.1.2-databricks4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.9.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.xbean--xbean-asm6-shaded--org.apache.xbean__xbean-asm6-shaded__4.8.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-jaxrs--org.codehaus.jackson__jackson-jaxrs__1.9.13.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-xc--org.codehaus.jackson__jackson-xc__1.9.13.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.janino--commons-compiler--org.codehaus.janino__commons-compiler__3.0.10.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.janino--janino--org.codehaus.janino__janino__3.0.10.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__3.2.6.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__3.2.10.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__3.2.9.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-client--org.eclipse.jetty__jetty-client__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-continuation--org.eclipse.jetty__jetty-continuation__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-http--org.eclipse.jetty__jetty-http__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-io--org.eclipse.jetty__jetty-io__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-jndi--org.eclipse.jetty__jetty-jndi__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-plus--org.eclipse.jetty__jetty-plus__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-proxy--org.eclipse.jetty__jetty-proxy__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-security--org.eclipse.jetty__jetty-security__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-server--org.eclipse.jetty__jetty-server__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlet--org.eclipse.jetty__jetty-servlet__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlets--org.eclipse.jetty__jetty-servlets__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-util--org.eclipse.jetty__jetty-util__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-webapp--org.eclipse.jetty__jetty-webapp__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-xml--org.eclipse.jetty__jetty-xml__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.fusesource.leveldbjni--leveldbjni-all--org.fusesource.leveldbjni__leveldbjni-all__1.8.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2.external--aopalliance-repackaged--org.glassfish.hk2.external__aopalliance-repackaged__2.4.0-b34.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2.external--javax.inject--org.glassfish.hk2.external__javax.inject__2.4.0-b34.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2--hk2-api--org.glassfish.hk2__hk2-api__2.4.0-b34.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2--hk2-locator--org.glassfish.hk2__hk2-locator__2.4.0-b34.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2--hk2-utils--org.glassfish.hk2__hk2-utils__2.4.0-b34.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2--osgi-resource-locator--org.glassfish.hk2__osgi-resource-locator__1.0.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.bundles.repackaged--jersey-guava--org.glassfish.jersey.bundles.repackaged__jersey-guava__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.containers--jersey-container-servlet-core--org.glassfish.jersey.containers__jersey-container-servlet-core__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.containers--jersey-container-servlet--org.glassfish.jersey.containers__jersey-container-servlet__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.core--jersey-client--org.glassfish.jersey.core__jersey-client__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.core--jersey-common--org.glassfish.jersey.core__jersey-common__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.core--jersey-server--org.glassfish.jersey.core__jersey-server__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.media--jersey-media-jaxb--org.glassfish.jersey.media__jersey-media-jaxb__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.hamcrest--hamcrest-core--org.hamcrest__hamcrest-core__1.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.hamcrest--hamcrest-library--org.hamcrest__hamcrest-library__1.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.hibernate--hibernate-validator--org.hibernate__hibernate-validator__5.1.1.Final.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.iq80.snappy--snappy--org.iq80.snappy__snappy__0.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.javassist--javassist--org.javassist__javassist__3.18.1-GA.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.jboss.logging--jboss-logging--org.jboss.logging__jboss-logging__3.1.3.GA.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.jdbi--jdbi--org.jdbi__jdbi__2.63.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.joda--joda-convert--org.joda__joda-convert__1.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.jodd--jodd-core--org.jodd__jodd-core__3.5.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.json4s--json4s-ast_2.11--org.json4s__json4s-ast_2.11__3.5.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.json4s--json4s-core_2.11--org.json4s__json4s-core_2.11__3.5.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.json4s--json4s-jackson_2.11--org.json4s__json4s-jackson_2.11__3.5.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.json4s--json4s-scalap_2.11--org.json4s__json4s-scalap_2.11__3.5.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.lz4--lz4-java--org.lz4__lz4-java__1.4.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.mariadb.jdbc--mariadb-java-client--org.mariadb.jdbc__mariadb-java-client__2.1.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.mockito--mockito-core--org.mockito__mockito-core__1.10.19.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.objenesis--objenesis--org.objenesis__objenesis__2.5.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.postgresql--postgresql--org.postgresql__postgresql__42.1.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.roaringbitmap--RoaringBitmap--org.roaringbitmap__RoaringBitmap__0.7.45.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.roaringbitmap--shims--org.roaringbitmap__shims__0.7.45.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.rocksdb--rocksdbjni--org.rocksdb__rocksdbjni__6.2.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.rosuda.REngine--REngine--org.rosuda.REngine__REngine__2.1.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.scalacheck--scalacheck_2.11--org.scalacheck__scalacheck_2.11__1.12.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.scalactic--scalactic_2.11--org.scalactic__scalactic_2.11__3.0.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-parser-combinators_2.11--org.scala-lang.modules__scala-parser-combinators_2.11__1.1.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-xml_2.11--org.scala-lang.modules__scala-xml_2.11__1.0.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.scala-lang--scala-compiler_2.11--org.scala-lang__scala-compiler__2.11.12.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.scala-lang--scala-library_2.11--org.scala-lang__scala-library__2.11.12.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.scala-lang--scala-reflect_2.11--org.scala-lang__scala-reflect__2.11.12.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.scalanlp--breeze_2.11--org.scalanlp__breeze_2.11__0.13.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.scalanlp--breeze-macros_2.11--org.scalanlp__breeze-macros_2.11__0.13.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.scala-sbt--test-interface--org.scala-sbt__test-interface__1.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.scalatest--scalatest_2.11--org.scalatest__scalatest_2.11__3.0.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.slf4j--jcl-over-slf4j--org.slf4j__jcl-over-slf4j__1.7.16.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.slf4j--jul-to-slf4j--org.slf4j__jul-to-slf4j__1.7.16.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.16.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.16.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.spark-project.hive--hive-beeline--org.spark-project.hive__hive-beeline__1.2.1.spark2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.spark-project.hive--hive-cli--org.spark-project.hive__hive-cli__1.2.1.spark2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.spark-project.hive--hive-jdbc--org.spark-project.hive__hive-jdbc__1.2.1.spark2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.spark-project.hive--hive-metastore--org.spark-project.hive__hive-metastore__1.2.1.spark2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.spark-project.spark--unused--org.spark-project.spark__unused__1.0.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.spire-math--spire_2.11--org.spire-math__spire_2.11__0.13.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.spire-math--spire-macros_2.11--org.spire-math__spire-macros_2.11__0.13.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.springframework--spring-core--org.springframework__spring-core__4.1.4.RELEASE.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.springframework--spring-test--org.springframework__spring-test__4.1.4.RELEASE.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.tukaani--xz--org.tukaani__xz__1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.typelevel--machinist_2.11--org.typelevel__machinist_2.11__0.6.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.typelevel--macro-compat_2.11--org.typelevel__macro-compat_2.11__1.1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.xerial--sqlite-jdbc--org.xerial__sqlite-jdbc__3.8.11.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.yaml--snakeyaml--org.yaml__snakeyaml__1.16.jar:/databricks/jars/spark--maven-trees--spark_2.4--oro--oro--oro__oro__2.0.8.jar:/databricks/jars/spark--maven-trees--spark_2.4--software.amazon.ion--ion-java--software.amazon.ion__ion-java__1.0.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--stax--stax-api--stax__stax-api__1.0.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--xmlenc--xmlenc--xmlenc__xmlenc__0.52.jar:/databricks/jars/spark--sql-extension--sql-extension-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--versions--2.4--avro_2.11_deploy_shaded.jar:/databricks/jars/spark--versions--2.4--catalyst_2.11_deploy.jar:/databricks/jars/spark--versions--2.4--com.fasterxml.jackson.core__jackson-annotations__2.9.0_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--com.fasterxml.jackson.core__jackson-core__2.9.4_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--com.fasterxml.jackson.core__jackson-databind__2.9.4_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--com.fasterxml.jackson.jaxrs__jackson-jaxrs-base__2.9.4_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--com.fasterxml.jackson.jaxrs__jackson-jaxrs-json-provider__2.9.4_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--com.fasterxml.jackson.module__jackson-module-jaxb-annotations__2.9.4_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--com.github.shyiko__mysql-binlog-connector-java__0.16.1_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--com.google.guava__guava__20.0_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--core_2.11_deploy.jar:/databricks/jars/spark--versions--2.4--debezium-connector-mysql_2.11_deploy_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--ganglia-lgpl_2.11_deploy.jar:/databricks/jars/spark--versions--2.4--graphx_2.11_deploy.jar:/databricks/jars/spark--versions--2.4--hive_2.11_deploy_shaded.jar:/databricks/jars/spark--versions--2.4--hive-thriftserver_2.11_deploy.jar:/databricks/jars/spark--versions--2.4--io.confluent__common-utils__4.0.0_shaded-for-avro.jar:/databricks/jars/spark--versions--2.4--io.confluent__kafka-schema-registry-client__4.0.0_shaded-for-avro.jar:/databricks/jars/spark--versions--2.4--io.debezium__debezium-core__0.8.3.Final_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--io.debezium__debezium-ddl-parser__0.8.3.Final_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--io.debezium__debezium-embedded__0.8.3.Final_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--javax.annotation__javax.annotation-api__1.2_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--javax.servlet__javax.servlet-api__3.1.0_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--javax.validation__validation-api__1.1.0.Final_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--javax.ws.rs__javax.ws.rs-api__2.0.1_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--kafka-clients_only_shaded.jar:/databricks/jars/spark--versions--2.4--libspark-sql-parser-compiled.jar:/databricks/jars/spark--versions--2.4--mllib_2.11_deploy_shaded.jar:/databricks/jars/spark--versions--2.4--mllib-local_2.11_deploy.jar:/databricks/jars/spark--versions--2.4--mysql-cdc_2.11_deploy_shaded.jar:/databricks/jars/spark--versions--2.4--net.sourceforge.argparse4j__argparse4j__0.7.0_shaded-for-mysql-cdc.jar:/databricks/jars/spark--versions--2.4--org.apache.commons__commons-pool\n*** WARNING: skipped 15975 bytes of output ***\n\n\n(&#39;spark.databricks.clusterUsageTags.driverNodeType&#39;, &#39;dev-tier-node&#39;)\n(&#39;spark.hadoop.spark.sql.sources.outputCommitterClass&#39;, &#39;com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter&#39;)\n(&#39;spark.databricks.clusterUsageTags.instanceBootstrapType&#39;, &#39;ssh&#39;)\n(&#39;spark.repl.class.uri&#39;, &#39;spark://10.172.226.154:40802/classes&#39;)\n(&#39;spark.streaming.driver.writeAheadLog.allowBatching&#39;, &#39;true&#39;)\n(&#39;spark.databricks.clusterSource&#39;, &#39;UI&#39;)\n(&#39;spark.hadoop.hive.server2.transport.mode&#39;, &#39;http&#39;)\n(&#39;spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled&#39;, &#39;false&#39;)\n(&#39;spark.databricks.driverNodeTypeId&#39;, &#39;dev-tier-node&#39;)\n(&#39;spark.sql.parquet.compression.codec&#39;, &#39;snappy&#39;)\n(&#39;spark.databricks.passthrough.glue.executorServiceFactoryClassName&#39;, &#39;com.databricks.backend.daemon.driver.GlueClientExecutorServiceFactory&#39;)\n(&#39;spark.databricks.clusterUsageTags.hailEnabled&#39;, &#39;false&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled&#39;, &#39;false&#39;)\n(&#39;spark.databricks.clusterUsageTags.containerType&#39;, &#39;LXC&#39;)\n(&#39;spark.eventLog.enabled&#39;, &#39;false&#39;)\n(&#39;spark.hadoop.fs.wasb.impl&#39;, &#39;shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem&#39;)\n(&#39;spark.executor.tempDirectory&#39;, &#39;/local_disk0/tmp&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterOwnerUserId&#39;, &#39;6647443176029732&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterAllTags&#39;, &#39;[{&#34;key&#34;:&#34;Name&#34;,&#34;value&#34;:&#34;ce2-worker&#34;}]&#39;)\n(&#39;spark.databricks.workerNodeTypeId&#39;, &#39;dev-tier-node&#39;)\n(&#39;spark.driver.port&#39;, &#39;40802&#39;)\n(&#39;spark.hadoop.mapred.output.committer.class&#39;, &#39;com.databricks.backend.daemon.data.client.DirectOutputCommitter&#39;)\n(&#39;spark.hadoop.hive.server2.thrift.http.port&#39;, &#39;10000&#39;)\n(&#39;spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version&#39;, &#39;2&#39;)\n(&#39;spark.sql.allowMultipleContexts&#39;, &#39;false&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterEbsVolumeSize&#39;, &#39;0&#39;)\n(&#39;spark.home&#39;, &#39;/databricks/spark&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterTargetWorkers&#39;, &#39;0&#39;)\n(&#39;spark.sql.warehouse.dir&#39;, &#39;/user/hive/warehouse&#39;)\n(&#39;spark.hadoop.hive.server2.idle.operation.timeout&#39;, &#39;7200000&#39;)\n(&#39;spark.task.reaper.enabled&#39;, &#39;true&#39;)\n(&#39;spark.databricks.passthrough.s3a.tokenProviderClassName&#39;, &#39;com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider&#39;)\n(&#39;spark.storage.memoryFraction&#39;, &#39;0.5&#39;)\n(&#39;spark.databricks.session.share&#39;, &#39;false&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterResourceClass&#39;, &#39;default&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterFirstOnDemand&#39;, &#39;0&#39;)\n(&#39;spark.driver.maxResultSize&#39;, &#39;4g&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterName&#39;, &#39;test_2&#39;)\n(&#39;spark.databricks.delta.multiClusterWrites.enabled&#39;, &#39;true&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterSku&#39;, &#39;STANDARD_SKU&#39;)\n(&#39;spark.worker.cleanup.enabled&#39;, &#39;false&#39;)\n(&#39;spark.databricks.clusterUsageTags.enableCredentialPassthrough&#39;, &#39;false&#39;)\n(&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType&#39;, &#39;ebs_volume_type: GENERAL_PURPOSE_SSD\\n&#39;)\n(&#39;spark.databricks.clusterUsageTags.enableJdbcAutoStart&#39;, &#39;true&#39;)\n(&#39;spark.databricks.sparkContextId&#39;, &#39;4070508562953178873&#39;)\n(&#39;spark.hadoop.spark.thriftserver.closeSessionHeaderName&#39;, &#39;X-Databricks-SqlService-CloseSession&#39;)\n(&#39;spark.databricks.passthrough.glue.credentialsProviderFactoryClassName&#39;, &#39;com.databricks.backend.daemon.driver.aws.DatabricksCredentialProviderFactory&#39;)\n(&#39;spark.hadoop.fs.wasb.impl.disable.cache&#39;, &#39;true&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterEbsVolumeType&#39;, &#39;GENERAL_PURPOSE_SSD&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterLogDestination&#39;, &#39;&#39;)\n(&#39;spark.cleaner.referenceTracking.blocking&#39;, &#39;false&#39;)\n(&#39;spark.hadoop.parquet.page.size.check.estimate&#39;, &#39;false&#39;)\n(&#39;spark.r.sql.derby.temp.dir&#39;, &#39;/tmp/RtmpsHOFBM&#39;)\n(&#39;spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class&#39;, &#39;com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory&#39;)\n(&#39;spark.databricks.delta.preview.enabled&#39;, &#39;true&#39;)\n(&#39;spark.databricks.clusterUsageTags.isSingleUserCluster&#39;, &#39;false&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterState&#39;, &#39;Pending&#39;)\n(&#39;spark.databricks.tahoe.logStore.azure.class&#39;, &#39;com.databricks.tahoe.store.AzureLogStore&#39;)\n(&#39;spark.hadoop.fs.azure.skip.metrics&#39;, &#39;true&#39;)\n(&#39;spark.databricks.clusterUsageTags.sparkVersion&#39;, &#39;6.2.x-scala2.11&#39;)\n(&#39;spark.scheduler.mode&#39;, &#39;FAIR&#39;)\n(&#39;spark.master&#39;, &#39;local[8]&#39;)\n(&#39;spark.databricks.delta.logStore.crossCloud.fatal&#39;, &#39;true&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterWorkers&#39;, &#39;0&#39;)\n(&#39;spark.repl.class.outputDir&#39;, &#39;/local_disk0/tmp/repl/spark-4070508562953178873-646dfc5f-e332-4f4b-bea9-b4264ea1a7a5&#39;)\n(&#39;spark.files.fetchFailure.unRegisterOutputOnHost&#39;, &#39;true&#39;)\n(&#39;spark.hadoop.fs.s3n.impl&#39;, &#39;com.databricks.s3a.S3AFileSystem&#39;)\n(&#39;spark.databricks.clusterUsageTags.driverContainerPrivateIp&#39;, &#39;10.172.226.154&#39;)\n(&#39;spark.databricks.clusterUsageTags.enableSqlAclsOnly&#39;, &#39;false&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterEbsVolumeCount&#39;, &#39;0&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterNumSshKeys&#39;, &#39;0&#39;)\n(&#39;spark.databricks.tahoe.logStore.aws.class&#39;, &#39;com.databricks.tahoe.store.S3LockBasedLogStore&#39;)\n(&#39;spark.speculation.quantile&#39;, &#39;0.9&#39;)\n(&#39;spark.shuffle.manager&#39;, &#39;SORT&#39;)\n(&#39;spark.files.overwrite&#39;, &#39;true&#39;)\n(&#39;spark.sql.hive.metastore.sharedPrefixes&#39;, &#39;org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks&#39;)\n(&#39;spark.databricks.io.directoryCommit.enableLogicalDelete&#39;, &#39;false&#39;)\n(&#39;spark.task.reaper.killTimeout&#39;, &#39;60s&#39;)\n(&#39;spark.r.numRBackendThreads&#39;, &#39;1&#39;)\n(&#39;spark.hadoop.fs.wasbs.impl.disable.cache&#39;, &#39;true&#39;)\n(&#39;spark.hadoop.parquet.block.size.row.check.min&#39;, &#39;10&#39;)\n(&#39;spark.hadoop.hive.server2.use.SSL&#39;, &#39;true&#39;)\n(&#39;spark.hadoop.fs.abfss.impl.disable.cache&#39;, &#39;true&#39;)\n(&#39;spark.sql.hive.metastore.version&#39;, &#39;0.13.0&#39;)\n(&#39;spark.shuffle.service.port&#39;, &#39;4048&#39;)\n(&#39;spark.databricks.acl.client&#39;, &#39;com.databricks.spark.sql.acl.client.SparkSqlAclClient&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterAvailability&#39;, &#39;ON_DEMAND&#39;)\n(&#39;spark.hadoop.hive.warehouse.subdir.inherit.perms&#39;, &#39;false&#39;)\n(&#39;spark.streaming.driver.writeAheadLog.closeFileAfterWrite&#39;, &#39;true&#39;)\n(&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb&#39;, &#39;0&#39;)\n(&#39;spark.hadoop.hive.server2.keystore.path&#39;, &#39;/databricks/keys/jetty-ssl-driver-keystore.jks&#39;)\n(&#39;spark.ui.port&#39;, &#39;43605&#39;)\n(&#39;spark.databricks.credential.redactor&#39;, &#39;com.databricks.logging.secrets.CredentialRedactorProxyImpl&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterPinned&#39;, &#39;false&#39;)\n(&#39;spark.sql.ui.retainedExecutions&#39;, &#39;100&#39;)\n(&#39;spark.databricks.acl.provider&#39;, &#39;com.databricks.sql.acl.ReflectionBackedAclProvider&#39;)\n(&#39;spark.extraListeners&#39;, &#39;com.databricks.backend.daemon.driver.DBCEventLoggingListener&#39;)\n(&#39;spark.executor.memory&#39;, &#39;4800m&#39;)\n(&#39;spark.databricks.clusterUsageTags.enableElasticDisk&#39;, &#39;false&#39;)\n(&#39;spark.sql.parquet.cacheMetadata&#39;, &#39;true&#39;)\n(&#39;spark.databricks.clusterUsageTags.driverContainerId&#39;, &#39;b16e7a40c8e141abbca31c0ba0f32beb&#39;)\n(&#39;spark.hadoop.fs.adl.impl&#39;, &#39;com.databricks.adl.AdlFileSystem&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterNodeType&#39;, &#39;dev-tier-node&#39;)\n(&#39;spark.databricks.clusterUsageTags.enableLocalDiskEncryption&#39;, &#39;false&#39;)\n(&#39;spark.databricks.tahoe.logStore.class&#39;, &#39;com.databricks.tahoe.store.DelegatingLogStore&#39;)\n(&#39;spark.databricks.cloudProvider&#39;, &#39;AWS&#39;)\n(&#39;spark.sql.hive.convertMetastoreParquet&#39;, &#39;true&#39;)\n(&#39;spark.executor.id&#39;, &#39;driver&#39;)\n(&#39;spark.databricks.passthrough.adls.tokenProviderClassName&#39;, &#39;com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider&#39;)\n(&#39;spark.app.name&#39;, &#39;Databricks Shell&#39;)\n(&#39;spark.driver.allowMultipleContexts&#39;, &#39;false&#39;)\n(&#39;spark.databricks.clusterUsageTags.workerEnvironmentId&#39;, &#39;default-worker-env&#39;)\n(&#39;spark.driver.host&#39;, &#39;10.172.226.154&#39;)\n(&#39;spark.rdd.compress&#39;, &#39;true&#39;)\n(&#39;spark.databricks.eventLog.dir&#39;, &#39;eventlogs&#39;)\n(&#39;spark.sql.catalogImplementation&#39;, &#39;hive&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterCreator&#39;, &#39;Webapp&#39;)\n(&#39;spark.databricks.clusterUsageTags.driverInstancePrivateIp&#39;, &#39;10.172.234.143&#39;)\n(&#39;spark.speculation&#39;, &#39;false&#39;)\n(&#39;spark.hadoop.fs.s3a.multipart.size&#39;, &#39;10485760&#39;)\n(&#39;spark.databricks.clusterUsageTags.cloudProvider&#39;, &#39;AWS&#39;)\n(&#39;spark.hadoop.databricks.dbfs.client.version&#39;, &#39;v1&#39;)\n(&#39;spark.hadoop.hive.server2.session.check.interval&#39;, &#39;60000&#39;)\n(&#39;spark.sql.hive.convertCTAS&#39;, &#39;true&#39;)\n(&#39;spark.metrics.conf&#39;, &#39;/databricks/spark/conf/metrics.properties&#39;)\n(&#39;spark.hadoop.spark.sql.parquet.output.committer.class&#39;, &#39;org.apache.spark.sql.parquet.DirectParquetOutputCommitter&#39;)\n(&#39;spark.hadoop.fs.s3a.fast.upload.default&#39;, &#39;true&#39;)\n(&#39;spark.akka.frameSize&#39;, &#39;256&#39;)\n(&#39;spark.hadoop.fs.s3a.fast.upload&#39;, &#39;true&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterGeneration&#39;, &#39;0&#39;)\n(&#39;spark.hadoop.fs.s3.impl&#39;, &#39;com.databricks.s3a.S3AFileSystem&#39;)\n(&#39;spark.databricks.clusterUsageTags.driverPublicDns&#39;, &#39;ec2-35-165-181-116.us-west-2.compute.amazonaws.com&#39;)\n(&#39;spark.hadoop.fs.abfs.impl.disable.cache&#39;, &#39;true&#39;)\n(&#39;spark.hadoop.fs.wasbs.impl&#39;, &#39;shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem&#39;)\n(&#39;spark.hadoop.hive.server2.keystore.password&#39;, &#39;gb1gQqZ9ZIHS&#39;)\n(&#39;spark.speculation.multiplier&#39;, &#39;3&#39;)\n(&#39;spark.storage.blockManagerTimeoutIntervalMs&#39;, &#39;300000&#39;)\n(&#39;spark.databricks.overrideDefaultCommitProtocol&#39;, &#39;org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterNoDriverDaemon&#39;, &#39;false&#39;)\n(&#39;spark.databricks.clusterUsageTags.instanceWorkerEnvId&#39;, &#39;default-worker-env&#39;)\n(&#39;spark.hadoop.parquet.memory.pool.ratio&#39;, &#39;0.5&#39;)\n(&#39;spark.executor.extraJavaOptions&#39;, &#39;-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=256m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Ddatabricks.serviceName=spark-executor-1&#39;)\n(&#39;spark.sparkr.use.daemon&#39;, &#39;false&#39;)\n(&#39;spark.scheduler.listenerbus.eventqueue.capacity&#39;, &#39;20000&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterScalingType&#39;, &#39;fixed_size&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterStateMessage&#39;, &#39;Starting Spark&#39;)\n(&#39;spark.sql.hive.metastore.jars&#39;, &#39;/databricks/hive/*&#39;)\n(&#39;spark.databricks.passthrough.adls.gen2.tokenProviderClassName&#39;, &#39;com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider&#39;)\n(&#39;spark.hadoop.parquet.page.write-checksum.enabled&#39;, &#39;true&#39;)\n(&#39;spark.hadoop.databricks.s3commit.client.sslTrustAll&#39;, &#39;false&#39;)\n(&#39;spark.hadoop.fs.s3a.threads.max&#39;, &#39;136&#39;)\n(&#39;spark.r.backendConnectionTimeout&#39;, &#39;604800&#39;)\n(&#39;spark.serializer.objectStreamReset&#39;, &#39;100&#39;)\n(&#39;spark.sql.sources.commitProtocolClass&#39;, &#39;com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol&#39;)\n(&#39;spark.hadoop.hive.server2.idle.session.timeout&#39;, &#39;900000&#39;)\n(&#39;spark.databricks.redactor&#39;, &#39;com.databricks.spark.util.DatabricksSparkLogRedactorProxy&#39;)\n(&#39;spark.databricks.clusterUsageTags.autoTerminationMinutes&#39;, &#39;120&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterPythonVersion&#39;, &#39;3&#39;)\n(&#39;spark.databricks.clusterUsageTags.enableDfAcls&#39;, &#39;false&#39;)\n(&#39;spark.hadoop.fs.abfss.impl&#39;, &#39;shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem&#39;)\n(&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount&#39;, &#39;0&#39;)\n(&#39;spark.shuffle.service.enabled&#39;, &#39;true&#39;)\n(&#39;spark.hadoop.spark.driverproxy.customHeadersToProperties&#39;, &#39;X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds&#39;)\n(&#39;spark.hadoop.parquet.page.verify-checksum.enabled&#39;, &#39;true&#39;)\n(&#39;spark.hadoop.fs.s3a.multipart.threshold&#39;, &#39;104857600&#39;)\n(&#39;spark.hadoop.fs.s3a.impl&#39;, &#39;com.databricks.s3a.S3AFileSystem&#39;)\n(&#39;spark.databricks.clusterUsageTags.dataPlaneRegion&#39;, &#39;us-west-2&#39;)\n(&#39;spark.rpc.message.maxSize&#39;, &#39;256&#39;)\n(&#39;spark.logConf&#39;, &#39;true&#39;)\n(&#39;spark.databricks.clusterUsageTags.enableJobsAutostart&#39;, &#39;true&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterMetastoreAccessType&#39;, &#39;RDS_DIRECT&#39;)\n(&#39;spark.hadoop.hive.server2.enable.doAs&#39;, &#39;false&#39;)\n(&#39;eventLog.rolloverIntervalSeconds&#39;, &#39;3600&#39;)\n(&#39;spark.shuffle.memoryFraction&#39;, &#39;0.2&#39;)\n(&#39;spark.databricks.clusterUsageTags.driverInstanceId&#39;, &#39;i-012430e34554631d4&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterOwnerOrgId&#39;, &#39;5741052354962972&#39;)\n(&#39;spark.databricks.clusterUsageTags.containerZoneId&#39;, &#39;us-west-2c&#39;)\n(&#39;spark.app.id&#39;, &#39;local-1578960722885&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterId&#39;, &#39;0114-001139-ember136&#39;)\n(&#39;spark.databricks.clusterUsageTags.region&#39;, &#39;us-west-2&#39;)\n(&#39;spark.databricks.clusterUsageTags.clusterSpotBidPricePercent&#39;, &#39;100&#39;)\n(&#39;spark.files.useFetchCache&#39;, &#39;false&#39;)\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.context import SparkContext\nprint(spark)\n\nspark.sparkContext.getConf().getAll()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;pyspark.sql.session.SparkSession object at 0x7fcbad6e8a58&gt;\nOut[6]: [(&#39;spark.databricks.preemption.enabled&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.abfs.impl&#39;,\n  &#39;shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem&#39;),\n (&#39;spark.driver.tempDirectory&#39;, &#39;/local_disk0/tmp&#39;),\n (&#39;spark.hadoop.fs.adl.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.parquet.block.size.row.check.max&#39;, &#39;10&#39;),\n (&#39;spark.hadoop.fs.s3a.connection.maximum&#39;, &#39;200&#39;),\n (&#39;spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2&#39;, &#39;0&#39;),\n (&#39;spark.shuffle.reduceLocality.enabled&#39;, &#39;false&#39;),\n (&#39;spark.sql.streaming.checkpointFileManagerClass&#39;,\n  &#39;com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverNodeType&#39;, &#39;dev-tier-node&#39;),\n (&#39;spark.hadoop.spark.sql.sources.outputCommitterClass&#39;,\n  &#39;com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverInstancePrivateIp&#39;,\n  &#39;10.172.239.124&#39;),\n (&#39;spark.databricks.clusterUsageTags.instanceBootstrapType&#39;, &#39;ssh&#39;),\n (&#39;spark.streaming.driver.writeAheadLog.allowBatching&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterSource&#39;, &#39;UI&#39;),\n (&#39;spark.hadoop.hive.server2.transport.mode&#39;, &#39;http&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverContainerPrivateIp&#39;,\n  &#39;10.172.251.172&#39;),\n (&#39;spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled&#39;, &#39;false&#39;),\n (&#39;spark.executor.memory&#39;, &#39;8278m&#39;),\n (&#39;spark.databricks.driverNodeTypeId&#39;, &#39;dev-tier-node&#39;),\n (&#39;spark.sql.parquet.compression.codec&#39;, &#39;snappy&#39;),\n (&#39;spark.databricks.passthrough.glue.executorServiceFactoryClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.GlueClientExecutorServiceFactory&#39;),\n (&#39;spark.databricks.clusterUsageTags.hailEnabled&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverContainerId&#39;,\n  &#39;e75be9826a4047e19a498eb192d6782e&#39;),\n (&#39;spark.databricks.clusterUsageTags.containerType&#39;, &#39;LXC&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled&#39;, &#39;false&#39;),\n (&#39;spark.eventLog.enabled&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.wasb.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem&#39;),\n (&#39;spark.executor.tempDirectory&#39;, &#39;/local_disk0/tmp&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterOwnerUserId&#39;, &#39;6647443176029732&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterAllTags&#39;,\n  &#39;[{&#34;key&#34;:&#34;Name&#34;,&#34;value&#34;:&#34;ce2-worker&#34;}]&#39;),\n (&#39;spark.databricks.workerNodeTypeId&#39;, &#39;dev-tier-node&#39;),\n (&#39;spark.hadoop.mapred.output.committer.class&#39;,\n  &#39;com.databricks.backend.daemon.data.client.DirectOutputCommitter&#39;),\n (&#39;spark.app.id&#39;, &#39;local-1585552399513&#39;),\n (&#39;spark.hadoop.hive.server2.thrift.http.port&#39;, &#39;10000&#39;),\n (&#39;spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version&#39;, &#39;2&#39;),\n (&#39;spark.sql.allowMultipleContexts&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterEbsVolumeSize&#39;, &#39;0&#39;),\n (&#39;spark.home&#39;, &#39;/databricks/spark&#39;),\n (&#39;spark.repl.class.uri&#39;, &#39;spark://10.172.251.172:43262/classes&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterTargetWorkers&#39;, &#39;0&#39;),\n (&#39;spark.sql.warehouse.dir&#39;, &#39;/user/hive/warehouse&#39;),\n (&#39;spark.hadoop.hive.server2.idle.operation.timeout&#39;, &#39;7200000&#39;),\n (&#39;spark.task.reaper.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.passthrough.s3a.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider&#39;),\n (&#39;spark.storage.memoryFraction&#39;, &#39;0.5&#39;),\n (&#39;spark.databricks.session.share&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterResourceClass&#39;, &#39;default&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterFirstOnDemand&#39;, &#39;0&#39;),\n (&#39;spark.master&#39;, &#39;local[2]&#39;),\n (&#39;spark.driver.maxResultSize&#39;, &#39;4g&#39;),\n (&#39;spark.databricks.delta.multiClusterWrites.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterSku&#39;, &#39;STANDARD_SKU&#39;),\n (&#39;spark.worker.cleanup.enabled&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkVersion&#39;, &#39;6.4.x-scala2.11&#39;),\n (&#39;spark.databricks.workspace.matplotlibInline.enabled&#39;, &#39;true&#39;),\n (&#39;spark.driver.port&#39;, &#39;43262&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableCredentialPassthrough&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType&#39;,\n  &#39;ebs_volume_type: GENERAL_PURPOSE_SSD\\n&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableJdbcAutoStart&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.spark.thriftserver.closeSessionHeaderName&#39;,\n  &#39;X-Databricks-SqlService-CloseSession&#39;),\n (&#39;spark.databricks.passthrough.glue.credentialsProviderFactoryClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.aws.DatabricksCredentialProviderFactory&#39;),\n (&#39;spark.hadoop.fs.wasb.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterEbsVolumeType&#39;,\n  &#39;GENERAL_PURPOSE_SSD&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterLogDestination&#39;, &#39;&#39;),\n (&#39;spark.cleaner.referenceTracking.blocking&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.parquet.page.size.check.estimate&#39;, &#39;false&#39;),\n (&#39;spark.databricks.sparkContextId&#39;, &#39;3774936958149314027&#39;),\n (&#39;spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class&#39;,\n  &#39;com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory&#39;),\n (&#39;spark.databricks.delta.preview.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.isSingleUserCluster&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterState&#39;, &#39;Pending&#39;),\n (&#39;spark.databricks.tahoe.logStore.azure.class&#39;,\n  &#39;com.databricks.tahoe.store.AzureLogStore&#39;),\n (&#39;spark.hadoop.fs.azure.skip.metrics&#39;, &#39;true&#39;),\n (&#39;spark.r.sql.derby.temp.dir&#39;, &#39;/tmp/RtmpWPmRVU&#39;),\n (&#39;spark.scheduler.mode&#39;, &#39;FAIR&#39;),\n (&#39;spark.databricks.delta.logStore.crossCloud.fatal&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterWorkers&#39;, &#39;0&#39;),\n (&#39;spark.files.fetchFailure.unRegisterOutputOnHost&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.s3n.impl&#39;, &#39;com.databricks.s3a.S3AFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableSqlAclsOnly&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterEbsVolumeCount&#39;, &#39;0&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNumSshKeys&#39;, &#39;0&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterName&#39;, &#39;test_02&#39;),\n (&#39;spark.databricks.tahoe.logStore.aws.class&#39;,\n  &#39;com.databricks.tahoe.store.S3LockBasedLogStore&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterId&#39;, &#39;0330-070946-pluck480&#39;),\n (&#39;spark.speculation.quantile&#39;, &#39;0.9&#39;),\n (&#39;spark.shuffle.manager&#39;, &#39;SORT&#39;),\n (&#39;spark.files.overwrite&#39;, &#39;true&#39;),\n (&#39;spark.sql.hive.metastore.sharedPrefixes&#39;,\n  &#39;org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks&#39;),\n (&#39;spark.databricks.io.directoryCommit.enableLogicalDelete&#39;, &#39;false&#39;),\n (&#39;spark.ui.port&#39;, &#39;40315&#39;),\n (&#39;spark.task.reaper.killTimeout&#39;, &#39;60s&#39;),\n (&#39;spark.r.numRBackendThreads&#39;, &#39;1&#39;),\n (&#39;spark.hadoop.fs.wasbs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.parquet.block.size.row.check.min&#39;, &#39;10&#39;),\n (&#39;spark.hadoop.hive.server2.use.SSL&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.abfss.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.sql.hive.metastore.version&#39;, &#39;0.13.0&#39;),\n (&#39;spark.shuffle.service.port&#39;, &#39;4048&#39;),\n (&#39;spark.databricks.acl.client&#39;,\n  &#39;com.databricks.spark.sql.acl.client.SparkSqlAclClient&#39;),\n (&#39;spark.repl.class.outputDir&#39;,\n  &#39;/local_disk0/tmp/repl/spark-3774936958149314027-621aaede-b5fe-4cf6-a26b-472512a8beec&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterAvailability&#39;, &#39;ON_DEMAND&#39;),\n (&#39;spark.hadoop.hive.warehouse.subdir.inherit.perms&#39;, &#39;false&#39;),\n (&#39;spark.streaming.driver.writeAheadLog.closeFileAfterWrite&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb&#39;, &#39;0&#39;),\n (&#39;spark.hadoop.hive.server2.keystore.path&#39;,\n  &#39;/databricks/keys/jetty-ssl-driver-keystore.jks&#39;),\n (&#39;spark.databricks.sqlservice.querylog.client&#39;,\n  &#39;com.databricks.spark.sqlservice.client.DriverToSqlServiceQueryLogClient&#39;),\n (&#39;spark.databricks.credential.redactor&#39;,\n  &#39;com.databricks.logging.secrets.CredentialRedactorProxyImpl&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterPinned&#39;, &#39;false&#39;),\n (&#39;spark.sql.ui.retainedExecutions&#39;, &#39;100&#39;),\n (&#39;spark.databricks.acl.provider&#39;,\n  &#39;com.databricks.sql.acl.ReflectionBackedAclProvider&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverPublicDns&#39;,\n  &#39;ec2-34-212-171-48.us-west-2.compute.amazonaws.com&#39;),\n (&#39;spark.extraListeners&#39;,\n  &#39;com.databricks.backend.daemon.driver.DBCEventLoggingListener&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableElasticDisk&#39;, &#39;false&#39;),\n (&#39;spark.sql.parquet.cacheMetadata&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.adl.impl&#39;, &#39;com.databricks.adl.AdlFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNodeType&#39;, &#39;dev-tier-node&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableLocalDiskEncryption&#39;, &#39;false&#39;),\n (&#39;spark.databricks.tahoe.logStore.class&#39;,\n  &#39;com.databricks.tahoe.store.DelegatingLogStore&#39;),\n (&#39;spark.databricks.cloudProvider&#39;, &#39;AWS&#39;),\n (&#39;spark.sql.hive.convertMetastoreParquet&#39;, &#39;true&#39;),\n (&#39;spark.executor.id&#39;, &#39;driver&#39;),\n (&#39;spark.databricks.passthrough.adls.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider&#39;),\n (&#39;spark.app.name&#39;, &#39;Databricks Shell&#39;),\n (&#39;spark.driver.allowMultipleContexts&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.workerEnvironmentId&#39;,\n  &#39;default-worker-env&#39;),\n (&#39;spark.rdd.compress&#39;, &#39;true&#39;),\n (&#39;spark.databricks.eventLog.dir&#39;, &#39;eventlogs&#39;),\n (&#39;spark.sql.catalogImplementation&#39;, &#39;hive&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterCreator&#39;, &#39;Webapp&#39;),\n (&#39;spark.speculation&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.s3a.multipart.size&#39;, &#39;10485760&#39;),\n (&#39;spark.databricks.clusterUsageTags.cloudProvider&#39;, &#39;AWS&#39;),\n (&#39;spark.hadoop.databricks.dbfs.client.version&#39;, &#39;v1&#39;),\n (&#39;spark.hadoop.hive.server2.session.check.interval&#39;, &#39;60000&#39;),\n (&#39;spark.sql.hive.convertCTAS&#39;, &#39;true&#39;),\n (&#39;spark.metrics.conf&#39;, &#39;/databricks/spark/conf/metrics.properties&#39;),\n (&#39;spark.hadoop.spark.sql.parquet.output.committer.class&#39;,\n  &#39;org.apache.spark.sql.parquet.DirectParquetOutputCommitter&#39;),\n (&#39;spark.hadoop.fs.s3a.fast.upload.default&#39;, &#39;true&#39;),\n (&#39;spark.akka.frameSize&#39;, &#39;256&#39;),\n (&#39;spark.hadoop.fs.s3a.fast.upload&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterGeneration&#39;, &#39;0&#39;),\n (&#39;spark.hadoop.fs.s3.impl&#39;, &#39;com.databricks.s3a.S3AFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverInstanceId&#39;, &#39;i-01e2d8f5d8655e69c&#39;),\n (&#39;spark.hadoop.fs.abfs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.driver.host&#39;, &#39;10.172.251.172&#39;),\n (&#39;spark.sql.streaming.stopTimeout&#39;, &#39;15s&#39;),\n (&#39;spark.hadoop.hive.server2.keystore.password&#39;, &#39;gb1gQqZ9ZIHS&#39;),\n (&#39;spark.speculation.multiplier&#39;, &#39;3&#39;),\n (&#39;spark.hadoop.fs.wasbs.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem&#39;),\n (&#39;spark.storage.blockManagerTimeoutIntervalMs&#39;, &#39;300000&#39;),\n (&#39;spark.databricks.overrideDefaultCommitProtocol&#39;,\n  &#39;org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol&#39;),\n (&#39;spark.executor.extraClassPath&#39;,\n  &#39;/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/api-base--api-base_java-spark_2.4_2.11_deploy.jar:/databricks/jars/api-base--api-base-spark_2.4_2.11_deploy.jar:/databricks/jars/api--rpc--rpc_parser-spark_2.4_2.11_deploy.jar:/databricks/jars/chauffeur-api--api--endpoints--endpoints-spark_2.4_2.11_deploy.jar:/databricks/jars/chauffeur-api--chauffeur-api-spark_2.4_2.11_deploy.jar:/databricks/jars/common--client--client-spark_2.4_2.11_deploy.jar:/databricks/jars/common--common-spark_2.4_2.11_deploy.jar:/databricks/jars/common--credentials--credentials-spark_2.4_2.11_deploy.jar:/databricks/jars/common--hadoop--hadoop-spark_2.4_2.11_deploy.jar:/databricks/jars/common--jetty--client--client-spark_2.4_2.11_deploy.jar:/databricks/jars/common--lazy--lazy-spark_2.4_2.11_deploy.jar:/databricks/jars/common--libcommon_resources.jar:/databricks/jars/common--path--path-spark_2.4_2.11_deploy.jar:/databricks/jars/common--rate-limiter--rate-limiter-spark_2.4_2.11_deploy.jar:/databricks/jars/common--storage--storage-spark_2.4_2.11_deploy.jar:/databricks/jars/common--tracing--tracing-spark_2.4_2.11_deploy.jar:/databricks/jars/daemon--data--client--client-spark_2.4_2.11_deploy.jar:/databricks/jars/daemon--data--client--conf--conf-spark_2.4_2.11_deploy.jar:/databricks/jars/daemon--data--client--utils-spark_2.4_2.11_deploy.jar:/databricks/jars/daemon--data--data-common--data-common-spark_2.4_2.11_deploy.jar:/databricks/jars/dbfs--utils--dbfs-utils-spark_2.4_2.11_deploy.jar:/databricks/jars/extern--acl--auth--auth-spark_2.4_2.11_deploy.jar:/databricks/jars/extern--extern-spark_2.4_2.11_deploy.jar:/databricks/jars/extern--libaws-regions.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-client-common_deploy.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-shim-common_deploy.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-shim-hive1_deploy.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-shim-hive2_deploy.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-shim-loader_deploy.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-spark-client_deploy.jar:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/jsonutil--jsonutil-spark_2.4_2.11_deploy.jar:/databricks/jars/libraries--api--managedLibraries--managedLibraries-spark_2.4_2.11_deploy.jar:/databricks/jars/libraries--api--typemappers--typemappers-spark_2.4_2.11_deploy.jar:/databricks/jars/libraries--libraries-spark_2.4_2.11_deploy.jar:/databricks/jars/logging--log4j-mod--log4j-mod-spark_2.4_2.11_deploy.jar:/databricks/jars/logging--utils--logging-utils-spark_2.4_2.11_deploy.jar:/databricks/jars/s3commit--client--client-spark_2.4_2.11_deploy.jar:/databricks/jars/s3commit--common--common-spark_2.4_2.11_deploy.jar:/databricks/jars/s3--s3-spark_2.4_2.11_deploy.jar:/databricks/jars/----scalapb_090--com.fasterxml.jackson.core__jackson-core__2.9.9_shaded.jar:/databricks/jars/----scalapb_090--com.google.android__annotations__4.1.1.4_shaded.jar:/databricks/jars/----scalapb_090--com.google.api.grpc__proto-google-common-protos__1.12.0_shaded.jar:/databricks/jars/----scalapb_090--com.google.code.gson__gson__2.7_shaded.jar:/databricks/jars/----scalapb_090--com.google.errorprone__error_prone_annotations__2.3.2_shaded.jar:/databricks/jars/----scalapb_090--com.google.guava__guava__20.0_shaded.jar:/databricks/jars/----scalapb_090--com.google.protobuf__protobuf-java__3.7.1_shaded.jar:/databricks/jars/----scalapb_090--com.google.protobuf__protobuf-java-util__3.7.1_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__fastparse_2.11__2.1.2_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__sourcecode_2.11__0.1.6_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-api__1.22.1_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-context__1.22.1_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-core__1.22.1_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-netty__1.22.1_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-protobuf__1.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-protobuf-lite__1.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-stub__1.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.opencensus__opencensus-api__0.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.opencensus__opencensus-contrib-grpc-metrics__0.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.perfmark__perfmark-api__0.16.0_shaded.jar:/databricks/jars/----scalapb_090--org.codehaus.mojo__animal-sniffer-annotations__1.17_shaded.jar:/databricks/jars/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.11_deploy_shaded.jar:/databricks/jars/secret-manager--api--api-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--common--spark-common-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--common-utils--utils-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--conf-reader--conf-reader_lib-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--dbutils--dbutils-api-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--antlr--parser-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--common--driver-common-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--display--display-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--driver-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--events-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--ml--ml-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--secret-redaction-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--spark--resources-resources.jar:/databricks/jars/spark--maven-trees--hive-exec-with-glue--hive-12679-patch_deploy.jar:/databricks/jars/spark--maven-trees--hive-exec-with-glue--hive-exec_shaded.jar:/databricks/jars/spark--maven-trees--spark_2.4--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.8.10.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-glue--com.amazonaws__aws-java-sdk-glue__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.carrotsearch--hppc--com.carrotsearch__hppc__0.7.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.chuusai--shapeless_2.11--com.chuusai__shapeless_2.11__2.3.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.7.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.databricks--dbml-local_2.11--com.databricks__dbml-local_2.11__0.5.0-db8-spark2.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.databricks--dbml-local_2.11-tests--com.databricks__dbml-local_2.11-tests__0.5.0-db8-spark2.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.databricks--Rserve--com.databricks__Rserve__1.8-3.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.databricks.scalapb--compilerplugin_2.11--com.databricks.scalapb__compilerplugin_2.11__0.4.15-9.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.databricks.scalapb--scalapb-runtime_2.11--com.databricks.scalapb__scalapb-runtime_2.11__0.4.15-9.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.esotericsoftware--kryo-shaded--com.esotericsoftware__kryo-shaded__4.0.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.esotericsoftware--minlog--com.esotericsoftware__minlog__1.3.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml--classmate--com.fasterxml__classmate__1.0.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.6.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.6.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.6.7.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.6.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.datatype--jackson-datatype-joda--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.6.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.module--jackson-module-paranamer--com.fasterxml.jackson.module__jackson-module-paranamer__2.6.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.module--jackson-module-scala_2.11--com.fasterxml.jackson.module__jackson-module-scala_2.11__2.6.7.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil--jniloader--com.github.fommil__jniloader__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--core--com.github.fommil.netlib__core__1.1.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--native_ref-java--com.github.fommil.netlib__native_ref-java__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--native_ref-java-natives--com.github.fommil.netlib__native_ref-java-natives__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--native_system-java--com.github.fommil.netlib__native_system-java__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--native_system-java-natives--com.github.fommil.netlib__native_system-java-natives__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--netlib-native_ref-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_ref-linux-x86_64-natives__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--netlib-native_system-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_system-linux-x86_64-natives__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.luben--zstd-jni--com.github.luben__zstd-jni__1.3.2-2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.github.rwl--jtransforms--com.github.rwl__jtransforms__2.4.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__2.0.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.googlecode.javaewah--JavaEWAH--com.googlecode.javaewah__JavaEWAH__0.3.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.google.guava--guava--com.google.guava__guava__15.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.h2database--h2--com.h2database__h2__1.3.174.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.jcraft--jsch--com.jcraft__jsch__0.1.50.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.2.8.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.microsoft.sqlserver--mssql-jdbc--com.microsoft.sqlserver__mssql-jdbc__6.2.2.jre8.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-dbcp--commons-dbcp--commons-dbcp__commons-dbcp__1.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-io--commons-io--commons-io__commons-io__2.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-net--commons-net--commons-net__commons-net__3.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--commons-pool--commons-pool--commons-pool__commons-pool__1.5.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.ning--compress-lzf--com.ning__compress-lzf__1.0.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.sun.mail--javax.mail--com.sun.mail__javax.mail__1.5.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.trueaccord.lenses--lenses_2.11--com.trueaccord.lenses__lenses_2.11__0.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.twitter--chill_2.11--com.twitter__chill_2.11__0.9.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.twitter--chill-java--com.twitter__chill-java__0.9.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.twitter--parquet-hadoop-bundle--com.twitter__parquet-hadoop-bundle__1.6.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.twitter--util-app_2.11--com.twitter__util-app_2.11__6.23.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.twitter--util-core_2.11--com.twitter__util-core_2.11__6.23.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.twitter--util-jvm_2.11--com.twitter__util-jvm_2.11__6.23.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.typesafe--config--com.typesafe__config__1.2.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-api_2.11--com.typesafe.scala-logging__scala-logging-api_2.11__2.1.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-slf4j_2.11--com.typesafe.scala-logging__scala-logging-slf4j_2.11__2.1.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.univocity--univocity-parsers--com.univocity__univocity-parsers__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.vlkan--flatbuffers--com.vlkan__flatbuffers__1.2.0-3f79e055.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.zaxxer--HikariCP--com.zaxxer__HikariCP__3.1.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.airlift--aircompressor--io.airlift__aircompressor__0.10.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-ganglia--io.dropwizard.metrics__metrics-ganglia__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-graphite--io.dropwizard.metrics__metrics-graphite__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-log4j--io.dropwizard.metrics__metrics-log4j__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__3.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.netty--netty-all--io.netty__netty-all__4.1.42.Final.jar:/databricks/jars/spark--maven-trees--spark_2.4--io.netty--netty--io.netty__netty__3.9.9.Final.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.activation--activation--javax.activation__activation__1.1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.annotation--javax.annotation-api--javax.annotation__javax.annotation-api__1.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.transaction--jta--javax.transaction__jta__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.validation--validation-api--javax.validation__validation-api__1.1.0.Final.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.ws.rs--javax.ws.rs-api--javax.ws.rs__javax.ws.rs-api__2.0.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.xml.bind--jaxb-api--javax.xml.bind__jaxb-api__2.2.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--javax.xml.stream--stax-api--javax.xml.stream__stax-api__1.0-2.jar:/databricks/jars/spark--maven-trees--spark_2.4--javolution--javolution--javolution__javolution__5.5.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--jline--jline--jline__jline__2.14.6.jar:/databricks/jars/spark--maven-trees--spark_2.4--joda-time--joda-time--joda-time__joda-time__2.9.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--junit--junit--junit__junit__4.12.jar:/databricks/jars/spark--maven-trees--spark_2.4--liball_deps_2.11.jar:/databricks/jars/spark--maven-trees--spark_2.4--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:/databricks/jars/spark--maven-trees--spark_2.4--log4j--log4j--log4j__log4j__1.2.17.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.hydromatic--eigenbase-properties--net.hydromatic__eigenbase-properties__1.1.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.razorvine--pyrolite--net.razorvine__pyrolite__4.13.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.sf.opencsv--opencsv--net.sf.opencsv__opencsv__2.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.sf.supercsv--super-csv--net.sf.supercsv__super-csv__2.2.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.snowflake--snowflake-ingest-sdk--net.snowflake__snowflake-ingest-sdk__0.9.6.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.snowflake--snowflake-jdbc--net.snowflake__snowflake-jdbc__3.12.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.snowflake--spark-snowflake_2.11--net.snowflake__spark-snowflake_2.11__2.5.9-spark_2.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--net.sourceforge.f2j--arpack_combined_all--net.sourceforge.f2j__arpack_combined_all__0.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.acplt--oncrpc--org.acplt__oncrpc__1.0.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.antlr--antlr4-runtime--org.antlr__antlr4-runtime__4.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.ant--ant-jsch--org.apache.ant__ant-jsch__1.9.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.arrow--arrow-format--org.apache.arrow__arrow-format__0.10.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.arrow--arrow-memory--org.apache.arrow__arrow-memory__0.10.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.arrow--arrow-vector--org.apache.arrow__arrow-vector__0.10.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.avro--avro-ipc--org.apache.avro__avro-ipc__1.8.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.avro--avro-mapred-hadoop2--org.apache.avro__avro-mapred-hadoop2__1.8.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.calcite--calcite-avatica--org.apache.calcite__calcite-avatica__1.2.0-incubating.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.calcite--calcite-core--org.apache.calcite__calcite-core__1.2.0-incubating.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.calcite--calcite-linq4j--org.apache.calcite__calcite-linq4j__1.2.0-incubating.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.8.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.commons--commons-crypto--org.apache.commons__commons-crypto__1.0.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.derby--derby--org.apache.derby__derby__10.12.1.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-client--org.apache.hadoop__hadoop-client__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-hdfs--org.apache.hadoop__hadoop-hdfs__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-app--org.apache.hadoop__hadoop-mapreduce-client-app__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-common--org.apache.hadoop__hadoop-mapreduce-client-common__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-core--org.apache.hadoop__hadoop-mapreduce-client-core__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-jobclient--org.apache.hadoop__hadoop-mapreduce-client-jobclient__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-shuffle--org.apache.hadoop__hadoop-mapreduce-client-shuffle__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-yarn-api--org.apache.hadoop__hadoop-yarn-api__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-yarn-client--org.apache.hadoop__hadoop-yarn-client__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-yarn-common--org.apache.hadoop__hadoop-yarn-common__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-yarn-server-common--org.apache.hadoop__hadoop-yarn-server-common__2.7.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.6.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.10.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.ivy--ivy--org.apache.ivy__ivy__2.4.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.orc--orc-core-nohive--org.apache.orc__orc-core-nohive__1.5.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.orc--orc-mapreduce-nohive--org.apache.orc__orc-mapreduce-nohive__1.5.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.orc--orc-shims--org.apache.orc__orc-shims__1.5.5.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-column--org.apache.parquet__parquet-column__1.10.1.2-databricks4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-common--org.apache.parquet__parquet-common__1.10.1.2-databricks4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-encoding--org.apache.parquet__parquet-encoding__1.10.1.2-databricks4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-format--org.apache.parquet__parquet-format__2.4.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-hadoop--org.apache.parquet__parquet-hadoop__1.10.1.2-databricks4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-jackson--org.apache.parquet__parquet-jackson__1.10.1.2-databricks4.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.9.3.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.xbean--xbean-asm6-shaded--org.apache.xbean__xbean-asm6-shaded__4.8.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-jaxrs--org.codehaus.jackson__jackson-jaxrs__1.9.13.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-xc--org.codehaus.jackson__jackson-xc__1.9.13.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.janino--commons-compiler--org.codehaus.janino__commons-compiler__3.0.10.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.janino--janino--org.codehaus.janino__janino__3.0.10.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__3.2.6.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__3.2.10.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__3.2.9.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-client--org.eclipse.jetty__jetty-client__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-continuation--org.eclipse.jetty__jetty-continuation__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-http--org.eclipse.jetty__jetty-http__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-io--org.eclipse.jetty__jetty-io__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-jndi--org.eclipse.jetty__jetty-jndi__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-plus--org.eclipse.jetty__jetty-plus__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-proxy--org.eclipse.jetty__jetty-proxy__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-security--org.eclipse.jetty__jetty-security__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-server--org.eclipse.jetty__jetty-server__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlet--org.eclipse.jetty__jetty-servlet__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlets--org.eclipse.jetty__jetty-servlets__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-util--org.eclipse.jetty__jetty-util__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-webapp--org.eclipse.jetty__jetty-webapp__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-xml--org.eclipse.jetty__jetty-xml__9.3.27.v20190418.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.fusesource.leveldbjni--leveldbjni-all--org.fusesource.leveldbjni__leveldbjni-all__1.8.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2.external--aopalliance-repackaged--org.glassfish.hk2.external__aopalliance-repackaged__2.4.0-b34.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2.external--javax.inject--org.glassfish.hk2.external__javax.inject__2.4.0-b34.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2--hk2-api--org.glassfish.hk2__hk2-api__2.4.0-b34.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2--hk2-locator--org.glassfish.hk2__hk2-locator__2.4.0-b34.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2--hk2-utils--org.glassfish.hk2__hk2-utils__2.4.0-b34.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2--osgi-resource-locator--org.glassfish.hk2__osgi-resource-locator__1.0.1.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.bundles.repackaged--jersey-guava--org.glassfish.jersey.bundles.repackaged__jersey-guava__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.containers--jersey-container-servlet-core--org.glassfish.jersey.containers__jersey-container-servlet-core__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.containers--jersey-container-servlet--org.glassfish.jersey.containers__jersey-container-servlet__2.22.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.core--jersey-client--org.glassfis\n*** WARNING: skipped 26301 bytes of output ***\n\n (&#39;spark.databricks.clusterUsageTags.clusterNoDriverDaemon&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.instanceWorkerEnvId&#39;,\n  &#39;default-worker-env&#39;),\n (&#39;spark.hadoop.parquet.memory.pool.ratio&#39;, &#39;0.5&#39;),\n (&#39;spark.executor.extraJavaOptions&#39;,\n  &#39;-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=256m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Ddatabricks.serviceName=spark-executor-1&#39;),\n (&#39;spark.sparkr.use.daemon&#39;, &#39;false&#39;),\n (&#39;spark.scheduler.listenerbus.eventqueue.capacity&#39;, &#39;20000&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterScalingType&#39;, &#39;fixed_size&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterStateMessage&#39;, &#39;Starting Spark&#39;),\n (&#39;spark.sql.hive.metastore.jars&#39;, &#39;/databricks/hive/*&#39;),\n (&#39;spark.databricks.passthrough.adls.gen2.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider&#39;),\n (&#39;spark.hadoop.parquet.page.write-checksum.enabled&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.databricks.s3commit.client.sslTrustAll&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.s3a.threads.max&#39;, &#39;136&#39;),\n (&#39;spark.r.backendConnectionTimeout&#39;, &#39;604800&#39;),\n (&#39;spark.serializer.objectStreamReset&#39;, &#39;100&#39;),\n (&#39;spark.sql.sources.commitProtocolClass&#39;,\n  &#39;com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol&#39;),\n (&#39;spark.hadoop.hive.server2.idle.session.timeout&#39;, &#39;900000&#39;),\n (&#39;spark.databricks.redactor&#39;,\n  &#39;com.databricks.spark.util.DatabricksSparkLogRedactorProxy&#39;),\n (&#39;spark.databricks.clusterUsageTags.autoTerminationMinutes&#39;, &#39;120&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterPythonVersion&#39;, &#39;3&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableDfAcls&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.abfss.impl&#39;,\n  &#39;shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount&#39;, &#39;0&#39;),\n (&#39;spark.shuffle.service.enabled&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.spark.driverproxy.customHeadersToProperties&#39;,\n  &#39;X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds&#39;),\n (&#39;spark.hadoop.parquet.page.verify-checksum.enabled&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.s3a.multipart.threshold&#39;, &#39;104857600&#39;),\n (&#39;spark.hadoop.fs.s3a.impl&#39;, &#39;com.databricks.s3a.S3AFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.dataPlaneRegion&#39;, &#39;us-west-2&#39;),\n (&#39;spark.rpc.message.maxSize&#39;, &#39;256&#39;),\n (&#39;spark.logConf&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableJobsAutostart&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterMetastoreAccessType&#39;,\n  &#39;RDS_DIRECT&#39;),\n (&#39;spark.hadoop.hive.server2.enable.doAs&#39;, &#39;false&#39;),\n (&#39;eventLog.rolloverIntervalSeconds&#39;, &#39;3600&#39;),\n (&#39;spark.shuffle.memoryFraction&#39;, &#39;0.2&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterOwnerOrgId&#39;, &#39;5741052354962972&#39;),\n (&#39;spark.databricks.clusterUsageTags.containerZoneId&#39;, &#39;us-west-2c&#39;),\n (&#39;spark.databricks.clusterUsageTags.region&#39;, &#39;us-west-2&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterSpotBidPricePercent&#39;, &#39;100&#39;),\n (&#39;spark.files.useFetchCache&#39;, &#39;false&#39;)]</div>"]}}],"execution_count":15},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%sh\nls -lrt /databricks/hive/"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">total 162968\n-r-xr-xr-x 1 root root    51789 Jan  1  1970 third_party--prometheus-client--simpleclient-spark_2.4_2.11_deploy.jar\n-r-xr-xr-x 1 root root    51789 Jan  1  1970 third_party--prometheus-client--simpleclient-jetty9-hadoop1_2.11_deploy.jar\n-r-xr-xr-x 1 root root    12590 Jan  1  1970 third_party--prometheus-client--simpleclient_dropwizard-spark_2.4_2.11_deploy.jar\n-r-xr-xr-x 1 root root     3721 Jan  1  1970 third_party--opencensus-shaded--org.codehaus.mojo__animal-sniffer-annotations__1.14_shaded.jar\n-r-xr-xr-x 1 root root     6568 Jan  1  1970 third_party--opencensus-shaded--org.checkerframework__checker-compat-qual__2.5.2_shaded.jar\n-r-xr-xr-x 1 root root   250052 Jan  1  1970 third_party--opencensus-shaded--org.apache.thrift__libthrift__0.11.0_shaded.jar\n-r-xr-xr-x 1 root root   337709 Jan  1  1970 third_party--opencensus-shaded--org.apache.httpcomponents__httpcore__4.4.1_shaded.jar\n-r-xr-xr-x 1 root root   750113 Jan  1  1970 third_party--opencensus-shaded--org.apache.httpcomponents__httpclient__4.4.1_shaded.jar\n-r-xr-xr-x 1 root root     8468 Jan  1  1970 third_party--opencensus-shaded--io.opentracing__opentracing-util__0.31.0_shaded.jar\n-r-xr-xr-x 1 root root    11050 Jan  1  1970 third_party--opencensus-shaded--io.opentracing__opentracing-noop__0.31.0_shaded.jar\n-r-xr-xr-x 1 root root    14418 Jan  1  1970 third_party--opencensus-shaded--io.opentracing__opentracing-api__0.31.0_shaded.jar\n-r-xr-xr-x 1 root root     7999 Jan  1  1970 third_party--opencensus-shaded--io.opentracing.contrib__opentracing-tracerresolver__0.1.5_shaded.jar\n-r-xr-xr-x 1 root root   201999 Jan  1  1970 third_party--opencensus-shaded--io.opencensus__opencensus-impl-core__0.22.1_shaded.jar\n-r-xr-xr-x 1 root root    13211 Jan  1  1970 third_party--opencensus-shaded--io.opencensus__opencensus-impl__0.22.1_shaded.jar\n-r-xr-xr-x 1 root root     4514 Jan  1  1970 third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-util__0.22.1_shaded.jar\n-r-xr-xr-x 1 root root    20400 Jan  1  1970 third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-jaeger__0.22.1_shaded.jar\n-r-xr-xr-x 1 root root   372106 Jan  1  1970 third_party--opencensus-shaded--io.opencensus__opencensus-api__0.22.1_shaded.jar\n-r-xr-xr-x 1 root root     2937 Jan  1  1970 third_party--opencensus-shaded--io.jaegertracing__jaeger-tracerresolver__0.33.1_shaded.jar\n-r-xr-xr-x 1 root root   723216 Jan  1  1970 third_party--opencensus-shaded--io.jaegertracing__jaeger-thrift__0.33.1_shaded.jar\n-r-xr-xr-x 1 root root   136742 Jan  1  1970 third_party--opencensus-shaded--io.jaegertracing__jaeger-core__0.33.1_shaded.jar\n-r-xr-xr-x 1 root root     1348 Jan  1  1970 third_party--opencensus-shaded--io.jaegertracing__jaeger-client__0.33.1_shaded.jar\n-r-xr-xr-x 1 root root    30832 Jan  1  1970 third_party--opencensus-shaded--io.grpc__grpc-context__1.19.0_shaded.jar\n-r-xr-xr-x 1 root root    84796 Jan  1  1970 third_party--opencensus-shaded--com.squareup.okio__okio__1.13.0_shaded.jar\n-r-xr-xr-x 1 root root   421263 Jan  1  1970 third_party--opencensus-shaded--com.squareup.okhttp3__okhttp__3.9.0_shaded.jar\n-r-xr-xr-x 1 root root    63711 Jan  1  1970 third_party--opencensus-shaded--commons-logging__commons-logging__1.2_shaded.jar\n-r-xr-xr-x 1 root root   273501 Jan  1  1970 third_party--opencensus-shaded--commons-codec__commons-codec__1.9_shaded.jar\n-r-xr-xr-x 1 root root    87759 Jan  1  1970 third_party--opencensus-shaded--com.lmax__disruptor__3.4.2_shaded.jar\n-r-xr-xr-x 1 root root     9626 Jan  1  1970 third_party--opencensus-shaded--com.google.j2objc__j2objc-annotations__1.1_shaded.jar\n-r-xr-xr-x 1 root root  2679075 Jan  1  1970 third_party--opencensus-shaded--com.google.guava__guava__26.0-android_shaded.jar\n-r-xr-xr-x 1 root root    14655 Jan  1  1970 third_party--opencensus-shaded--com.google.errorprone__error_prone_annotations__2.1.3_shaded.jar\n-r-xr-xr-x 1 root root   243079 Jan  1  1970 third_party--opencensus-shaded--com.google.code.gson__gson__2.8.2_shaded.jar\n-r-xr-xr-x 1 root root    22005 Jan  1  1970 third_party--opencensus-shaded--com.google.code.findbugs__jsr305__3.0.2_shaded.jar\n-r-xr-xr-x 1 root root   554099 Jan  1  1970 third_party--jetty-client--jetty-util_shaded.jar\n-r-xr-xr-x 1 root root   161713 Jan  1  1970 third_party--jetty-client--jetty-io_shaded.jar\n-r-xr-xr-x 1 root root   214592 Jan  1  1970 third_party--jetty-client--jetty-http_shaded.jar\n-r-xr-xr-x 1 root root   305366 Jan  1  1970 third_party--jetty-client--jetty-client_shaded.jar\n-r-xr-xr-x 1 root root   289052 Jan  1  1970 third_party--jetty8-shaded-client--jetty-util_shaded.jar\n-r-xr-xr-x 1 root root    24876 Jan  1  1970 third_party--jetty8-shaded-client--jetty-jmx_shaded.jar\n-r-xr-xr-x 1 root root   106527 Jan  1  1970 third_party--jetty8-shaded-client--jetty-io_shaded.jar\n-r-xr-xr-x 1 root root    97056 Jan  1  1970 third_party--jetty8-shaded-client--databricks-patched-jetty-http-jar_shaded.jar\n-r-xr-xr-x 1 root root    90874 Jan  1  1970 third_party--jetty8-shaded-client--databricks-patched-jetty-client-jar_shaded.jar\n-r-xr-xr-x 1 root root    35266 Jan  1  1970 third_party--jackson--paranamer_only_shaded.jar\n-r-xr-xr-x 1 root root    34096 Jan  1  1970 third_party--jackson--jsr305_only_shaded.jar\n-r-xr-xr-x 1 root root   584466 Jan  1  1970 third_party--jackson--jackson-module-scala-shaded_2.11_deploy.jar\n-r-xr-xr-x 1 root root  2208515 Jan  1  1970 third_party--jackson--guava_only_shaded.jar\n-r-xr-xr-x 1 root root   354141 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.11_shaded_20180920_b33d810.jar\n-r-xr-xr-x 1 root root   339812 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root   795619 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.11_shaded_20180920_b33d810.jar\n-r-xr-xr-x 1 root root   769407 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root  1510875 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--org.apache.htrace__htrace-core__3.1.0-incubating_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root   623410 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--joda-time__joda-time__2.4_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root    25949 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--javax.xml.stream__stax-api__1.0-2_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root   111993 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--javax.xml.bind__jaxb-api__2.2.2_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root     2926 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--javax.inject__javax.inject__1_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root    66053 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--javax.activation__activation__1.1_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root  1213628 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--io.reactivex__rxjava__1.2.4_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root  2351408 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--io.netty__netty-all__4.0.52.Final_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root   565969 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180920_b33d810-spark_2.4_2.11_deploy_shaded.jar\n-r-xr-xr-x 1 root root   469484 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180625_3682417-spark_2.4_2.11_deploy_shaded.jar\n-r-xr-xr-x 1 root root   938536 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--com.sun.xml.bind__jaxb-impl__2.2.3-1_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root    93957 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__retrofit__2.1.0_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root     5629 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__converter-jackson__2.1.0_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root    21159 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__adapter-rxjava__2.1.0_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root    74605 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--com.squareup.okio__okio__1.8.0_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root    27949 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp-urlconnection__3.3.1_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root   358031 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp__3.3.1_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root     8514 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__logging-interceptor__3.3.1_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root    65780 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.11_shaded_20180920_b33d810.jar\n-r-xr-xr-x 1 root root    63904 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root   283415 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.11_shaded_20180920_b33d810.jar\n-r-xr-xr-x 1 root root   274620 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root    82909 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--com.microsoft.rest__client-runtime__1.1.0_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root   835352 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__7.0.0_2.11_shaded_20180920_b33d810.jar\n-r-xr-xr-x 1 root root     5253 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.11_shaded_20180920_b33d810.jar\n-r-xr-xr-x 1 root root    11688 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-annotations__1.2.0_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root   746783 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--com.google.inject__guice__3.0_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root  2301786 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__16.0_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root  1797060 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__11.0.2_2.11_shaded_20180920_b33d810.jar\n-r-xr-xr-x 1 root root    39640 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--com.google.code.findbugs__jsr305__1.3.9_2.11_shaded_20180920_b33d810.jar\n-r-xr-xr-x 1 root root    68472 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root  1233029 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-databind__2.7.2_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root   262572 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.11_shaded_20180920_b33d810.jar\n-r-xr-xr-x 1 root root   257326 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root    53345 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-annotations__2.7.0_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root     5080 Jan  1  1970 third_party--hadoop-azure-2.7.3-abfs--aopalliance__aopalliance__1.0_2.11_shaded_20180625_3682417.jar\n-r-xr-xr-x 1 root root    22541 Jan  1  1970 third_party--datalake--datalake-spark_2.4_2.11_deploy.jar\n-r-xr-xr-x 1 root root   440528 Jan  1  1970 third_party--azure--org.apache.commons__commons-lang3__3.4_shaded.jar\n-r-xr-xr-x 1 root root   787147 Jan  1  1970 third_party--azure--com.microsoft.azure__azure-storage__7.0.0_shaded.jar\n-r-xr-xr-x 1 root root     4348 Jan  1  1970 third_party--azure--com.microsoft.azure__azure-keyvault-core__1.0.0_shaded.jar\n-r-xr-xr-x 1 root root   255295 Jan  1  1970 third_party--azure--com.fasterxml.jackson.core__jackson-core__2.7.2_shaded.jar\n-r-xr-xr-x 1 root root    15010 Jan  1  1970 spark--maven-trees--spark_2.4--xmlenc--xmlenc--xmlenc__xmlenc__0.52.jar\n-r-xr-xr-x 1 root root   565410 Jan  1  1970 spark--maven-trees--spark_2.4--software.amazon.ion--ion-java--software.amazon.ion__ion-java__1.0.2.jar\n-r-xr-xr-x 1 root root  2021167 Jan  1  1970 spark--maven-trees--spark_2.4--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.7.3.jar\n-r-xr-xr-x 1 root root    99555 Jan  1  1970 spark--maven-trees--spark_2.4--org.tukaani--xz--org.tukaani__xz__1.5.jar\n-r-xr-xr-x 1 root root   502553 Jan  1  1970 spark--maven-trees--spark_2.4--org.springframework--spring-test--org.springframework__spring-test__4.1.4.RELEASE.jar\n-r-xr-xr-x 1 root root  1006987 Jan  1  1970 spark--maven-trees--spark_2.4--org.springframework--spring-core--org.springframework__spring-core__4.1.4.RELEASE.jar\n-r-xr-xr-x 1 root root     9939 Jan  1  1970 spark--maven-trees--spark_2.4--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.16.jar\n-r-xr-xr-x 1 root root    40509 Jan  1  1970 spark--maven-trees--spark_2.4--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.16.jar\n-r-xr-xr-x 1 root root 10423653 Jan  1  1970 spark--maven-trees--spark_2.4--org.scalatest--scalatest_2.11--org.scalatest__scalatest_2.11__3.0.3.jar\n-r-xr-xr-x 1 root root  4623075 Jan  1  1970 spark--maven-trees--spark_2.4--org.scala-lang--scala-reflect_2.11--org.scala-lang__scala-reflect__2.11.12.jar\n-r-xr-xr-x 1 root root  5749423 Jan  1  1970 spark--maven-trees--spark_2.4--org.scala-lang--scala-library_2.11--org.scala-lang__scala-library__2.11.12.jar\n-r-xr-xr-x 1 root root   671138 Jan  1  1970 spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-xml_2.11--org.scala-lang.modules__scala-xml_2.11__1.0.5.jar\n-r-xr-xr-x 1 root root   471925 Jan  1  1970 spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-parser-combinators_2.11--org.scala-lang.modules__scala-parser-combinators_2.11__1.1.0.jar\n-r-xr-xr-x 1 root root   740209 Jan  1  1970 spark--maven-trees--spark_2.4--org.scalactic--scalactic_2.11--org.scalactic__scalactic_2.11__3.0.3.jar\n-r-xr-xr-x 1 root root   102981 Jan  1  1970 spark--maven-trees--spark_2.4--org.joda--joda-convert--org.joda__joda-convert__1.7.jar\n-r-xr-xr-x 1 root root   962858 Jan  1  1970 spark--maven-trees--spark_2.4--org.jdbi--jdbi--org.jdbi__jdbi__2.63.1.jar\n-r-xr-xr-x 1 root root    57183 Jan  1  1970 spark--maven-trees--spark_2.4--org.jboss.logging--jboss-logging--org.jboss.logging__jboss-logging__3.1.3.GA.jar\n-r-xr-xr-x 1 root root   623269 Jan  1  1970 spark--maven-trees--spark_2.4--org.hibernate--hibernate-validator--org.hibernate__hibernate-validator__5.1.1.Final.jar\n-r-xr-xr-x 1 root root   462141 Jan  1  1970 spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-util--org.eclipse.jetty__jetty-util__9.3.27.v20190418.jar\n-r-xr-xr-x 1 root root    86557 Jan  1  1970 spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlets--org.eclipse.jetty__jetty-servlets__9.3.27.v20190418.jar\n-r-xr-xr-x 1 root root   118957 Jan  1  1970 spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlet--org.eclipse.jetty__jetty-servlet__9.3.27.v20190418.jar\n-r-xr-xr-x 1 root root   527987 Jan  1  1970 spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-server--org.eclipse.jetty__jetty-server__9.3.27.v20190418.jar\n-r-xr-xr-x 1 root root    95072 Jan  1  1970 spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-security--org.eclipse.jetty__jetty-security__9.3.27.v20190418.jar\n-r-xr-xr-x 1 root root    87148 Jan  1  1970 spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-proxy--org.eclipse.jetty__jetty-proxy__9.3.27.v20190418.jar\n-r-xr-xr-x 1 root root   122464 Jan  1  1970 spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-io--org.eclipse.jetty__jetty-io__9.3.27.v20190418.jar\n-r-xr-xr-x 1 root root   150666 Jan  1  1970 spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-http--org.eclipse.jetty__jetty-http__9.3.27.v20190418.jar\n-r-xr-xr-x 1 root root    15968 Jan  1  1970 spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-continuation--org.eclipse.jetty__jetty-continuation__9.3.27.v20190418.jar\n-r-xr-xr-x 1 root root   263807 Jan  1  1970 spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-client--org.eclipse.jetty__jetty-client__9.3.27.v20190418.jar\n-r-xr-xr-x 1 root root   232248 Jan  1  1970 spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar\n-r-xr-xr-x 1 root root   792964 Jan  1  1970 spark--maven-trees--spark_2.4--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar\n-r-xr-xr-x 1 root root   326356 Jan  1  1970 spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.10.jar\n-r-xr-xr-x 1 root root   767140 Jan  1  1970 spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.6.jar\n-r-xr-xr-x 1 root root  1475955 Jan  1  1970 spark--maven-trees--spark_2.4--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar\n-r-xr-xr-x 1 root root  3479293 Jan  1  1970 spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.3.jar\n-r-xr-xr-x 1 root root    94150 Jan  1  1970 spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.3.jar\n-r-xr-xr-x 1 root root    40863 Jan  1  1970 spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.3.jar\n-r-xr-xr-x 1 root root   691479 Jan  1  1970 spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar\n-r-xr-xr-x 1 root root    44925 Jan  1  1970 spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar\n-r-xr-xr-x 1 root root    79912 Jan  1  1970 spark--maven-trees--spark_2.4--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar\n-r-xr-xr-x 1 root root    16560 Jan  1  1970 spark--maven-trees--spark_2.4--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar\n-r-xr-xr-x 1 root root   270342 Jan  1  1970 spark--maven-trees--spark_2.4--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar\n-r-xr-xr-x 1 root root   186273 Jan  1  1970 spark--maven-trees--spark_2.4--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar\n-r-xr-xr-x 1 root root    69500 Jan  1  1970 spark--maven-trees--spark_2.4--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar\n-r-xr-xr-x 1 root root  2035066 Jan  1  1970 spark--maven-trees--spark_2.4--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar\n-r-xr-xr-x 1 root root   365552 Jan  1  1970 spark--maven-trees--spark_2.4--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.8.1.jar\n-r-xr-xr-x 1 root root  1556863 Jan  1  1970 spark--maven-trees--spark_2.4--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar\n-r-xr-xr-x 1 root root    87603 Jan  1  1970 spark--maven-trees--spark_2.4--org.acplt--oncrpc--org.acplt__oncrpc__1.0.7.jar\n-r-xr-xr-x 1 root root   489884 Jan  1  1970 spark--maven-trees--spark_2.4--log4j--log4j--log4j__log4j__1.2.17.jar\n-r-xr-xr-x 1 root root   448794 Jan  1  1970 spark--maven-trees--spark_2.4--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar\n-r-xr-xr-x 1 root root   627814 Jan  1  1970 spark--maven-trees--spark_2.4--joda-time--joda-time--joda-time__joda-time__2.9.3.jar\n-r-xr-xr-x 1 root root    63777 Jan  1  1970 spark--maven-trees--spark_2.4--javax.validation--validation-api--javax.validation__validation-api__1.1.0.Final.jar\n-r-xr-xr-x 1 root root   100636 Jan  1  1970 spark--maven-trees--spark_2.4--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar\n-r-xr-xr-x 1 root root    95806 Jan  1  1970 spark--maven-trees--spark_2.4--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar\n-r-xr-xr-x 1 root root    38863 Jan  1  1970 spark--maven-trees--spark_2.4--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar\n-r-xr-xr-x 1 root root  1330219 Jan  1  1970 spark--maven-trees--spark_2.4--io.netty--netty--io.netty__netty__3.9.9.Final.jar\n-r-xr-xr-x 1 root root  4080249 Jan  1  1970 spark--maven-trees--spark_2.4--io.netty--netty-all--io.netty__netty-all__4.1.42.Final.jar\n-r-xr-xr-x 1 root root    16571 Jan  1  1970 spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__3.1.5.jar\n-r-xr-xr-x 1 root root     3937 Jan  1  1970 spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-log4j--io.dropwizard.metrics__metrics-log4j__3.1.5.jar\n-r-xr-xr-x 1 root root    39283 Jan  1  1970 spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__3.1.5.jar\n-r-xr-xr-x 1 root root    15824 Jan  1  1970 spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__3.1.5.jar\n-r-xr-xr-x 1 root root    19289 Jan  1  1970 spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__3.1.5.jar\n-r-xr-xr-x 1 root root    10168 Jan  1  1970 spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__3.1.5.jar\n-r-xr-xr-x 1 root root     9217 Jan  1  1970 spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-ganglia--io.dropwizard.metrics__metrics-ganglia__3.1.5.jar\n-r-xr-xr-x 1 root root   120465 Jan  1  1970 spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__3.1.5.jar\n-r-xr-xr-x 1 root root    43067 Jan  1  1970 spark--maven-trees--spark_2.4--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.7.jar\n-r-xr-xr-x 1 root root    24064 Jan  1  1970 spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-slf4j_2.11--com.typesafe.scala-logging__scala-logging-slf4j_2.11__2.1.2.jar\n-r-xr-xr-x 1 root root     7694 Jan  1  1970 spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-api_2.11--com.typesafe.scala-logging__scala-logging-api_2.11__2.1.2.jar\n-r-xr-xr-x 1 root root   219554 Jan  1  1970 spark--maven-trees--spark_2.4--com.typesafe--config--com.typesafe__config__1.2.1.jar\n-r-xr-xr-x 1 root root   151470 Jan  1  1970 spark--maven-trees--spark_2.4--com.twitter--util-jvm_2.11--com.twitter__util-jvm_2.11__6.23.0.jar\n-r-xr-xr-x 1 root root  1260130 Jan  1  1970 spark--maven-trees--spark_2.4--com.twitter--util-core_2.11--com.twitter__util-core_2.11__6.23.0.jar\n-r-xr-xr-x 1 root root   130103 Jan  1  1970 spark--maven-trees--spark_2.4--com.twitter--util-app_2.11--com.twitter__util-app_2.11__6.23.0.jar\n-r-xr-xr-x 1 root root    32124 Jan  1  1970 spark--maven-trees--spark_2.4--com.trueaccord.lenses--lenses_2.11--com.trueaccord.lenses__lenses_2.11__0.3.jar\n-r-xr-xr-x 1 root root    34654 Jan  1  1970 spark--maven-trees--spark_2.4--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar\n-r-xr-xr-x 1 root root   273370 Jan  1  1970 spark--maven-trees--spark_2.4--commons-net--commons-net--commons-net__commons-net__3.1.jar\n-r-xr-xr-x 1 root root    62050 Jan  1  1970 spark--maven-trees--spark_2.4--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar\n-r-xr-xr-x 1 root root   284220 Jan  1  1970 spark--maven-trees--spark_2.4--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar\n-r-xr-xr-x 1 root root   185140 Jan  1  1970 spark--maven-trees--spark_2.4--commons-io--commons-io--commons-io__commons-io__2.4.jar\n-r-xr-xr-x 1 root root   305001 Jan  1  1970 spark--maven-trees--spark_2.4--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar\n-r-xr-xr-x 1 root root   143602 Jan  1  1970 spark--maven-trees--spark_2.4--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar\n-r-xr-xr-x 1 root root   298829 Jan  1  1970 spark--maven-trees--spark_2.4--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar\n-r-xr-xr-x 1 root root   588337 Jan  1  1970 spark--maven-trees--spark_2.4--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar\n-r-xr-xr-x 1 root root   284184 Jan  1  1970 spark--maven-trees--spark_2.4--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar\n-r-xr-xr-x 1 root root    41123 Jan  1  1970 spark--maven-trees--spark_2.4--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar\n-r-xr-xr-x 1 root root   246918 Jan  1  1970 spark--maven-trees--spark_2.4--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.4.jar\n-r-xr-xr-x 1 root root    92685 Jan  1  1970 spark--maven-trees--spark_2.4--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.2.8.jar\n-r-xr-xr-x 1 root root   596642 Jan  1  1970 spark--maven-trees--spark_2.4--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar\n-r-xr-xr-x 1 root root  2172168 Jan  1  1970 spark--maven-trees--spark_2.4--com.google.guava--guava--com.google.guava__guava__15.0.jar\n-r-xr-xr-x 1 root root   190432 Jan  1  1970 spark--maven-trees--spark_2.4--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar\n-r-xr-xr-x 1 root root    31866 Jan  1  1970 spark--maven-trees--spark_2.4--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__2.0.1.jar\n-r-xr-xr-x 1 root root    48468 Jan  1  1970 spark--maven-trees--spark_2.4--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.6.7.jar\n-r-xr-xr-x 1 root root  1165323 Jan  1  1970 spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.6.7.1.jar\n-r-xr-xr-x 1 root root   258919 Jan  1  1970 spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.6.7.jar\n-r-xr-xr-x 1 root root    46986 Jan  1  1970 spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.6.7.jar\n-r-xr-xr-x 1 root root    60282 Jan  1  1970 spark--maven-trees--spark_2.4--com.fasterxml--classmate--com.fasterxml__classmate__1.0.0.jar\n-r-xr-xr-x 1 root root   100317 Jan  1  1970 spark--maven-trees--spark_2.4--com.databricks.scalapb--scalapb-runtime_2.11--com.databricks.scalapb__scalapb-runtime_2.11__0.4.15-9.jar\n-r-xr-xr-x 1 root root   326066 Jan  1  1970 spark--maven-trees--spark_2.4--com.databricks.scalapb--compilerplugin_2.11--com.databricks.scalapb__compilerplugin_2.11__0.4.15-9.jar\n-r-xr-xr-x 1 root root   379697 Jan  1  1970 spark--maven-trees--spark_2.4--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar\n-r-xr-xr-x 1 root root    26880 Jan  1  1970 spark--maven-trees--spark_2.4--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.595.jar\n-r-xr-xr-x 1 root root   126170 Jan  1  1970 spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.595.jar\n-r-xr-xr-x 1 root root  1011886 Jan  1  1970 spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.595.jar\n-r-xr-xr-x 1 root root   476105 Jan  1  1970 spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.595.jar\n-r-xr-xr-x 1 root root   949982 Jan  1  1970 spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.595.jar\n-r-xr-xr-x 1 root root    26514 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--stax--stax-api--stax__stax-api__1.0.1.jar\n-r-xr-xr-x 1 root root    65261 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--oro--oro--oro__oro__2.0.8.jar\n-r-xr-xr-x 1 root root  1056168 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.2.6.jar\n-r-xr-xr-x 1 root root    99555 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.tukaani--xz--org.tukaani__xz__1.5.jar\n-r-xr-xr-x 1 root root   537956 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.protobuf--protobuf-java--org.spark-project.protobuf__protobuf-java__2.5.0-spark.jar\n-r-xr-xr-x 1 root root    74847 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-common-secure--org.spark-project.hive.shims__hive-shims-common-secure__0.13.1a.jar\n-r-xr-xr-x 1 root root    38881 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-common--org.spark-project.hive.shims__hive-shims-common__0.13.1a.jar\n-r-xr-xr-x 1 root root    37825 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.23--org.spark-project.hive.shims__hive-shims-0.23__0.13.1a.jar\n-r-xr-xr-x 1 root root    25090 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.20S--org.spark-project.hive.shims__hive-shims-0.20S__0.13.1a.jar\n-r-xr-xr-x 1 root root    29875 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.20--org.spark-project.hive.shims__hive-shims-0.20__0.13.1a.jar\n-r-xr-xr-x 1 root root     7672 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-shims--org.spark-project.hive__hive-shims__0.13.1a.jar\n-r-xr-xr-x 1 root root  1835517 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-service--org.spark-project.hive__hive-service__0.13.1a.jar\n-r-xr-xr-x 1 root root   730838 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-serde--org.spark-project.hive__hive-serde__0.13.1a.jar\n-r-xr-xr-x 1 root root  4683239 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-metastore--org.spark-project.hive__hive-metastore__0.13.1a.jar\n-r-xr-xr-x 1 root root   136005 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-jdbc--org.spark-project.hive__hive-jdbc__0.13.1a.jar\n-r-xr-xr-x 1 root root  6492166 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-exec--org.spark-project.hive__hive-exec__0.13.1a.jar\n-r-xr-xr-x 1 root root   169960 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-common--org.spark-project.hive__hive-common__0.13.1a.jar\n-r-xr-xr-x 1 root root    37299 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-cli--org.spark-project.hive__hive-cli__0.13.1a.jar\n-r-xr-xr-x 1 root root   115711 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-beeline--org.spark-project.hive__hive-beeline__0.13.1a.jar\n-r-xr-xr-x 1 root root    37324 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-ant--org.spark-project.hive__hive-ant__0.13.1a.jar\n-r-xr-xr-x 1 root root     8869 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.5.jar\n-r-xr-xr-x 1 root root    26084 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.5.jar\n-r-xr-xr-x 1 root root    36046 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.objenesis--objenesis--org.objenesis__objenesis__1.2.jar\n-r-xr-xr-x 1 root root    45944 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.json--json--org.json__json__20090211.jar\n-r-xr-xr-x 1 root root    48720 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.iq80.snappy--snappy--org.iq80.snappy__snappy__0.2.jar\n-r-xr-xr-x 1 root root  1809447 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__3.2.9.jar\n-r-xr-xr-x 1 root root  1890075 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__3.2.10.jar\n-r-xr-xr-x 1 root root   339666 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__3.2.6.jar\n-r-xr-xr-x 1 root root   780664 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar\n-r-xr-xr-x 1 root root   232248 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar\n-r-xr-xr-x 1 root root  6377448 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.groovy--groovy-all--org.codehaus.groovy__groovy-all__2.1.6.jar\n-r-xr-xr-x 1 root root   792964 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar\n-r-xr-xr-x 1 root root   392124 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.apache.velocity--velocity--org.apache.velocity__velocity__1.5.jar\n-r-xr-xr-x 1 root root   227712 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.9.2.jar\n-r-xr-xr-x 1 root root   275186 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.0.jar\n-r-xr-xr-x 1 root root   322234 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.1.jar\n-r-xr-xr-x 1 root root   720931 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.4.1.jar\n-r-xr-xr-x 1 root root  2831358 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.apache.derby--derby--org.apache.derby__derby__10.10.1.1.jar\n-r-xr-xr-x 1 root root   434678 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.4.jar\n-r-xr-xr-x 1 root root   378217 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.9.jar\n-r-xr-xr-x 1 root root  1556863 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar\n-r-xr-xr-x 1 root root  2000557 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar\n-r-xr-xr-x 1 root root    18333 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar\n-r-xr-xr-x 1 root root   148627 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar\n-r-xr-xr-x 1 root root   236660 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.antlr--ST4--org.antlr__ST4__4.0.4.jar\n-r-xr-xr-x 1 root root   164368 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.4.jar\n-r-xr-xr-x 1 root root    12131 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar\n-r-xr-xr-x 1 root root   489884 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--log4j--log4j--log4j__log4j__1.2.17.jar\n-r-xr-xr-x 1 root root   121070 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--junit--junit--junit__junit__3.8.1.jar\n-r-xr-xr-x 1 root root    87325 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--jline--jline--jline__jline__0.9.94.jar\n-r-xr-xr-x 1 root root   395195 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--javolution--javolution--javolution__javolution__5.5.1.jar\n-r-xr-xr-x 1 root root    15071 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--javax.transaction--jta--javax.transaction__jta__1.1.jar\n-r-xr-xr-x 1 root root   201124 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar\n-r-xr-xr-x 1 root root  1230201 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--io.netty--netty--io.netty__netty__3.8.0.Final.jar\n-r-xr-xr-x 1 root root  2665345 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--com.twitter--parquet-hadoop-bundle--com.twitter__parquet-hadoop-bundle__1.3.2.jar\n-r-xr-xr-x 1 root root    34654 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar\n-r-xr-xr-x 1 root root    62050 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar\n-r-xr-xr-x 1 root root   284220 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar\n-r-xr-xr-x 1 root root   208700 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--commons-io--commons-io--commons-io__commons-io__2.5.jar\n-r-xr-xr-x 1 root root   305001 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar\n-r-xr-xr-x 1 root root   588337 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar\n-r-xr-xr-x 1 root root   263865 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--commons-codec--commons-codec--commons-codec__commons-codec__1.8.jar\n-r-xr-xr-x 1 root root    41123 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar\n-r-xr-xr-x 1 root root   110600 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar\n-r-xr-xr-x 1 root root    16993 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--com.googlecode.javaewah--JavaEWAH--com.googlecode.javaewah__JavaEWAH__0.3.2.jar\n-r-xr-xr-x 1 root root    65612 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.reflectasm--reflectasm-shaded--com.esotericsoftware.reflectasm__reflectasm-shaded__1.07.jar\n-r-xr-xr-x 1 root root     4965 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.minlog--minlog--com.esotericsoftware.minlog__minlog__1.2.jar\n-r-xr-xr-x 1 root root   363460 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.kryo--kryo--com.esotericsoftware.kryo__kryo__2.21.jar\n-r-xr-xr-x 1 root root   445288 Jan  1  1970 spark--maven-trees--spark_1.4_hive_0.13--antlr--antlr--antlr__antlr__2.7.7.jar\n-r-xr-xr-x 1 root root  4665768 Jan  1  1970 ----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.11_deploy_shaded.jar\n-r-xr-xr-x 1 root root     3612 Jan  1  1970 ----scalapb_090--org.codehaus.mojo__animal-sniffer-annotations__1.17_shaded.jar\n-r-xr-xr-x 1 root root     4645 Jan  1  1970 ----scalapb_090--io.perfmark__perfmark-api__0.16.0_shaded.jar\n-r-xr-xr-x 1 root root    15308 Jan  1  1970 ----scalapb_090--io.opencensus__opencensus-contrib-grpc-metrics__0.21.0_shaded.jar\n-r-xr-xr-x 1 root root   366386 Jan  1  1970 ----scalapb_090--io.opencensus__opencensus-api__0.21.0_shaded.jar\n-r-xr-xr-x 1 root root    42008 Jan  1  1970 ----scalapb_090--io.grpc__grpc-stub__1.21.0_shaded.jar\n-r-xr-xr-x 1 root root     8014 Jan  1  1970 ----scalapb_090--io.grpc__grpc-protobuf-lite__1.21.0_shaded.jar\n-r-xr-xr-x 1 root root     5385 Jan  1  1970 ----scalapb_090--io.grpc__grpc-protobuf__1.21.0_shaded.jar\n-r-xr-xr-x 1 root root   245231 Jan  1  1970 ----scalapb_090--io.grpc__grpc-netty__1.22.1_shaded.jar\n-r-xr-xr-x 1 root root   626187 Jan  1  1970 ----scalapb_090--io.grpc__grpc-core__1.22.1_shaded.jar\n-r-xr-xr-x 1 root root    30681 Jan  1  1970 ----scalapb_090--io.grpc__grpc-context__1.22.1_shaded.jar\n-r-xr-xr-x 1 root root   226920 Jan  1  1970 ----scalapb_090--io.grpc__grpc-api__1.22.1_shaded.jar\n-r-xr-xr-x 1 root root   123818 Jan  1  1970 ----scalapb_090--com.lihaoyi__sourcecode_2.11__0.1.6_shaded.jar\n-r-xr-xr-x 1 root root   371741 Jan  1  1970 ----scalapb_090--com.lihaoyi__fastparse_2.11__2.1.2_shaded.jar\n-r-xr-xr-x 1 root root    72527 Jan  1  1970 ----scalapb_090--com.google.protobuf__protobuf-java-util__3.7.1_shaded.jar\n-r-xr-xr-x 1 root root  1446635 Jan  1  1970 ----scalapb_090--com.google.protobuf__protobuf-java__3.7.1_shaded.jar\n-r-xr-xr-x 1 root root  2481640 Jan  1  1970 ----scalapb_090--com.google.guava__guava__20.0_shaded.jar\n-r-xr-xr-x 1 root root    13684 Jan  1  1970 ----scalapb_090--com.google.errorprone__error_prone_annotations__2.3.2_shaded.jar\n-r-xr-xr-x 1 root root   238959 Jan  1  1970 ----scalapb_090--com.google.code.gson__gson__2.7_shaded.jar\n-r-xr-xr-x 1 root root  1229978 Jan  1  1970 ----scalapb_090--com.google.api.grpc__proto-google-common-protos__1.12.0_shaded.jar\n-r-xr-xr-x 1 root root     3329 Jan  1  1970 ----scalapb_090--com.google.android__annotations__4.1.1.4_shaded.jar\n-r-xr-xr-x 1 root root   328436 Jan  1  1970 ----scalapb_090--com.fasterxml.jackson.core__jackson-core__2.9.9_shaded.jar\n-r-xr-xr-x 1 root root   314121 Jan  1  1970 s3--s3-spark_2.4_2.11_deploy.jar\n-r-xr-xr-x 1 root root     2267 Jan  1  1970 s3commit--common--common-spark_2.4_2.11_deploy.jar\n-r-xr-xr-x 1 root root   159343 Jan  1  1970 s3commit--client--client-spark_2.4_2.11_deploy.jar\n-r-xr-xr-x 1 root root     6167 Jan  1  1970 logging--log4j-mod--log4j-mod-spark_2.4_2.11_deploy.jar\n-r-xr-xr-x 1 root root   119725 Jan  1  1970 jsonutil--jsonutil-spark_2.4_2.11_deploy.jar\n-r-xr-xr-x 1 root root    55533 Jan  1  1970 ----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar\n-r-xr-xr-x 1 root root  1115929 Jan  1  1970 ----jackson_databind_shaded--libjackson-databind.jar\n-r-xr-xr-x 1 root root   231350 Jan  1  1970 ----jackson_core_shaded--libjackson-core.jar\n-r-xr-xr-x 1 root root    39205 Jan  1  1970 ----jackson_annotations_shaded--libjackson-annotations.jar\n-r-xr-xr-x 1 root root     5824 Jan  1  1970 extern--libaws-regions.jar\n-r-xr-xr-x 1 root root  2389932 Jan  1  1970 extern--extern-spark_2.4_2.11_deploy.jar\n-r-xr-xr-x 1 root root     2235 Jan  1  1970 extern--acl--auth--auth-spark_2.4_2.11_deploy.jar\n-r-xr-xr-x 1 root root    13660 Jan  1  1970 dbfs--utils--dbfs-utils-spark_2.4_2.11_deploy.jar\n-r-xr-xr-x 1 root root   440056 Jan  1  1970 daemon--data--data-common--data-common-spark_2.4_2.11_deploy.jar\n-r-xr-xr-x 1 root root    20348 Jan  1  1970 daemon--data--client--conf--conf-spark_2.4_2.11_deploy.jar\n-r-xr-xr-x 1 root root   419039 Jan  1  1970 daemon--data--client--client-spark_2.4_2.11_deploy.jar\n-r-xr-xr-x 1 root root     9607 Jan  1  1970 common--path--path-spark_2.4_2.11_deploy.jar\n-r-xr-xr-x 1 root root     1863 Jan  1  1970 common--lazy--lazy-spark_2.4_2.11_deploy.jar\n-r-xr-xr-x 1 root root    52058 Jan  1  1970 common--jetty--client--client-spark_2.4_2.11_deploy.jar\n-r-xr-xr-x 1 root root    18770 Jan  1  1970 common--hadoop--hadoop-spark_2.4_2.11_deploy.jar\n-r-xr-xr-x 1 root root   112754 Jan  1  1970 common--credentials--credentials-spark_2.4_2.11_deploy.jar\n-r-xr-xr-x 1 root root    48214 Jan  1  1970 common--client--client-spark_2.4_2.11_deploy.jar\n-r-xr-xr-x 1 root root  4218921 Jan  1  1970 api-base--api-base-spark_2.4_2.11_deploy.jar\n-r-xr-xr-x 1 root root  1860192 Jan  1  1970 api-base--api-base_java-spark_2.4_2.11_deploy.jar\ndrwxr-xr-x 2 root root     4096 Jan 14 00:11 conf\n-rw-r--r-- 1 root root      410 Jan 14 00:11 bonecp-configs.jar\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.functions import *\n\nspark.read.format(\"csv\").options(path=\"\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2858653532987896&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> sampleData<span class=\"ansi-blue-fg\">=</span>spark<span class=\"ansi-blue-fg\">.</span>readStream<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;rate&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;rowsPerSecond&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-cyan-fg\">100</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;numPartitions&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-cyan-fg\">4</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 6</span><span class=\"ansi-red-fg\"> </span>sampleData<span class=\"ansi-blue-fg\">.</span>writeStream<span class=\"ansi-blue-fg\">.</span>outputMode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;append&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;console&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>trigger<span class=\"ansi-blue-fg\">(</span>Trigger<span class=\"ansi-blue-fg\">.</span>processingTime<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;10 second&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>start<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>awaitTermination<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span> \n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;Trigger&#39; is not defined</div>"]}}],"execution_count":18}],"metadata":{"name":"Learning_2","notebookId":2677037226184566},"nbformat":4,"nbformat_minor":0}
