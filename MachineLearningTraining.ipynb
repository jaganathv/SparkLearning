{"cells":[{"cell_type":"code","source":["7**4"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: 2401</div>"]}}],"execution_count":1},{"cell_type":"code","source":["s = \"Hi there Sam!\"\ntemp=s.split( )\nprint(temp)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;Hi&#39;, &#39;there&#39;, &#39;Sam!&#39;]\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["planet = \"Earth\"\ndiameter = 12742\nprint(\"The diameter of {0} is {1} kilometers.\".format(planet,diameter))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The diameter of Earth is 12742 kilometers.\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["lst = [1,2,[3,4],[5,[100,200,['hello']],23,11],1,7]\nprint(lst[3][1][2][0])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">hello\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["d = {'k1':[1,2,3,{'tricky':['oh','man','inception',{'target':[1,2,3,'hello']}]}]}\n\nd['k1'][3]['tricky'][3]['target'][3]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[20]: &#39;hello&#39;</div>"]}}],"execution_count":5},{"cell_type":"code","source":["def func(email):\n  return email.split('@')[-1]\n\nemail='user@domain.com'\nprint(func(email))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">domain.com\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["def checkDogExistence(name):\n  return \"dog\" in name.lower()\n \nprint(checkDogExistence('Is there a dog here?'))\n  \n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">True\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["def dogcount(name):\n  count=0\n  for i in name.lower().split():\n    if i == 'dog':\n      count+=1\n  return count\n\nprint(dogcount('This dog runs faster than the other dog dude!'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">2\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["def speedingTicket(speed,isBirthday):\n  if isBirthday:\n    if int(speed) <= 65:\n         result='No Ticket'\n    elif int(speed) >=66 and int(speed) <=85:\n         result='Small Ticket'\n    elif int(speed) >85:\n         result='Big Ticket'\n  else:\n    if int(speed) <= 60:\n      result='No ticket'\n    elif int(speed) >=61 and int(speed) <=80:\n      result='Small ticket'\n    elif int(speed) > 80:\n      result='Big Ticket'\n  return result\n    \nprint(speedingTicket('81',False))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Big Ticket\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark=SparkSession.builder.appName(\"Basics\").getOrCreate()\nprint(spark)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;pyspark.sql.session.SparkSession object at 0x7f5dd29b0898&gt;\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["df=spark.read.csv(\"/FileStore/tables/sales_info.csv\",inferSchema=True,header=True)\nprint(df.printSchema())\nprint(df.show())\n\nprint(df.groupBy(\"Company\").agg({\"Sales\":\"max\"}).show())\nprint(df.groupBy(\"Company\").count().show())\nprint(df.groupBy(\"Company\").max().show())\nprint(df.groupBy(\"Company\").min().show())\nprint(df.groupBy(\"Company\").sum().show())\nprint(df.groupBy(\"Company\").avg().show())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- Company: string (nullable = true)\n-- Person: string (nullable = true)\n-- Sales: double (nullable = true)\n\nNone\n+-------+-------+-----+\nCompany| Person|Sales|\n+-------+-------+-----+\n   GOOG|    Sam|200.0|\n   GOOG|Charlie|120.0|\n   GOOG|  Frank|340.0|\n   MSFT|   Tina|600.0|\n   MSFT|    Amy|124.0|\n   MSFT|Vanessa|243.0|\n     FB|   Carl|870.0|\n     FB|  Sarah|350.0|\n   APPL|   John|250.0|\n   APPL|  Linda|130.0|\n   APPL|   Mike|750.0|\n   APPL|  Chris|350.0|\n+-------+-------+-----+\n\nNone\n+-------+----------+\nCompany|max(Sales)|\n+-------+----------+\n   APPL|     750.0|\n   GOOG|     340.0|\n     FB|     870.0|\n   MSFT|     600.0|\n+-------+----------+\n\nNone\n+-------+-----+\nCompany|count|\n+-------+-----+\n   APPL|    4|\n   GOOG|    3|\n     FB|    2|\n   MSFT|    3|\n+-------+-----+\n\nNone\n+-------+----------+\nCompany|max(Sales)|\n+-------+----------+\n   APPL|     750.0|\n   GOOG|     340.0|\n     FB|     870.0|\n   MSFT|     600.0|\n+-------+----------+\n\nNone\n+-------+----------+\nCompany|min(Sales)|\n+-------+----------+\n   APPL|     130.0|\n   GOOG|     120.0|\n     FB|     350.0|\n   MSFT|     124.0|\n+-------+----------+\n\nNone\n+-------+----------+\nCompany|sum(Sales)|\n+-------+----------+\n   APPL|    1480.0|\n   GOOG|     660.0|\n     FB|    1220.0|\n   MSFT|     967.0|\n+-------+----------+\n\nNone\n+-------+-----------------+\nCompany|       avg(Sales)|\n+-------+-----------------+\n   APPL|            370.0|\n   GOOG|            220.0|\n     FB|            610.0|\n   MSFT|322.3333333333333|\n+-------+-----------------+\n\nNone\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["from pyspark.sql.functions import *\ndf.select(avg('Sales')).show()\nprint(df.select(countDistinct('Company')).show())\n#print(df.agg({'Company':'countDistinct'}).show())\n\nprint(df.select(stddev('Sales')).show())\nprint(df.select(mean('Sales')).show())\nprint(df.select(avg('Sales')).show())\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+\n       avg(Sales)|\n+-----------------+\n360.5833333333333|\n+-----------------+\n\n+-----------------------+\ncount(DISTINCT Company)|\n+-----------------------+\n                      4|\n+-----------------------+\n\nNone\n+------------------+\nstddev_samp(Sales)|\n+------------------+\n250.08742410799007|\n+------------------+\n\nNone\n+-----------------+\n       avg(Sales)|\n+-----------------+\n360.5833333333333|\n+-----------------+\n\nNone\n+-----------------+\n       avg(Sales)|\n+-----------------+\n360.5833333333333|\n+-----------------+\n\nNone\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["# No GroupBy. Just perform aggregation function based out of a Single Column\ndf.agg({'Sales':'max'}).show()\nprint(df.agg({'Sales':'min'}).show())\nprint(df.agg({'Sales':'max'}).show())\nprint(df.agg({'Sales':'sum'}).show())\nprint(df.agg({'Sales':'avg'}).show())\nprint(df.agg({'Sales':'count'}).show())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+\nmax(Sales)|\n+----------+\n     870.0|\n+----------+\n\n+----------+\nmin(Sales)|\n+----------+\n     120.0|\n+----------+\n\nNone\n+----------+\nmax(Sales)|\n+----------+\n     870.0|\n+----------+\n\nNone\n+----------+\nsum(Sales)|\n+----------+\n    4327.0|\n+----------+\n\nNone\n+-----------------+\n       avg(Sales)|\n+-----------------+\n360.5833333333333|\n+-----------------+\n\nNone\n+------------+\ncount(Sales)|\n+------------+\n          12|\n+------------+\n\nNone\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["from pyspark.sql.functions import format_number\n\n\ndf.orderBy('Sales',ascending=False).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-------+-----+\nCompany| Person|Sales|\n+-------+-------+-----+\n     FB|   Carl|870.0|\n   APPL|   Mike|750.0|\n   MSFT|   Tina|600.0|\n     FB|  Sarah|350.0|\n   APPL|  Chris|350.0|\n   GOOG|  Frank|340.0|\n   APPL|   John|250.0|\n   MSFT|Vanessa|243.0|\n   GOOG|    Sam|200.0|\n   APPL|  Linda|130.0|\n   MSFT|    Amy|124.0|\n   GOOG|Charlie|120.0|\n+-------+-------+-----+\n\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["# Handling Missing Values in Python\n\nfrom pyspark.sql import SparkSession\n\ndf_with_null=spark.read.csv(\"/FileStore/tables/ContainsNull.csv\",inferSchema=True,header=True)\ndf_with_null.show()\n\n# Dropping Records with 'any' nulls\nprint(df_with_null.na.drop().show())\nprint(df_with_null.na.drop(how='any').show())\n# Dropping Records with 'all' nulls\nprint(df_with_null.na.drop(how='all').show())\n\n# Dropping Records with atleast n columns having values as null\nprint(df_with_null.na.drop(thresh=1).show())\n\nprint(df_with_null.dropna(subset=['sales']).show())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----+-----+\n  Id| Name|Sales|\n+----+-----+-----+\nemp1| John| null|\nemp2| null| null|\nemp3| null|345.0|\nemp4|Cindy|456.0|\n+----+-----+-----+\n\n+----+-----+-----+\n  Id| Name|Sales|\n+----+-----+-----+\nemp4|Cindy|456.0|\n+----+-----+-----+\n\nNone\n+----+-----+-----+\n  Id| Name|Sales|\n+----+-----+-----+\nemp4|Cindy|456.0|\n+----+-----+-----+\n\nNone\n+----+-----+-----+\n  Id| Name|Sales|\n+----+-----+-----+\nemp1| John| null|\nemp2| null| null|\nemp3| null|345.0|\nemp4|Cindy|456.0|\n+----+-----+-----+\n\nNone\n+----+-----+-----+\n  Id| Name|Sales|\n+----+-----+-----+\nemp1| John| null|\nemp2| null| null|\nemp3| null|345.0|\nemp4|Cindy|456.0|\n+----+-----+-----+\n\nNone\n+----+-----+-----+\n  Id| Name|Sales|\n+----+-----+-----+\nemp3| null|345.0|\nemp4|Cindy|456.0|\n+----+-----+-----+\n\nNone\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["df_with_null.fillna('FILL VALUE').show()\n# Identifies all character columns in the DataFrame df_with_null and if any of the identified columns has a NULL value in it, It is replaced with 'FILL VALUE'\n\ndf_with_null.fillna(0).show()\n# Identifies all numeric columns in the DataFrame df_with_null and if any of the identified columns has a NULL value in it, It is replaced with 0\n\nprint(df_with_null.fillna('No Name',['Name']).show())\nmean_sales=df_with_null.select(avg('Sales')).collect()[0][0]\nprint(mean_sales)\nprint(df_with_null.fillna(mean_sales,subset=['Sales']).show())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----------+-----+\n  Id|      Name|Sales|\n+----+----------+-----+\nemp1|      John| null|\nemp2|FILL VALUE| null|\nemp3|FILL VALUE|345.0|\nemp4|     Cindy|456.0|\n+----+----------+-----+\n\n+----+-----+-----+\n  Id| Name|Sales|\n+----+-----+-----+\nemp1| John|  0.0|\nemp2| null|  0.0|\nemp3| null|345.0|\nemp4|Cindy|456.0|\n+----+-----+-----+\n\n+----+-------+-----+\n  Id|   Name|Sales|\n+----+-------+-----+\nemp1|   John| null|\nemp2|No Name| null|\nemp3|No Name|345.0|\nemp4|  Cindy|456.0|\n+----+-------+-----+\n\nNone\n400.5\n+----+-----+-----+\n  Id| Name|Sales|\n+----+-----+-----+\nemp1| John|400.5|\nemp2| null|400.5|\nemp3| null|345.0|\nemp4|Cindy|456.0|\n+----+-----+-----+\n\nNone\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["import requests\nresp=requests.get(\"https://raw.githubusercontent.com/jubins/Spark-And-MLlib-Projects/master/Spark_DataFrame_API_Project/appl_stock.csv\")\n#print(str(resp.content))\n\nwith open(\"/dbfs/FileStore/tables/appl_stock.csv\",\"w\") as file_rw_obj:\n  file_rw_obj.write(str(resp.content).replace('\\\\n','\\n').replace(\"b'\",\"\"))\n\nstock=spark.read.csv(\"/FileStore/tables/appl_stock.csv\",header=True,inferSchema=True,sep=',')  \nprint(stock.printSchema())\n\nprint(stock.show(10))\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- Date: string (nullable = true)\n-- Open: double (nullable = true)\n-- High: double (nullable = true)\n-- Low: double (nullable = true)\n-- Close: double (nullable = true)\n-- Volume: integer (nullable = true)\n-- Adj Close: double (nullable = true)\n\nNone\n+----------+------------------+------------------+------------------+------------------+---------+------------------+\n      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n+----------+------------------+------------------+------------------+------------------+---------+------------------+\n2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n+----------+------------------+------------------+------------------+------------------+---------+------------------+\nonly showing top 10 rows\n\nNone\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.context import SparkContext\n\nspark=SparkSession.builder.appName(\"walmart\").getOrCreate()\nwalmart_df=spark.read.csv(\"/FileStore/tables/walmart_stock.csv\",inferSchema=True,header=True)\nprint(walmart_df.columns)\nprint(walmart_df.printSchema())\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;Date&#39;, &#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39;, &#39;Volume&#39;, &#39;Adj Close&#39;]\nroot\n-- Date: timestamp (nullable = true)\n-- Open: double (nullable = true)\n-- High: double (nullable = true)\n-- Low: double (nullable = true)\n-- Close: double (nullable = true)\n-- Volume: integer (nullable = true)\n-- Adj Close: double (nullable = true)\n\nNone\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["walmart_df.select(walmart_df.columns[0:5]).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+------------------+------------------+------------------+------------------+\n               Date|              Open|              High|               Low|             Close|\n+-------------------+------------------+------------------+------------------+------------------+\n2012-01-03 00:00:00|         59.970001|         61.060001|         59.869999|         60.330002|\n2012-01-04 00:00:00|60.209998999999996|         60.349998|         59.470001|59.709998999999996|\n2012-01-05 00:00:00|         59.349998|         59.619999|         58.369999|         59.419998|\n2012-01-06 00:00:00|         59.419998|         59.450001|         58.869999|              59.0|\n2012-01-09 00:00:00|         59.029999|         59.549999|         58.919998|             59.18|\n2012-01-10 00:00:00|             59.43|59.709998999999996|             58.98|59.040001000000004|\n2012-01-11 00:00:00|         59.060001|         59.529999|59.040001000000004|         59.400002|\n2012-01-12 00:00:00|59.790001000000004|              60.0|         59.400002|              59.5|\n2012-01-13 00:00:00|             59.18|59.610001000000004|59.009997999999996|59.540001000000004|\n2012-01-17 00:00:00|         59.869999|60.110001000000004|             59.52|         59.849998|\n2012-01-18 00:00:00|59.790001000000004|         60.029999|         59.650002|60.009997999999996|\n2012-01-19 00:00:00|             59.93|             60.73|             59.75|60.610001000000004|\n2012-01-20 00:00:00|             60.75|             61.25|         60.669998|61.009997999999996|\n2012-01-23 00:00:00|         60.810001|             60.98|60.509997999999996|             60.91|\n2012-01-24 00:00:00|             60.75|              62.0|             60.75|61.389998999999996|\n2012-01-25 00:00:00|             61.18|61.610001000000004|61.040001000000004|         61.470001|\n2012-01-26 00:00:00|         61.799999|             61.84|             60.77|         60.970001|\n2012-01-27 00:00:00|60.860001000000004|         61.119999|60.540001000000004|60.709998999999996|\n2012-01-30 00:00:00|         60.470001|             61.32|         60.349998|         61.299999|\n2012-01-31 00:00:00|         61.529999|             61.57|         60.580002|61.360001000000004|\n+-------------------+------------------+------------------+------------------+------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import format_number\nsummary=walmart_df.describe()\nprint(walmart_df.columns)\nprint(walmart_df.printSchema())\n\nprint(summary.show())\n\nprint(summary.select('summary',format_number(summary['Open'].cast('double'),2)).show())\nsummary.createOrReplaceTempView(\"summary\")\nspark.sql(\"select summary,round(CAST(Open as float),2) as Open, \\\n                          round(CAST(High as float),2) as High,  \\\n                          round(CAST(Low as float),2) as Low, \\\n                          round(CAST(Close as float),2) as Close, \\\n                          round(CAST(Volume as float),2) as Volume, \\\n                          round(CAST(`Adj Close` as float),2) as `Adj Close` from summary\").show()\n\n\n\n\"\"\"\nsummary.select(summary['summary'], \\\n                 format_number(summary['Open'].cast('float'),2)), \\\n                 format_number(summary['High'])\n            \n  \"\"\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;Date&#39;, &#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39;, &#39;Volume&#39;, &#39;Adj Close&#39;]\nroot\n-- Date: timestamp (nullable = true)\n-- Open: double (nullable = true)\n-- High: double (nullable = true)\n-- Low: double (nullable = true)\n-- Close: double (nullable = true)\n-- Volume: integer (nullable = true)\n-- Adj Close: double (nullable = true)\n\nNone\n+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\nsummary|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n  count|              1258|             1258|             1258|             1258|             1258|             1258|\n   mean| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n stddev|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n    min|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n    max|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n\nNone\n+-------+--------------------------------------+\nsummary|format_number(CAST(Open AS DOUBLE), 2)|\n+-------+--------------------------------------+\n  count|                              1,258.00|\n   mean|                                 72.36|\n stddev|                                  6.77|\n    min|                                 56.39|\n    max|                                 90.80|\n+-------+--------------------------------------+\n\nNone\n+-------+------+------+------+------+-----------+----------+\nsummary|  Open|  High|   Low| Close|     Volume|Adj Close1|\n+-------+------+------+------+------+-----------+----------+\n  count|1258.0|1258.0|1258.0|1258.0|     1258.0|    1258.0|\n   mean| 72.36| 72.84| 71.92| 72.39|  8222093.5|     67.24|\n stddev|  6.77|  6.77|  6.74|  6.76|  4519781.0|      6.72|\n    min| 56.39| 57.06|  56.3| 56.42|  2094900.0|     50.36|\n    max|  90.8| 90.97| 89.25| 90.47|8.0898096E7|     84.91|\n+-------+------+------+------+------+-----------+----------+\n\nOut[32]: &#34;\\nsummary.select(summary[&#39;summary&#39;],                  format_number(summary[&#39;Open&#39;].cast(&#39;float&#39;),2)),                  format_number(summary[&#39;High&#39;])\\n            \\n  &#34;</div>"]}}],"execution_count":20},{"cell_type":"code","source":["#walmart_df.withColumn('HV Ratio',walmart_df['High']/walmart_df['Volume']).select(['HV Ratio']).show()\n# What day had the Peak High in Price?\nfrom pyspark.sql.functions import *\n#walmart_df.show()\n\n#walmart_df.groupBy('Date').agg({'High':'max'}).orderBy('max(High)',ascending=False).head(1)[0][0]\nwalmart_df.orderBy('Date',ascending=False).select('Date').head(1)[0][0]\nwalmart_df.createOrReplaceTempView(\"walmart_tbl\")\nspark.sql(\"select Date from walmart_tbl where High in (select max(High) from walmart_tbl)\").show()\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+\n               Date|\n+-------------------+\n2015-01-13 00:00:00|\n+-------------------+\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["print(walmart_df.agg({'Close':'avg'}).show())\nprint(walmart_df.agg({'Close':'mean'}).show())\nwalmart_df.agg({'Volume':'max','Volume':'min'}).show()\nprint(walmart_df.select(min(walmart_df['Volume']),max(walmart_df['Volume'])).show())\n\nfrom pyspark.sql.functions import max, min\nprint(walmart_df.select(max('Volume'),min('Volume')).show())\n\n#How many days was the Close lower than 60 dollars?\nwalmart_df.where(walmart_df['Close'] < 60).count()\n\n# What percentage of the time was the High greater than 80 dollars ?\npercentage=(walmart_df.where(walmart_df['High'] > 80).count()/walmart_df.count())*100\nprint(\"percentage:{0}\".format(percentage))\n\n# Pearsson Co-efficient\nprint('Pearsson Co-efficient: {0}'.format(walmart_df.corr('High','Volume')))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+\n       avg(Close)|\n+-----------------+\n72.38844998012726|\n+-----------------+\n\nNone\n+-----------------+\n       avg(Close)|\n+-----------------+\n72.38844998012726|\n+-----------------+\n\nNone\n+-----------+\nmin(Volume)|\n+-----------+\n    2094900|\n+-----------+\n\n+-----------+-----------+\nmin(Volume)|max(Volume)|\n+-----------+-----------+\n    2094900|   80898100|\n+-----------+-----------+\n\nNone\n+-----------+-----------+\nmax(Volume)|min(Volume)|\n+-----------+-----------+\n   80898100|    2094900|\n+-----------+-----------+\n\nNone\npercentage:9.141494435612083\nPearsson Co-efficient: -0.3384326061737161\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["#What is the max High per year?\nfrom pyspark.sql.functions import year,month\nprint(\"Max High Per Year\")\nwalmart_df.select(year(walmart_df['Date']).alias('Year'),'High').groupBy('Year').agg({'High':'max'}).show()\n#walmart_df.withColumn('Year',year(walmart_df['Date'])).groupBy('Year').agg({'High':'max'}).show()\n\n#What is the average Close for each Calendar Month?Â¶\n#print('Average Close Per calender Month')\n#print(walmart_df.withColumn('Month',month(walmart_df['Date'])).groupBy('Month').agg({'Close':'avg'}).orderBy('Month').show())\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Max High Per Year\n+----+---------+\nYear|max(High)|\n+----+---------+\n2015|90.970001|\n2013|81.370003|\n2014|88.089996|\n2012|77.599998|\n2016|75.190002|\n+----+---------+\n\n</div>"]}}],"execution_count":23}],"metadata":{"name":"MachineLearningTraining","notebookId":1731485616563893},"nbformat":4,"nbformat_minor":0}
